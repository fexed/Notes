\documentclass[10pt]{book}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{makecell}
\graphicspath{ {./img/} }
\usepackage{color}

\begin{document}
\renewcommand*\contentsname{Indice}
\title{Teoria dell'Informazione}
\author{Federico Matteoni}
\date{A.A. 2019/20}
\maketitle
\tableofcontents
\pagebreak
\chapter{Introduzione}
Francesco Romani \texttt{romani@di.unipi.it}\\
Mathematical Theory of Communication, Shannon.
\paragraph{Teoria} Fondamentale per l'informatica, passata in secondo piano rispetto alle nuove architetture, linguaggi moderni, multi-processing. Cambia tutto con la preponderanza della rete e con la diffusione degli smartphone. La base della trasmissione e della comunicazione, sia per 0/1 che per segnali, è basato sui concetti in questo corso. Importante perché le varie generazioni di reti telefoniche, differiscono per bande di frequenza ecc\ldots, ma per aumentare la trasmissione non si aumenta la potenza di trasmissione del segnale (rischi per la salute, diminuzione durata batteria). Piuttosto con algoritmi di trasmissioni più furbi, sistemi sofisticati di riconoscimento dei segnali, errori\ldots.\\
Noi partiamo da più lontano.

\paragraph{Esperimento finito} Estrazione su un alfabeto discreto. Probabilità uscita di un elemento può essere diversa dalle altre.\\
$p(x_1) \neq p(x_k)$\\
Ogni esperimento ha una sua incertezza. Detto così il concetto è vago, ma ci sono esperimenti più incerti di altri. Un indice di incertezza dei risultati sportivi sono le quote. L'incertezza non è influenzata da nozioni sull'esperimento.
\section{Assiomi di Shannon}
Shannon definì una funzione di incertezza con tre assiomi:
\begin{enumerate}
	\item Incertezza di p1, p2, \ldots, pk è una funzione continua delle pi. pi sono probabilità, quindi sommano a 1.
	\item Incertezza di un oggetto con k esperimenti equiprobabili è strettamente minore di quello con k+1 esperimenti.
	\item Esperimento composto. Faccio esperimento con certo n risultati. Per ognuno di questi faccio esperimento con certo m risultati. L'incertezza totale è l'incertezza del primo esperimento sommata all'incertezza di ciascuno esperimento successivo pesata con la probabilità del risultato corrispondente.
\end{enumerate}
\paragraph{Postulato} Definizione che si da per vero a priori senza dimostrazione.
\paragraph{Assioma} Definizioni che si mettono a base di una teoria, senza che siano per forza veri nella realtà.\\
Gli assiomi sono posizioni \textbf{ragionevoli} di Shannon su cui ha costruito la teoria. Consentono di trovare il valore di H incertezza.
\paragraph{Lemma 1.1} Se \textit{f} positiva\ldots\textbf{dim}
\paragraph{Lemma 1.2} L'incertezza di un esperimento ripetuto n volte è n volte l'incertezza dell'esperimento. $H(Y^n) = nH(Y)$ quindi $H(Y^n) = H(Y) + H(Y^{n-1})$ \textbf{dim}
\paragraph{Lemma 1.3} L'incertezza associata ad un esperimento con k uscite equiprobabili è $C\log(k)$
\paragraph{Teorema 1.1} L'unica funzione H che soddisfa i tre assiomi è $$-C \sum_{i=1}^k p_i \log(p_i)$$
\paragraph{Entropia} Incertezza associata ad un esperimento X, con C= 1/log2, quindi incertezza in bit. Un bit è per definizione la quantità di incertezza associata all'esperimento con due sole uscite equiprobabili. Vale 1 quindi per il caso p=1/2, lancio di una moneta fedele.\\
Perché Shannon la definisce entropia? Perché è ha la stessa formula in fisica, ed è la stessa cosa (entropia dell'informazione e entropia fisica).
\paragraph{Lemma del logaritmo} Quando i q tendono ai p questa espressione arriva ad un minimo.
\paragraph{Entropia} Compresa tra 0 e logk
\end{document}