\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {chapter}{\numberline {1}Machine Learning}{2}{chapter.1}% 
\contentsline {section}{\numberline {1.1}Machine Learning}{2}{section.1.1}% 
\contentsline {subsection}{\numberline {1.1.1}Machine Learning}{2}{subsection.1.1.1}% 
\contentsline {paragraph}{Definition}{2}{section*.2}% 
\contentsline {paragraph}{Data}{2}{section*.3}% 
\contentsline {paragraph}{Task}{2}{section*.4}% 
\contentsline {subsubsection}{Supervised learning}{3}{section*.5}% 
\contentsline {subsubsection}{Unsupervised learning}{3}{section*.6}% 
\contentsline {subsubsection}{Learning algorithm}{3}{section*.7}% 
\contentsline {paragraph}{Learning}{3}{section*.8}% 
\contentsline {paragraph}{Inductive bias}{4}{section*.9}% 
\contentsline {paragraph}{Version Space}{4}{section*.10}% 
\contentsline {paragraph}{Unbiased Learner}{4}{section*.11}% 
\contentsline {paragraph}{Loss}{4}{section*.12}% 
\contentsline {paragraph}{Learning and generalization}{4}{section*.13}% 
\contentsline {paragraph}{ML issues}{4}{section*.14}% 
\contentsline {subsection}{\numberline {1.1.2}Statistical Learning Theory}{5}{subsection.1.1.2}% 
\contentsline {subsubsection}{Vapnik-Chervoneniks dim and SLT}{5}{section*.15}% 
\contentsline {subsubsection}{Structural risk minimization}{5}{section*.16}% 
\contentsline {subsubsection}{Complexity control}{5}{section*.17}% 
\contentsline {paragraph}{Validation}{6}{section*.18}% 
\contentsline {paragraph}{Confusion Matrix}{6}{section*.19}% 
\contentsline {paragraph}{ROC Curve}{6}{section*.20}% 
\contentsline {subsection}{\numberline {1.1.3}Linear Models}{6}{subsection.1.1.3}% 
\contentsline {subsubsection}{Univariate Linear Regression}{7}{section*.21}% 
\contentsline {subsubsection}{Classification}{7}{section*.22}% 
\contentsline {subsubsection}{Learning Algorithms}{7}{section*.23}% 
\contentsline {paragraph}{Direct Approach with a normal equation}{8}{section*.24}% 
\contentsline {subsection}{\numberline {1.1.4}Gradient Descent}{8}{subsection.1.1.4}% 
\contentsline {subsubsection}{Batch version}{9}{section*.25}% 
\contentsline {subsubsection}{Online/Stochastic version}{9}{section*.26}% 
\contentsline {subsubsection}{Gradient Descent as error correction delta rule}{9}{section*.27}% 
\contentsline {subparagraph}{Limitations}{9}{section*.28}% 
\contentsline {subsection}{\numberline {1.1.5}Extending the linear model}{10}{subsection.1.1.5}% 
\contentsline {paragraph}{Learning Timing}{10}{section*.29}% 
\contentsline {paragraph}{k-NN}{10}{section*.30}% 
\contentsline {subparagraph}{Voronoi Diagram}{10}{section*.31}% 
\contentsline {subparagraph}{K-NN vs linear}{10}{section*.32}% 
\contentsline {subparagraph}{Bayes Error Rate}{10}{section*.33}% 
\contentsline {subparagraph}{Inductive bias of K-NN}{10}{section*.34}% 
\contentsline {subparagraph}{Limitations}{11}{section*.35}% 
\contentsline {section}{\numberline {1.2}Neural Networks}{11}{section.1.2}% 
\contentsline {subsection}{\numberline {1.2.1}Artificial Neuron}{12}{subsection.1.2.1}% 
\contentsline {subsubsection}{Perceptron}{12}{section*.36}% 
\contentsline {subparagraph}{Xor}{12}{section*.37}% 
\contentsline {paragraph}{Learning for one unit model}{12}{section*.38}% 
\contentsline {paragraph}{Perceptron Convergence theorem}{13}{section*.39}% 
\contentsline {subparagraph}{Preliminaries}{13}{section*.40}% 
\contentsline {subparagraph}{Proof}{13}{section*.41}% 
\contentsline {paragraph}{Differences}{13}{section*.42}% 
\contentsline {paragraph}{Activation functions}{14}{section*.43}% 
\contentsline {paragraph}{Gradient descent algorithm}{14}{section*.44}% 
\contentsline {paragraph}{Neural Network}{14}{section*.45}% 
\contentsline {subparagraph}{Architectures}{15}{section*.46}% 
\contentsline {subparagraph}{Flexibility}{15}{section*.47}% 
\contentsline {subparagraph}{Universal approximation}{15}{section*.48}% 
\contentsline {subparagraph}{Learning Algorithm}{15}{section*.49}% 
\contentsline {paragraph}{Backpropagation algorithm}{16}{section*.50}% 
\contentsline {subparagraph}{Step (1)}{16}{section*.51}% 
\contentsline {paragraph}{Issues in training neural networks}{16}{section*.52}% 
\contentsline {subparagraph}{Starting values}{16}{section*.53}% 
\contentsline {subparagraph}{Multiple minima}{17}{section*.54}% 
\contentsline {subparagraph}{Online/batch}{17}{section*.55}% 
\contentsline {subparagraph}{Learning rate}{17}{section*.56}% 
\contentsline {subparagraph}{Stopping criteria}{17}{section*.57}% 
\contentsline {subparagraph}{Overfitting and regularization}{17}{section*.58}% 
\contentsline {subparagraph}{Number of units}{18}{section*.59}% 
\contentsline {subparagraph}{Input scaling and output representation}{19}{section*.60}% 
\contentsline {section}{\numberline {1.3}Model Selection and Model Assessment}{20}{section.1.3}% 
\contentsline {subsection}{\numberline {1.3.1}Bias-Variance}{20}{subsection.1.3.1}% 
\contentsline {subsection}{\numberline {1.3.2}Motivations}{20}{subsection.1.3.2}% 
\contentsline {subsection}{\numberline {1.3.3}Validation}{20}{subsection.1.3.3}% 
\contentsline {paragraph}{Two aims}{20}{section*.61}% 
\contentsline {paragraph}{Grid Search}{21}{section*.62}% 
\contentsline {paragraph}{K-fold Cross Validation}{21}{section*.63}% 
\contentsline {paragraph}{Double cross-validation}{22}{section*.64}% 
\contentsline {paragraph}{Example with $k$-fold CV}{22}{section*.65}% 
\contentsline {paragraph}{Particular cases}{22}{section*.66}% 
\contentsline {subparagraph}{Lucky/unlucky sampling}{22}{section*.67}% 
\contentsline {subparagraph}{Very few data}{22}{section*.68}% 
\contentsline {subparagraph}{When to stop?}{22}{section*.69}% 
\contentsline {subparagraph}{Early Stopping}{22}{section*.70}% 
\contentsline {subparagraph}{Random initialization}{23}{section*.71}% 
\contentsline {section}{\numberline {1.4}Statistical Learning Theory}{23}{section.1.4}% 
\contentsline {subsection}{\numberline {1.4.1}VC-dim}{23}{subsection.1.4.1}% 
\contentsline {paragraph}{Shattering}{23}{section*.72}% 
\contentsline {subparagraph}{Example}{23}{section*.73}% 
\contentsline {paragraph}{VC-Dimension}{23}{section*.74}% 
\contentsline {paragraph}{Analytical Bound}{23}{section*.75}% 
\contentsline {paragraph}{Remarks}{24}{section*.76}% 
\contentsline {subsection}{\numberline {1.4.2}Structural Risk Minimization}{24}{subsection.1.4.2}% 
\contentsline {paragraph}{Model selection}{24}{section*.77}% 
\contentsline {paragraph}{Use of the bound}{24}{section*.78}% 
\contentsline {section}{\numberline {1.5}Support Vector Machines}{25}{section.1.5}% 
\contentsline {paragraph}{Separating hyperplane}{25}{section*.79}% 
\contentsline {paragraph}{Support Vector}{25}{section*.80}% 
\contentsline {paragraph}{Computing the distance}{27}{section*.81}% 
\contentsline {paragraph}{Computing the margin}{27}{section*.82}% 
\contentsline {paragraph}{Quadratic Optimization Problem}{27}{section*.83}% 
\contentsline {subparagraph}{Primal Form}{27}{section*.84}% 
\contentsline {subparagraph}{Kuhn-Tucker Conditions}{28}{section*.85}% 
\contentsline {subparagraph}{How does this improve the generalization?}{28}{section*.86}% 
\contentsline {subparagraph}{Theorem (Vapnik)}{29}{section*.87}% 
\contentsline {paragraph}{An elegant approach}{29}{section*.88}% 
\contentsline {paragraph}{Soft margin SVM}{29}{section*.89}% 
\contentsline {subparagraph}{Kuhn-Tucker Conditions}{29}{section*.90}% 
\contentsline {subparagraph}{Solving the problem}{30}{section*.91}% 
\contentsline {subsection}{\numberline {1.5.1}High-Dimensional feature spaces}{30}{subsection.1.5.1}% 
\contentsline {paragraph}{Kernel}{30}{section*.92}% 
\contentsline {subparagraph}{Kernel Matrix}{30}{section*.93}% 
\contentsline {subparagraph}{Mercer's Theorem}{30}{section*.94}% 
\contentsline {subparagraph}{Properties}{30}{section*.95}% 
\contentsline {paragraph}{Wrapping up}{31}{section*.96}% 
\contentsline {paragraph}{Examples of kernels}{31}{section*.97}% 
\contentsline {subsection}{\numberline {1.5.2}SVM for non-linear regression}{31}{subsection.1.5.2}% 
\contentsline {paragraph}{$\epsilon $-insensitive loss function}{31}{section*.98}% 
\contentsline {paragraph}{Optimization problem}{31}{section*.99}% 
\contentsline {paragraph}{Wrapping up}{32}{section*.100}% 
\contentsline {paragraph}{Summary of the main characteristics}{32}{section*.101}% 
\contentsline {subparagraph}{Pros}{32}{section*.102}% 
\contentsline {subparagraph}{Cons}{33}{section*.103}% 
\contentsline {subparagraph}{In practice}{33}{section*.104}% 
\contentsline {subsection}{\numberline {1.5.3}Kernel Methods}{33}{subsection.1.5.3}% 
\contentsline {section}{\numberline {1.6}Bias-Variance}{33}{section.1.6}% 
\contentsline {subparagraph}{Recall of statistics}{34}{section*.105}% 
\contentsline {subsection}{\numberline {1.6.1}Bias-Variance Decomposition}{34}{subsection.1.6.1}% 
\contentsline {section}{\numberline {1.7}Ensemble Learning}{35}{section.1.7}% 
\contentsline {subsection}{\numberline {1.7.1}Bagging}{35}{subsection.1.7.1}% 
\contentsline {paragraph}{Bootstrap Aggregating}{35}{section*.106}% 
\contentsline {subsection}{\numberline {1.7.2}Boosting}{35}{subsection.1.7.2}% 
\contentsline {subsection}{\numberline {1.7.3}Feature Selection}{35}{subsection.1.7.3}% 
\contentsline {section}{\numberline {1.8}Applications}{35}{section.1.8}% 
\contentsline {subsection}{\numberline {1.8.1}Character recognition (classification)}{35}{subsection.1.8.1}% 
\contentsline {paragraph}{First approaches}{35}{section*.107}% 
\contentsline {paragraph}{Basic idea}{35}{section*.108}% 
\contentsline {subsection}{\numberline {1.8.2}Convolutional Neural Networks}{36}{subsection.1.8.2}% 
\contentsline {paragraph}{The name}{36}{section*.109}% 
\contentsline {paragraph}{2D convolution}{36}{section*.110}% 
\contentsline {paragraph}{Pooling}{37}{section*.111}% 
\contentsline {paragraph}{Overview}{38}{section*.112}% 
\contentsline {paragraph}{Advantages}{38}{section*.113}% 
\contentsline {paragraph}{How to use?}{38}{section*.114}% 
\contentsline {paragraph}{Modern CNNs}{38}{section*.115}% 
\contentsline {paragraph}{Parallelize linear operations on GPU}{38}{section*.116}% 
\contentsline {subsection}{\numberline {1.8.3}Deep Learning}{38}{subsection.1.8.3}% 
\contentsline {paragraph}{Framework}{38}{section*.117}% 
\contentsline {paragraph}{Implement}{38}{section*.118}% 
\contentsline {subparagraph}{Hierarchical Abstraction}{38}{section*.119}% 
\contentsline {paragraph}{Techniques}{39}{section*.120}% 
\contentsline {paragraph}{Do we \textit {need} many layers?}{39}{section*.121}% 
\contentsline {subparagraph}{Examples}{39}{section*.122}% 
\contentsline {subparagraph}{Theoretical Analysis}{39}{section*.123}% 
\contentsline {subparagraph}{Inductive Bias}{39}{section*.124}% 
\contentsline {subparagraph}{Curse of Dimensionality}{40}{section*.125}% 
\contentsline {subparagraph}{Practical issues}{40}{section*.126}% 
\contentsline {paragraph}{Representation Learning}{40}{section*.127}% 
\contentsline {subparagraph}{Basic ideas}{40}{section*.128}% 
\contentsline {subparagraph}{Obtaining or exploiting hidden representation}{40}{section*.129}% 
\contentsline {subsubsection}{Implementing Deep Learning}{40}{section*.130}% 
\contentsline {paragraph}{Pretraining}{40}{section*.131}% 
\contentsline {paragraph}{Autoencoders}{40}{section*.132}% 
\contentsline {subparagraph}{Layer-wise pretraining}{41}{section*.133}% 
\contentsline {subparagraph}{Needed?}{41}{section*.134}% 
\contentsline {paragraph}{Transfer Learning}{41}{section*.135}% 
\contentsline {subparagraph}{Example of pretrained networks}{41}{section*.136}% 
\contentsline {paragraph}{Distributed Representation}{41}{section*.137}% 
\contentsline {subparagraph}{Input or internal representation?}{42}{section*.138}% 
\contentsline {subparagraph}{Count the difference}{42}{section*.139}% 
\contentsline {subparagraph}{Sharing attributes}{42}{section*.140}% 
\contentsline {subparagraph}{Example}{42}{section*.141}% 
\contentsline {subparagraph}{Beyond Neural Networks}{42}{section*.142}% 
\contentsline {subparagraph}{Interpretability}{42}{section*.143}% 
\contentsline {paragraph}{Deep Distributed Representation}{42}{section*.144}% 
\contentsline {paragraph}{Deep Learning Techniques}{42}{section*.145}% 
\contentsline {paragraph}{Gradient Issues}{42}{section*.146}% 
\contentsline {subparagraph}{ReLU}{43}{section*.147}% 
\contentsline {paragraph}{Batch Normalization}{43}{section*.148}% 
\contentsline {paragraph}{Dropout}{43}{section*.149}% 
\contentsline {paragraph}{L1 Regularization}{44}{section*.150}% 
\contentsline {paragraph}{Adversarial Training}{44}{section*.151}% 
\contentsline {subsection}{\numberline {1.8.4}Random Weights Neural Networks}{44}{subsection.1.8.4}% 
\contentsline {paragraph}{Overview}{44}{section*.152}% 
\contentsline {paragraph}{Structure}{44}{section*.153}% 
\contentsline {paragraph}{Cover's Theorem}{44}{section*.154}% 
\contentsline {subsubsection}{Feedforward Ranbdomized Neural Networks}{44}{section*.155}% 
\contentsline {paragraph}{Pros and cons}{44}{section*.156}% 
\contentsline {paragraph}{Suitable for}{45}{section*.157}% 
\contentsline {subsection}{\numberline {1.8.5}Unsupervised Learning in Neural Networks}{45}{subsection.1.8.5}% 
\contentsline {paragraph}{Unsupervised Learning}{45}{section*.158}% 
\contentsline {subparagraph}{Tasks}{45}{section*.159}% 
\contentsline {paragraph}{Clustering}{45}{section*.160}% 
\contentsline {subparagraph}{Similarity Measures}{45}{section*.161}% 
\contentsline {paragraph}{Vector Quantization}{45}{section*.162}% 
\contentsline {subparagraph}{Goal}{46}{section*.163}% 
\contentsline {subparagraph}{Loss function}{46}{section*.164}% 
\contentsline {paragraph}{Online $K$-means}{46}{section*.165}% 
\contentsline {paragraph}{Softmax}{47}{section*.166}% 
\contentsline {subsubsection}{Self-Organizing Maps}{47}{section*.167}% 
\contentsline {paragraph}{Usage}{47}{section*.168}% 
\contentsline {paragraph}{Competitive learning}{47}{section*.169}% 
\contentsline {paragraph}{Learning algorithm}{47}{section*.170}% 
\contentsline {paragraph}{Topological Order}{48}{section*.171}% 
\contentsline {paragraph}{Visualization}{48}{section*.172}% 
