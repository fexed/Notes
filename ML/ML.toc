\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {chapter}{\numberline {1}Machine Learning}{2}{chapter.1}% 
\contentsline {section}{\numberline {1.1}Machine Learning}{2}{section.1.1}% 
\contentsline {subsection}{\numberline {1.1.1}Machine Learning}{2}{subsection.1.1.1}% 
\contentsline {paragraph}{Definition}{2}{section*.2}% 
\contentsline {paragraph}{Data}{2}{section*.3}% 
\contentsline {paragraph}{Task}{2}{section*.4}% 
\contentsline {subsubsection}{Supervised learning}{3}{section*.5}% 
\contentsline {subsubsection}{Unsupervised learning}{3}{section*.6}% 
\contentsline {subsubsection}{Learning algorithm}{3}{section*.7}% 
\contentsline {paragraph}{Learning}{3}{section*.8}% 
\contentsline {paragraph}{Inductive bias}{3}{section*.9}% 
\contentsline {paragraph}{Version Space}{3}{section*.10}% 
\contentsline {paragraph}{Unbiased Learner}{4}{section*.11}% 
\contentsline {paragraph}{Loss}{4}{section*.12}% 
\contentsline {paragraph}{Learning and generalization}{4}{section*.13}% 
\contentsline {paragraph}{ML issues}{4}{section*.14}% 
\contentsline {subsection}{\numberline {1.1.2}Statistical Learning Theory}{4}{subsection.1.1.2}% 
\contentsline {subsubsection}{Vapnik-Chervoneniks dim and SLT}{5}{section*.15}% 
\contentsline {subsubsection}{Structural risk minimization}{5}{section*.16}% 
\contentsline {subsubsection}{Complexity control}{5}{section*.17}% 
\contentsline {paragraph}{Validation}{5}{section*.18}% 
\contentsline {paragraph}{Confusion Matrix}{6}{section*.19}% 
\contentsline {paragraph}{ROC Curve}{6}{section*.20}% 
\contentsline {subsection}{\numberline {1.1.3}Linear Models}{6}{subsection.1.1.3}% 
\contentsline {subsubsection}{Univariate Linear Regression}{6}{section*.21}% 
\contentsline {subsubsection}{Classification}{6}{section*.22}% 
\contentsline {subsubsection}{Learning Algorithms}{7}{section*.23}% 
\contentsline {paragraph}{Direct Approach with a normal equation}{7}{section*.24}% 
\contentsline {subsection}{\numberline {1.1.4}Gradient Descent}{8}{subsection.1.1.4}% 
\contentsline {subsubsection}{Batch version}{8}{section*.25}% 
\contentsline {subsubsection}{Online/Stochastic version}{8}{section*.26}% 
\contentsline {subsubsection}{Gradient Descent as error correction delta rule}{8}{section*.27}% 
\contentsline {subparagraph}{Limitations}{9}{section*.28}% 
\contentsline {subsection}{\numberline {1.1.5}Extending the linear model}{9}{subsection.1.1.5}% 
\contentsline {paragraph}{Learning Timing}{9}{section*.29}% 
\contentsline {paragraph}{k-NN}{9}{section*.30}% 
\contentsline {subparagraph}{Voronoi Diagram}{9}{section*.31}% 
\contentsline {subparagraph}{K-NN vs linear}{9}{section*.32}% 
\contentsline {subparagraph}{Bayes Error Rate}{9}{section*.33}% 
\contentsline {subparagraph}{Inductive bias of K-NN}{10}{section*.34}% 
\contentsline {subparagraph}{Limitations}{10}{section*.35}% 
\contentsline {section}{\numberline {1.2}Neural Networks}{10}{section.1.2}% 
\contentsline {paragraph}{Artificial Neuron}{10}{section*.36}% 
\contentsline {paragraph}{Perceptron}{11}{section*.37}% 
\contentsline {subparagraph}{Xor}{11}{section*.38}% 
\contentsline {paragraph}{Learning for one unit model}{11}{section*.39}% 
\contentsline {paragraph}{Perceptron Convergence theorem}{11}{section*.40}% 
\contentsline {subparagraph}{Preliminaries}{11}{section*.41}% 
\contentsline {subparagraph}{Proof}{12}{section*.42}% 
\contentsline {paragraph}{Differences}{12}{section*.43}% 
\contentsline {paragraph}{Activation functions}{12}{section*.44}% 
\contentsline {paragraph}{Gradient descent algorithm}{13}{section*.45}% 
\contentsline {paragraph}{Neural Network}{13}{section*.46}% 
\contentsline {subparagraph}{Architectures}{13}{section*.47}% 
\contentsline {subparagraph}{Flexibility}{13}{section*.48}% 
\contentsline {subparagraph}{Universal approximation}{14}{section*.49}% 
\contentsline {subparagraph}{Learning Algorithm}{14}{section*.50}% 
\contentsline {paragraph}{Backpropagation algorithm}{14}{section*.51}% 
\contentsline {subparagraph}{Step (1)}{15}{section*.52}% 
\contentsline {paragraph}{Issues in training neural networks}{15}{section*.53}% 
\contentsline {subparagraph}{Starting values}{15}{section*.54}% 
\contentsline {subparagraph}{Multiple minima}{15}{section*.55}% 
\contentsline {subparagraph}{Online/batch}{15}{section*.56}% 
\contentsline {subparagraph}{Learning rate}{16}{section*.57}% 
\contentsline {subparagraph}{Stopping criteria}{16}{section*.58}% 
\contentsline {subparagraph}{Overfitting and regularization}{16}{section*.59}% 
\contentsline {subparagraph}{Number of units}{17}{section*.60}% 
\contentsline {subparagraph}{Input scaling and output representation}{17}{section*.61}% 
\contentsline {section}{\numberline {1.3}Model Selection and Model Assessment}{18}{section.1.3}% 
\contentsline {subsection}{\numberline {1.3.1}Bias-Variance}{18}{subsection.1.3.1}% 
\contentsline {subsection}{\numberline {1.3.2}Motivations}{18}{subsection.1.3.2}% 
\contentsline {subsection}{\numberline {1.3.3}Validation}{18}{subsection.1.3.3}% 
\contentsline {paragraph}{Two aims}{18}{section*.62}% 
\contentsline {paragraph}{Grid Search}{19}{section*.63}% 
\contentsline {paragraph}{K-fold Cross Validation}{19}{section*.64}% 
\contentsline {paragraph}{Double cross-validation}{19}{section*.65}% 
\contentsline {paragraph}{Example with $k$-fold CV}{19}{section*.66}% 
\contentsline {paragraph}{Particular cases}{19}{section*.67}% 
\contentsline {subparagraph}{Lucky/unlucky sampling}{19}{section*.68}% 
\contentsline {subparagraph}{Very few data}{20}{section*.69}% 
\contentsline {subparagraph}{When to stop?}{20}{section*.70}% 
\contentsline {subparagraph}{Early Stopping}{20}{section*.71}% 
\contentsline {subparagraph}{Random initialization}{20}{section*.72}% 
\contentsline {section}{\numberline {1.4}Statistical Learning Theory}{20}{section.1.4}% 
\contentsline {subsection}{\numberline {1.4.1}VC-dim}{20}{subsection.1.4.1}% 
\contentsline {paragraph}{Shattering}{20}{section*.73}% 
\contentsline {subparagraph}{Example}{20}{section*.74}% 
\contentsline {paragraph}{VC-Dimension}{20}{section*.75}% 
\contentsline {paragraph}{Analytical Bound}{21}{section*.76}% 
\contentsline {paragraph}{Remarks}{21}{section*.77}% 
\contentsline {subsection}{\numberline {1.4.2}Structural Risk Minimization}{21}{subsection.1.4.2}% 
\contentsline {paragraph}{Model selection}{21}{section*.78}% 
\contentsline {paragraph}{Use of the bound}{21}{section*.79}% 
\contentsline {section}{\numberline {1.5}Support Vector Machines}{22}{section.1.5}% 
\contentsline {paragraph}{Separating hyperplane}{22}{section*.80}% 
\contentsline {paragraph}{Support Vector}{22}{section*.81}% 
\contentsline {paragraph}{Computing the distance}{23}{section*.82}% 
\contentsline {paragraph}{Computing the margin}{23}{section*.83}% 
\contentsline {paragraph}{Quadratic Optimization Problem}{23}{section*.84}% 
\contentsline {subparagraph}{Primal Form}{23}{section*.85}% 
\contentsline {subparagraph}{Kuhn-Tucker Conditions}{24}{section*.86}% 
\contentsline {subparagraph}{How does this improve the generalization?}{24}{section*.87}% 
\contentsline {subparagraph}{Theorem (Vapnik)}{24}{section*.88}% 
\contentsline {paragraph}{An elegant approach}{24}{section*.89}% 
\contentsline {paragraph}{Soft margin SVM}{25}{section*.90}% 
\contentsline {subparagraph}{Kuhn-Tucker Conditions}{25}{section*.91}% 
\contentsline {subparagraph}{Solving the problem}{25}{section*.92}% 
\contentsline {subsection}{\numberline {1.5.1}High-Dimensional feature spaces}{25}{subsection.1.5.1}% 
\contentsline {paragraph}{Kernel}{25}{section*.93}% 
\contentsline {subparagraph}{Kernel Matrix}{26}{section*.94}% 
\contentsline {subparagraph}{Mercer's Theorem}{26}{section*.95}% 
\contentsline {subparagraph}{Properties}{26}{section*.96}% 
\contentsline {paragraph}{Wrapping up}{26}{section*.97}% 
\contentsline {paragraph}{Examples of kernels}{26}{section*.98}% 
\contentsline {subsection}{\numberline {1.5.2}SVM for non-linear regression}{26}{subsection.1.5.2}% 
\contentsline {paragraph}{$\epsilon $-insensitive loss function}{27}{section*.99}% 
\contentsline {paragraph}{Optimization problem}{27}{section*.100}% 
\contentsline {paragraph}{Wrapping up}{27}{section*.101}% 
