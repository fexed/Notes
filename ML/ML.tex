\documentclass[10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{textcomp}
\graphicspath{ {./img/} }
\usepackage{listings}
\usepackage{makecell}
\definecolor{backcolour}{RGB}{255,255,255}
\definecolor{codegreen}{RGB}{27,168,11}
\definecolor{codeblue}{RGB}{35,35,205}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{205,35,56}
\lstdefinestyle{myPython}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{codeblue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\small\ttfamily,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=2pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	language=python
}
\begin{document}
\title{Machine Learning}
\author{Federico Matteoni}
\date{A.A. 2021/22}
\renewcommand*\contentsname{Index}

\maketitle
\begin{multicols}{2}
\tableofcontents
\end{multicols}
\pagebreak
\chapter{Machine Learning}
\section{Machine Learning}
What is ML? Area of research combining aims of creating computers that could learn and powerful and adaptive statistical tools with rigorous foundation in computational science. Luxury or necessity? Growing availability and need for analysis of empirical data and difficult to provide intelligence and adaptivity by programming it. Change of paradigm.\\
Examples: spam classification, written text recognition\ldots No or poor prior knowledge and rules for solving the problem, but easier to have a source of training experience.\\
ML is considered the latest general-purpose technology, capable of drastically affect pre-existing economic and social structures. And already has. The ultimate aim is to bring benefits to the people by solving big and small problems, accelerating human progress and empowering humans to add intelligence in any other science field.
\subsection{Machine Learning} We restrict to the computational framework: principles, methods and algorithms for learning and prediction, from experience. Building a model to be used for predictions. Common framework: infer a model or a \textbf{function} from a set of examples which allows the generalization (accurate response to new data).\\
When can we use ML? Be aware of the opportunity and awareness. ML is useful when there's no or poor theory surrounding the phenomenon, or uncertain, noisy or incomplete data which hinders formalization of solutions. The requests are: source of training experience (representative data) and a tolerance on the precision of results. The best examples are models to solve real-world problems that are difficult to be treated with traditional techniques: face and voice recognition (knowledge too difficult to formalize in an algorithm), predicting biding strength of molecules to proteins (not enough human knowledge) and personalized behavior, such as recommendation systems, scoring messages according to user preferences\ldots
\paragraph{Definition} The ML studies and proposes methods to build functions/hypothesis from examples of observed data that fits the known examples and able to generalize, with reasonable accuracy, for new data (according to verifiable results and under statistical and computational conditions and criteria.
\paragraph{Data} Data \textbf{represents the available experience}. Representation problem: capturing the structure of the analyzed objects. Flat (attribute-value), structured\ldots, categorical or continuous, missing data\ldots \textbf{preprocessing}: variable scaling, encoding, selection\ldots
\paragraph{Task} The task defines the purpose of the application: knowledge that we want to achieve? which is the helpful nature of the result? what information are available?\\
\textbf{Predictive} task, classification and regression: function approximation\\
\textbf{Descriptive} task, cluster analysis and association rules: find subsets or groups of unclassified data.
\begin{center}
	\includegraphics[scale=0.5]{1.png}
\end{center}
\subsubsection{Supervised learning} Given a set of training examples as $\langle$input, output$\rangle$ = $\langle x,d\rangle$ (\textbf{labeled examples}) for an unknown function $f$, find a \textit{good approximation} of $f$, an hypothesis $h$ that can be used for making predictions on unseen data $x'$.\\
The target $d$ can be:
\begin{list}{}{}
	\item Discrete value, for \textbf{classification tasks}.\\
	$f(x) \in \{1,2,\ldots,k\}$\\
	Patterns, feature vectors, are seen as members of a class and the goal is to assign the new patterns observed to the correct class (or label)\\
	If the number of possible classes is two, then $f$ is a \textit{boolean function} and the task is called \textbf{binary classification} or \textbf{concept learning}: true or false, positive or negative, 0 or 1\ldots\\
	If the number of classes is greater that two then is a \textbf{multi-class classification task}.
	\item Real continuous value, for \textbf{regression tasks}.\\
	The patterns are seen as sets of variables (real values), and the task is a curve fitting task. The process aims to estimate a real-value function based of a finite set of noisy samples $\langle x, f(x) +$ random noise$\rangle$
\end{list}
\subsubsection{Unsupervised learning} No teacher. The training set is a set of unlabeled data $\langle x\rangle$. Examples: clustering, finding natural groupings in a set of data.
\subsubsection{Learning algorithm} Basing on data, task and model: heuristic search through the hypothesis space $H$ of the \textbf{best hypothesis}. I. e. the best approximation of the unknown target function, typically searching for the $h$ with the minimum \textit{error}. $H$ may not coincide with the set of all possible function and the search cannot be exhaustive, we need to make \textbf{assumpitons} (\textbf{inductive bias}).
\paragraph{Learning} Also called:
\begin{list}{}{}
	\item Inference, in statistics
	\item Adapting, in biology and systems
	\item Optimizing, in mathematics
	\item Training, in neural networks
	\item Function approximations, in mathematics
	\item \ldots
\end{list}
After introducing data, task, model and learning algorithm we will focus on: inductive bias, loss and concepts of generalization and validation.
\paragraph{Inductive bias} To set up a model we can make assumptions about the nature of the target function, concerning either:
\begin{list}{}{}
	\item constraints in the model, \textbf{language bias} (in the hypothesis space $H$, due to the set of hypothesis that we can express or consider
	\item constraints or preferences in learning algorithm/search strategy, \textbf{search bias} which is preferred
	\item or both
\end{list}
Such assumptions are needed to obtain an useful model for the ML aims, i.e. a model with generalization capabilities.\\
We can imagine learning a discrete function with discrete inputs assuming \textbf{conjunctive rules}, so using a \textbf{language bias} to work with a restricted hypothesis space.
\paragraph{Version Space} An hypothesis $h$ is consistent with the TR if $h(x) = d(x)$ for each training example $\langle x, d(x)\rangle$.\\
The \textbf{version space} $VS_{H, TR}$ is the subset of $H$ of the hypothesis consistent with all the training examples $\langle x, d(x)\rangle$ in the TR.\\
It's possible to do an exhaustive search in an efficient way, using clever algorithms. This means finding the set of all the hypothesis $h$ consistent with the TR set. 
\paragraph{Unbiased Learner} The language bias (ex: using only conjunctive rules, may be too restrictive: if the target concept is not in $H$ it cannot be represented in $H$. We can use an $H$ that expresses every teachable concept (among propositions), that means that $H$ is the set of all possible subsets of $X$: the power set $P(X)$. If $n = 10$ binary inputs, then $|X| = 2^{10} = 1024$ and $|P(X)| = 2^{1024} = 10^{308}$ possible concepts, which is much more than the number of the atoms in the universe.\\
An unbiased learner is unable to generalize: the only examples that are unambiguously classified by an unbiased learner represented with the VS are the training examples themselves. Each unobserved instance will be classified positively by exactly half of the hypothesis in the VS and negative by the other half. Indeed: $\forall\: h$ consistent with $x_i$, $\exists\:h'$ identical to $h$ except $h'(x_i) \neq h(x_i)$, $h\in$ VS $\Rightarrow h'\in$ VS (because they are identical on the TR)\\\\
Why prefer the search bias? In ML we use flexible approaches (expressive hypothesis spaces with universal capability of the models, for example neural networks or decision trees. We avoid the language bias, so we do not exclude a priori the unknown target function, but we focus on the search bias (ruled by the learning algorithm).
\paragraph{Loss} How to measure the quality of an approximation? We want to measure the distance between $h(x)$ and $d$, using a loss function/measure $L(h(x), d)$ for a pattern $x$ which has high value in cases of bad approximation. The error (or risk or loss) is an expected value of this $L$, for example $E(w) = \frac{1}{l}\sum_{p=1}^l L(h(x_p), d_p)$. Different $L$ for different tasks. Examples of loss functions:
\begin{list}{}{}
	\item Regression: $L(h(x_p), d_p) = (d_p - h(x_p))^2$, the squared error. MSE (mean squared error) over the data set
	\item Classification: $L(h(x_p), d_p) = \left\{\begin{array}{l l}
		0 & h(x_p) = d_p\\
		1 & else
	\end{array}\right.$
\end{list}
\paragraph{Learning and generalization} Learning: search for a \textbf{good function} in a function space from known data (typically minimizing an error/loss). \textbf{Good} with respect to generalization error: it measures how accurately the model predicts over novel samples of data (\textbf{measured over \underline{new} data}).\\
Generalization is the crucial point of ML. Performance in ML is the generalization accuracy or \textit{predictive accuracy} estimated by the error on the test set.
\paragraph{ML issues} Inferring general functions from known data is an ill posed problem, which means that in general the solution is not unique because we can't expect the exact solution with finite data. What can we represent? And so, what can we learn?\\
Learning phase: building the model including training. The prediction phase is evaluating the learned function over new never-seen-before samples (generalization capability). Inductive learning hypothesis: any $h$ that approximates $f$ well on training examples will also approximate $f$ well on new unseen instances $x$.\\
\textbf{Overfitting}: a learner overfits data if it outputs an hypothesis $h\in H$ having true/generalization error (risk) $R$ and empirical (training) error $E$, but there's another $h'\in H$ with $E' > E$ and $R' < R$, which means that $h'$ is the better one despite having a worse fitting.
\subsection{Statistical Learning Theory} Under what mathematical conditions is a model able to generalize? We want to investigate the generalization capability of a model, measured as a risk or test error, the role of the model complexity and the role of the number of data.\\
\textbf{Formal Setting}: approximate a function $f(x)$, with $d$ target ($d = f(x) +$ noise), minimizing the \textbf{risk function} $$R = \int L(d, h(x))\:dP(x,d)$$ which is the \textbf{true error over all the data}, given:
\begin{list}{}{}
	\item a value $d$ from the teacher and the probability distribution $P(x,d)$
	\item a loss function $L(h(x),d) = (d - h(x))^2$
\end{list}
We search for $h\in H\:|\: min\:R$, but we only have the finite data set $TR = (x_p, d_p)$ with $p = 1\ldots l$. Looking for $h$ means minimizing the empirical risk (the training error $E$), finding the best values for the model free parameters $$R_{emp} = \frac{1}{l}\sum_{p=1}^l (d_p - h(x_p))^2$$
The inductive principle is the \textbf{ERM}, Empirical Risk Minimization: can we use $R_{emp}$ to approximate $R$?
\subsubsection{Vapnik-Chervoneniks dim and SLT} Given the VC dimension (simply VC), a measure of complexity of $H$ and by that we mean its flexibility to fit data.\\
The VC-bound states that it holds with probability $\frac{1}{\delta}$ that $$R \leq R_{emp} + \epsilon\left(\frac{1}{l}, \textsl{VC},\frac{1}{\delta}\right)$$
\begin{list}{}{}
	\item $\epsilon$ is a function called VC-confidence, that grows with VC and decreases with higher $l$ and $\delta$
	\item $R_{emp}$ decreases using complex models (with high VC)
	\item $\delta$ is the confidence, and it rules the probability that the bound holds.\\
	$\delta = 0.01 \Rightarrow$ the bound holds with probability $0.99$
\end{list}
Intuitively:
\begin{list}{}{}
	\item Higher $l$ (data) $\Rightarrow$ lower VC confidence and bound closer to $R$
	\item A too simple model, meaning with low VC, can be not sufficient due to high $R_{emp}$ (\textbf{underfitting})
	\item An higher VC with fix $l \Rightarrow$ lower $R_{emp}$ but VC and hence $R$ may increase (\textbf{overfitting})
\end{list}
\subsubsection{Structural risk minimization} Minimize the bound! There are different bounds formulations according to different classes of $f$, of tasks\ldots\\
In other words, we can make a good approximation of $f$ from examples, provided that we have a good number of data and the complexity of the model is suitable for the task.
\subsubsection{Complexity control} The Statistical Learning Theory allows for a formal framing of the problem of generalization and overfitting, providing an analytic upper bound to the risk $R$ for the prediction over all the data, regardless of the type of learning algorithm or the details of the model. So \textbf{the machine learning is well founded}, the learning risk can be analytically limited and only a few concepts are fundamental. This leads to new models (such as the Support Vector Machine) and other methods that directly consider the control of the complexity in the construction of the model.
\paragraph{Validation} Central role for the applications and the project. Two aims:
\begin{list}{}{}
	\item \textbf{Model Selection}: estimating the performance (\textbf{generalization error}) of different models in order to choose the best one. This includes searching for the best hyperparameters of the model.\\
	It returns a model.
	\item \textbf{Model Assessment}: with the final model, estimating/evaluating its prediction error/risk (\textbf{generalization error}) over new test data.\\
	It returns an estimation.
\end{list}
\textbf{Golden rule}: keep the two goals separated and use different datasets for each one.\\\\
In an ideal world, we'd have a large training set, a large validation set for model selection and a very large external unseen data test set. With finite and often small data sets we have just an estimation of the generalization performance. We have to use some techniques: hold-out and k-fold cross validation, for example.
\begin{list}{}{}
	\item \textbf{Hold-Out}: we partition the dataset $D$ into \textbf{training set} TR, \textbf{validation/selection set} VL and \textbf{test set} TS.\\
	All three are disjoint: TR is used to run the training algorithm, VL can be used to select the best model (hyperparameters tuning) and the \textbf{TS is only used for model assessment}.
	\begin{center}
		\includegraphics[scale=0.5]{2.png}
	\end{center}
	\item \begin{multicols}{2}
	\textbf{K-Fold}: this technique can make use of insufficient data. We split the dataset $D$ into $k$ mutually exclusive subsets $D_1,\ldots, D_k$, we train on $D - D_i$ and test it on $D_i$.\\
	This can be applied to both VL and TS splitting. Can be computationally very expensive and there's the issue of choosing the number of folds $k$.
	\begin{center}
		\includegraphics[scale=0.5]{3.png}
	\end{center}
	\end{multicols}
\end{list}
\paragraph{Confusion Matrix} \begin{center}
	\begin{tabular}{|c|c|c|}
		\hline
		Actual/Predicted & \textbf{Positive} & \textbf{Negative}\\
		\hline
		\textbf{Positive} & TP & FN\\
		\hline
		\textbf{Negative} & FP & TN\\
		\hline
	\end{tabular}
\end{center}
\textbf{Specificity} = $\frac{\textsl{TN}}{\textsl{FP} + \textsl{TN}}$, and \textbf{true negative rate} $= 1 - \textsl{FPR}$\\
\textbf{Sensitivity} = $\frac{\textsl{TP}}{\textsl{TP} + \textsl{FN}}$, also known as \textbf{true positive rate} or \textbf{recall}\\
\textbf{Precision} = $\frac{\textsl{TP}}{\textsl{TP} + \textsl{FP}}$\\
\textbf{Accuracy}: \% of correctly classified patterns = $\frac{\textsl{TP + TN}}{\textsl{total}}$. Note that, for example, a 50\% accuracy on a binary classifier is equivalent to a random predictor.
\paragraph{ROC Curve} We plot \textbf{specificity} on \textbf{x-axis} and \textbf{sensitivity} on the \textbf{y-axis}. The diagonal corresponds to the worst classifier, the random guesser. Better curves have greater Area Under the Curve (AUC)
\subsection{Linear Models} Mainstay of statistics.
\subsubsection{Univariate Linear Regression} Simple linear regression: we start with 1 input variable $x$ and 1 output variable $y$. We assume a model $h_w(x)$ expressed as $out = w_1x + w_0$ where $w$ are real-valued coefficients or \textbf{free parameters}, also called \textbf{weights}.\\
Given that the $w$s are continuous valued, we have an infinite hypothesis space but a nice solution from classical math. We can learn with this basic tool and, although simple, it includes many relevant concepts of modern ML and many methods in the field are based on this.\\
\textbf{Least Mean Square}: learning means finding $w$ such that it minimizes the error/empirical loss, with best data fitting on the training set with $l$ examples.\\
So given a set of $l$ training examples $(x_p, y_p)$ with $p = 1,\ldots,l$, we have to find $h_w(x)$ in the form $w_1x + w_0$ that minimizes the expected loss on the training data. For the loss, we use the square of errors: \textbf{least mean square}, find $w$ to \textbf{minimize} the residual sum of squares.
$$Loss(h_w) = E(w) = \sum_{p=1}^l (y_p - h_w(x_p))^2 = \sum_{p=1}^l (y_p - (w_1x_p + w_0))^2$$
To have the mean, divide by $l$. How to solve? Local minimum as stationary point, so the gradient $\frac{\partial E(w)}{\partial w_i} = 0$ with $i = 1,\ldots,$ dim\_input $+1 = 1,\ldots, n+1$. For the simple linear regression (2 free parameters):
$$\frac{\partial E(w)}{\partial w_0} = 0\:\:\:\:\frac{\partial E(w)}{\partial w_1} = 0$$
$$\frac{\partial E(w)}{\partial w_0} = -2(y - h_w(x))\:\:\:\:\frac{\partial E(w)}{\partial w_1} = -2(y - h_w(x))\cdot x$$
\subsubsection{Classification} The same models used for regression can be used for classification: \textbf{categorical targets} $y$ or $d$, for example 0/1, -1/+1\ldots\\
We use an hyperplane ($wx$) assuming negative or positive values. We exploit such models to decide if a point $x$ belongs to the positive or the negative zone of the hyperplane to classify it. So we want to learn $w$ such that we get a good classification accuracy. The decision boundary is $x^Tw = w^Tx = w_0 + w_1x_1 + w_2x_2 = 0$ and we can introduce a threshold function which can be written in many ways: \begin{list}{}{}
	\item $h(x) = \left\{\begin{array}{c l}
	1 & \textsl{if }wx + w_0 \geq 0\\
	0 & \textsl{else}
\end{array}	 \right.$
	\item $h(x) = \textsl{sign}(wx + w_0)$
	\item \ldots
\end{list}
$w_0$ is called \textbf{threshold} or \textbf{bias}. $h(x) = w^Tx + w_0 \geq 0 \Leftrightarrow w^Tx \geq -w_0$
\subsubsection{Learning Algorithms} Introducing 2 learning algorithms, both based on LSM and used for the linear model on regression and classification tasks. We start redefining the learning problem and the loss for them (in the case of $l$ data and multidimensional inputs).\\\\
\textbf{Learning problem for classification tasks}: given a set of $l$ training examples $(x_p, y_p)$ and a loss function $L$, with $y_p \in \{0,1\}$ or $y_p \in \{-1, +1\}$, find the weight vector $w$ that minimizes expected loss on the training data $$R_{emp} = \frac{1}{l}\sum_{p=1}^l L(h(x_p), y_p)$$
The expected loss can be approximated by a smooth function. We can make the optimization problem easier by replacing the original objective function $L$ (0/1 loss) with a smooth, differentiable function: for example, the MSE loss (mean squared error).
\begin{center}
	\includegraphics[scale=0.75]{4.png}
\end{center}
No classification error minimizing either 0/1 loss or MSE loss. Given $l$ training examples $(x_p, y_p)$, find $w$ that minimizes the residual sum of squares $$E(w)= \sum_{p=1}^l (y_p - x_p^T w)^2 = ||y - Xw||^2$$
We can't use $h(x)$ in $E(w)$, as for regression, because $h(x) = \textsl{sign}(w^T x)$ is non-differentiable. Also, this is a quadratic function so the minimum always exists (but may not be unique). $X$ is a $l\times n$ matrix with a row for each $x_p$.
\paragraph{Direct Approach with a normal equation} Differentiating $E(w)$ with respect to $w$ to get the \textbf{normal equation} $$(X^T X)w = X^T y$$
In the derivation we also find that $$\frac{\partial E(w)}{\partial w_j} = -2 \sum_{p=1}^l (x_p)_j \cdot (y_p - x_p^T w)$$
If $X^T X$ is not singular, then the unique solution is given by $$w = (X^T X)^{-1}X^T y = X^+ y$$ with $X^+$ being the Moor-Penrose pseudoinverse. Else the solutions are infinite, so we can choose the \textbf{min norm}$(w)$ solution.\\
The \textbf{Singular Value Decomposition} can be used for computing the pseudoinverse of a matrix ($X^+$). With\\ $X = U\Sigma V^T \Rightarrow X^+ = V\Sigma^+ U^T$ replacing every non-zero entry by its reciprocal. We can apply SVD directly to compute $w = X^+ y$, obtaining the minimal norm (on $w$) solution of least squares problem.
$$\frac{\partial E(w)}{\partial w_j} = \frac{\partial \sum_{p=1}^l (y_p - x_p^T w)^2}{\partial w_j} = \ldots = -2 \sum_{p=1}^l (y_p - x_p^T w)(x_p)_j$$
\subsection{Gradient Descent} The derivation suggests an approach based on an iterative algorithm based on\\$\frac{\partial E(w)}{\partial w_j} = -2 \sum_{p=1}^l (y_p - x_p^T w)(x_p)_j$. The \textbf{gradient} is the \textbf{ascent direction}. We can move toward the minimum with a gradient descent $\Delta w = -$ gradient of $E(w)$. \textbf{Local search}: we begin with a initial weight vector and modify it iteratively to minimize the error function. The gradient vector is
$$\Delta w = -\frac{\partial E(w)}{\partial w} = \left[\begin{array}{c}
-\frac{\partial E(w)}{\partial w_1}\\\vdots\\-\frac{\partial E(w)}{\partial w_n}
\end{array}\right] = \left[\begin{array}{c}
\Delta w_1\\\vdots\\\Delta w_n
\end{array} \right]$$
Allowing us to work in a multi dimensional space without the need to visualize it. Hence, the iterative approach will move using a learning rule based on a "delta" of $w$ proportional to the opposite of the local gradient. The movements will be made according to $$w_{new} = w + \eta\cdot\Delta w$$
The simple algorithm is as follows:
\begin{enumerate}
	\item Start with weight vector $w_{initial}$ and fix $0 < \eta < 1$
	\item Compute $\Delta w = -$ gradient of $E(w) = -\frac{\partial E(w)}{\partial w}$ (or for each $w_i$)
	\item Compute $w_{new} = w + \eta\cdot\Delta w$ (or for each $w_i$)\\
	$\eta$ is the step size or \textbf{learning rate}
	\item Repeat from $2$ until convergence or $E(w)$ sufficiently small
\end{enumerate}
\subsubsection{Batch version} The gradient is the sum over all the $l$ patterns. Provides a more precise evaluation of the gradient over $l$ data. We upgrade the weight after the sum
$$\frac{\partial E(w)}{\partial w_j} = -2 \sum_{p=1}^l (y_p - x_p^T w)(x_p)_j$$
\subsubsection{Online/Stochastic version} We upgrade the weights with the error that is computed for each pattern. Hence, the second pattern output is based on weights already updated from the first and so on. In makes progress with each example it sees. Can be faster, but needs smaller $\eta$
$$\frac{\partial E_p(w)}{\partial w_j} = -2(y_p - x_p^T w)(x_p)_j = -\Delta_p w_j$$
\subsubsection{Gradient Descent as error correction delta rule} The error correction rule, also called Widrow-Hoff or delta rule, changes $w_j$ proportionally to the error (target $y$ $-$ output)
$$\Delta w_j = 2 \sum_{p=1}^l (x_p)_j (y_p - x_p^T w)$$
$$w_{new} = w + \eta\cdot\Delta w$$
We improve by learning on previous errors.\\\\
Gradient descent is a simple and effective local search approach to a LMS solution. It allows to search through an infinite hypothesis space, can be easily applied for continuous $H$ and differentiable losses and isn't only for linear models (also neural networks and deep learning models).\\
Many possible improvements (Newton, quasi-Newton methods, conjugate gradients\ldots)
\begin{list}{}{}
	\item \textbf{Language bias}: $H$ is a set of linear functions.
	\item \textbf{Search Bias}: ordered search guided by the least squares minimization goal. For instance, we could prefer a different method to obtain a restriction on the values of parameters, achieving a different solution with other properties.
\end{list}
Shows that even for a simple model there are many possibilities. We still need a principled approach.
\subparagraph{Limitations} In geometry, two set of points are linearly separable in an $n$-dimensional space if they can be separated by a $(n-1)$-dimensional hyper-plane. In 2 dimensions, if they can be separated by a line, in 3 dimensions, by a plane\ldots\\
The linear decision boundary can provide exact solutions only for linearly separable sets of points.
\subsection{Extending the linear model} We can use transformed inputs, such as $x, x^2, x^3\ldots$ with a non-linear relationship between inputs and output, maintaining the learning machinery used so far. $$h_w(x) = w_0 + w_1 x + w_2 x^2 + \ldots + w_M x^M = \sum_{j=0}^M w_j x^j$$ \textbf{Linear basis expansion} (LBE) $$h_w(x) = \sum_{k=0}^K w_k\phi_k(x)$$ Augments the input vector with additional variables which are transformations of $x$ according to a function $\phi_k : R^n \rightarrow R$, so number of parameters $K > n$: linear in the parameters, so we can use the same learning algorithms as before.\\
Which $\phi$? Towards the dictionary approaches. Pro: can model more complicated relationships than linear, so it's more expressive. Cons: with large basis of functions, we easily risk overfitting, hence we require controlling the complexity (as in flexibility of the model to fit the data). How to do that? Many approaches:
\begin{list}{}{}
	\item \textbf{Ridge Regression} (or \textbf{Tikhonov Regularization}): smoothed model.\\
	Add constraints to the sum of value of $|w_j|$, penalizing models with high values of $|w|$ (so favoring sparse models, using less terms due to weights $w_j = 0$ or close)
	$$Loss(w) = \sum_{p=1}^l (y_p - x_p^T w)^2 + \lambda||w||^2$$ with $\lambda$ being the regularization hyper-parameter. It implements the control of the model complexity, leading to a model with less VC-dim with a trade-off controlled through a single parameter, $\lambda$.\\
	This uses $||\:||_2$
	\item Lasso uses $||\:||_1$
	\item Elastic nets uses both $||\:||_1$ and $||\:||_2$
\end{list}
\paragraph{Learning Timing} \begin{list}{}{}
	\item \textbf{Eager}: analyze data and construct an explicit hypothesis
	\item \textbf{Lazy}: store tr data and wait test data point, then construct an ad hoc hypothesis.
\end{list}
\paragraph{k-NN}
The algorithm is simple: store the training data and given and input $x$ find the $k$ nearest training examples $x_i$, then output the mean label.
\subparagraph{Voronoi Diagram} Each cell consists of point closer to $x$ than any other patterns. The segments are all points in plan equidistant to two patterns. It is implicitly used by K-NN.
\subparagraph{K-NN vs linear} Two extremes of the ML panorama:
\begin{center}
	\begin{tabular}{c c}
		Linear & K-NN\\
		Rigid (low variance) & flexible (high variance)\\
		Eager & Lazy\\
		Parametric & Instance-Based
	\end{tabular}
\end{center}
\subparagraph{Bayes Error Rate} If we know the density $P(x,y)$, we classify the most probable class, using the conditional distribution as: output the class $v\:|\:$ is $max\:P(v\:|\:x)$.\\
The error rate of this classifier (called the Bayes classifier) is called the Bayes error rate: the minimum achievable error rate given the distribution of the data. K-NN directly approximates this solution (majority vote in a nearest neighborhood) except that conditional probability is relaxed to conditional probability withing a neighbordhood and probabilities are estimated by training sample proportions
\subparagraph{Inductive bias of K-NN} The assumed distance tells us which are the most similar examples. The classification is assumed similar to the classification of the neighbors according to the assumed metric.
\subparagraph{Limitations} The computational cost is deferred to the prediction phase: makes the local approximation to the target function for each new example to be predicted.\\
Moreover, high retrieval cost: computationally intensive, in time, for each new input because computes the distance from the test sample to all stored vectors, so high cost in space too (all training data).\\
It provides a good approximation if we can find a significant set of data close to any $x$. Can fail when we have a lot of input variables (high $n$, high dimensionality) due to the curse of dimensionality:
\begin{list}{}{}
	\item \textbf{Hard to find nearby points} in high dimensions\\
	K-NN can fail in high dimensions because it becomes difficult to gather $K$ observation close to a target point $x_q$: near neighborhoods tend to be spatially large and estimates are no longer local
	\begin{center}
		\includegraphics[scale=0.75]{5.png}
	\end{center}
	\item \textbf{Low sampling density} for high-dim data\\
	Sampling density is proportional to $l^{\frac{l}{d}}$ with $l$ data and $d$ data dim. If 100 points are needed to estimate a function in $R$ (1 dim), then $100^{10}$ are needed in $R^{10}$
	\item Irrelevant features: the \textbf{curse of noisy}\\
	If the target depends only on a few features in $x$, we could retrieve a similar pattern with the similarity dominated by the large number of irrelevant features.\\
	This grows with dimensionality.\\
	We may weight features according to the relevance, or adopt feature selection approaches (eliminating some variables)
\end{list}
% ML-21-NN-part1-v.0.1.pdf
\section{Neural Networks} Models used to:
\begin{list}{}{}
	\item Study and model biological systems and learning processes (biological realism is essential)
	\item Introduce effective machine learning systems and algorithms (often losing a strict biological realism, but machine learning, computational and algorithmic properties are essential)
\end{list}
For us: \textbf{Artificial Neural Networks} (ANN): a flexible machine learning tool in the sens of approximating functions (builds a mathematical function $h(x)$ with special properties). A neural network:
\begin{list}{}{}
	\item Can learn from examples
	\item Are universal approximators (\textbf{Theorem of Cybenko}): flexible approaches for arbitrary functions (including non-linear)
	\item Can deal with noisy and incomplete data, with a graceful degradation of performance
	\item Can handle continuous real and discrete data for both regression and classification tasks
	\item It's a \textbf{paradigm}: it encompasses a wide set of models.
\end{list}
\subsection{Artificial Neuron} Input from external source or other units, with weights $w$ as free parameters: can be modified by the learning process. The unit $i$ computes $f(\sum_j w_{ij}x_j)$ with $w_{ij}$ the weight from input $j$ to unit $i$. $f$ is called \textbf{activation function}: linear, threshold or logistic (sigmoid). The weighted sum ($\sum_j w_{ij}x_j$) is called net input to unit $i$, or $net_i$.
\begin{center}
	\includegraphics[scale=0.75]{6.png}
\end{center}
Three types of activation functions:
\begin{list}{}{}
	\item \textbf{Linear}, or identity: $f(x) = x$
	\item \textbf{Threshold}, for the \textbf{perceptrons}: $f(x) = sign(x)$
	\item \textbf{Logistic}: $f(x) = \frac{1}{1 + e^{-\alpha x}}$
\end{list}
\subsubsection{Perceptron} A neuron that uses a threshold as activation function. Can be composed and connected to build a network: \textbf{MLP}, Multi Layer Perceptron 
\subparagraph{Xor} $x_1 \oplus x_2 = x_1\cdot \overline{x_2} + \overline{x_1}\cdot x_2$. Let $h_1 = x_1\cdot x_2, h_2 x_1 + x_2$ then $x_1 \oplus x_2 = \overline{h_1} \cdot h_2$ with $\wedge = \cdot$ and $\vee = +$.\\
So two layers are sufficient, but single layer cannot model all functions due to limits of single perceptron and the linear separable problems.
\paragraph{Learning for one unit model} \begin{enumerate}
	\item \textbf{Adaline}, adaptive linear neuron: LMS direct solution and gradient descent solution
	\item \textbf{Perceptron}, non linear: only classificiation\\
	Minimize number of misclassified patterns, find $w\:|\:sign(w^Tx) = d$. Online algorithm, a step can be made for each input pattern.
	\begin{enumerate}
		\item Initialize weights
		\item Pick learning rate $\eta$ (between 0 and 1)
		\item Until stopping condition (es weights don't change)\\
		For each training pattern $(x, d)$ compute output activation $out = sign(w^T x)$.\\
		If $out = d$ don't change weights, if $out \neq d$ update weights $w_{new} = w + \eta\cdot d\cdot x$ adding $+\eta x$ if $wx\leq 0$ and $d = +1$ or $-\eta x$ if $wx > 0$ and $d = -1$.\\Different form $w_{new} = w + \frac{1}{2}\cdot\eta\cdot (d-out)\cdot x$
	\end{enumerate}
	\textbf{Delta Rule}: the form $w_{new} = w + \eta\cdot d\cdot x$ is the \textbf{Hebbian learning} form, while the form $w_{new} = w + \frac{1}{2}\cdot\eta\cdot (d-out)\cdot x$ is the \textbf{error correction learning} form. It's a recall from LMS: a error correction/delta/Widrow-Hoff/Adaline/LMS rule that changes the $w$ proportionally to the error (target $d -$ output).\\
	In terms of neurons, the adjustment made to a synaptic weight is proportional to the product of error signal and the input signal that excite the synapse. Easy to compute when errors signal $\delta$ is directly measurable (meaning that we know the desired response for each unit).
\end{enumerate}
\paragraph{Perceptron Convergence theorem} A perceptron can represent linear decision boundaries, so it can solve linearly separable problems. Also, it can always learn the solution with the perceptron learning algorithm.\\
The \textbf{perceptron convergence theorem} is a milestone: a biologically inspired model with well-defined and proved computational capabilities and proved by a theorem. It states that \textbf{the perceptron is guarateed to converge} (classifying correctly all the input patterns) \textbf{in a finite number of steps if the problem is linearly separable}. This independently of the starting point, although the final solution is not unique and it depends on the starting point.
\subparagraph{Preliminaries} We focus on positive patterns, assuming $(x_i, d_i)\in$ TR set with $d_i = +1$ or $-1$. We also omit $^T$ for the dot product $w^T x$ (sometimes).\\
Linearly separable $\Rightarrow\exists\: w^*$ solution $|\:d_i(w^* x_i)\geq \alpha = \hbox{min}_i\:d_i(w^*x_i) > 0$, henche $w^*(d_ix_i)\geq \alpha$.\\
With $x_i' = (d_ix_i)$ then $w^*$ solution $\Leftrightarrow w^*$ solution of $(x_i', +1)$\\
This because $w^*$ solves $\Rightarrow d_i(w^*x_i) \geq \alpha \Rightarrow (w^*d_ix_i)\geq\alpha\Rightarrow (w^*x_i')\geq\alpha \Rightarrow w^*$ solution of $(x_i', +1)$.\\
And if $w^*$ is a solution of $(x_i', +1)\Rightarrow(w^*d_ix_i)\geq \alpha\Rightarrow d_i(w^*x_i)\geq \alpha\Rightarrow w^*$ solves for $x_i$\\\\
Also assuming $w(0) = 0$ (at step $0$), $\eta = 1$ and $\beta = \hbox{max}_i |x_i|^2$ where $|\:|$ is the euclidean norm.\\
After $q$ errors (all false negatives), $w(q) = \sum_{j=1}^q x_{ij}$ with $_{ij}$ denoting the patterns belonging to the subset of misclassified patterns.
\subparagraph{Proof} The basic idea is that we can find lower and upper bound to $|w|$ as a function of $q^2$ steps (lower bound) and $q$ steps (upper bound) $\Rightarrow$ we can find the number of steps $q\:|$ the algorithm converges.\\
Lower bound on $|w(q)|$ is $w^* w(q) = w^*\sum_{j=1}^q x_{ij}\geq q\alpha$ recalling that $\alpha=\hbox{min}_i\:(w^*x_i)$. With Cauchy-Schwartz we know that $(wv)^2 \leq |w|^2|v|^2$ where $|w|^2 = ||w||_2 ^2$.\\
$|w^*|^2|w(q)|^2 \geq (w^*w(q))^2 \geq (q\alpha)^2 \Rightarrow |w(q)|^2 \geq \frac{(q\alpha)^2}{|w^*|^2}$. Also $|w(q)|^2 = |w(q-1)+ x_{iq}|^2 = |w(q-1)|^2 + 2w(q-1)x_{iq} + |x_{iq}|^2$ because $|a + b|^2 = |a|^2 + 2ab + |b|^2$\\
For the $q$-th error, $2w(q-1)x_{iq} < 0$, so $|w(q)|^2 \leq |w(q-1)|^2 + |x_{iq}|^2$ and by iteration $w(0) = 0$ we have $|w(q)|^2 \leq \sum_{j=1}^q |x_{ij}|^2 \leq q\beta$ with $\beta = \hbox{max}_i |x_i|^2$.\\\\
So we have an upper bound $q\beta$ and a lower bound $\frac{(q\alpha)^2}{|w^*|^2}$, so $$q\beta\geq |w(q)|^2\geq \frac{(q\alpha)^2}{|w^*|^2}$$ $$q\beta\geq q^2\alpha'$$ $$\beta\geq q\alpha'$$ $$q \leq \frac{\beta}{\alpha'}$$
\paragraph{Differences}
	$$w_{new} = w + \eta(d-\hbox{out})x$$
Apparently similar but:
\begin{multicols}{2}
	\begin{list}{}{\textbf{LMS algorithm}}
		\item LSM rule derived without threshold activation functions
		\item Hence for training $\delta = d - w^Tx$
		\item Can still be used for classification using\\$h(x) = sign(w^T x)$, LTU
		\item Minimizes $E(w)$ with out = $w^Tx$
		\item Asymptotic convergence also for not linear separable problems
		\item Not always zero classification errors
		\item Can be extended to network of units (NN) using the gradient based approach
	\end{list}
	\begin{list}{}{\textbf{Perceptron learning algorithm}}
		\item Perceptron uses out $= sign(w^Tx)$
		\item versus $\delta = d - sign(w^Tx)$
		\item
		\item Minimizes misclassifications (out = $sign(w^Tx)$)
		\item Always converges in a finite number of steps for a linear separable problem to a perfect classifier\\
		Else it doesn't converge
		\item 
		\item Difficult to be extended to network of units (NN)
	\end{list}
\end{multicols}
\paragraph{Activation functions}
\begin{list}{}{}
	\item \textbf{Linear}, or identity: $f(x) = x$
	\item \textbf{Threshold}, for the \textbf{perceptrons}: $f(x) = sign(x)$
	\item \textbf{Logistic}: $f(x) = \frac{1}{1 + e^{-\alpha x}}$\\
	This is a non-linear squashing function like the sigmoidal logistic function: it assumes a continuous range of values in the bounded interval $[0,1]$. It has the property of being a smoothed differentiable threshold function, with $\alpha$ being the slope parameter of the sigmoid function.
	\item \textbf{Radial basis}: $f(x) = e^{-\alpha x^2}$
	\item \textbf{Softmax}
	\item \textbf{Stochastic neurons}, where the output is +1 with probability $P(net)$ or -1 with $1-P(net) \Rightarrow$ Boltzmann machines and other models rooted in statistical mechanics.
\end{list}
For the derivatives, a step function has no derivative, which is exactly why it isn't used. The sigmoids have\\$\frac{df_\sigma(x)}{dx} = f_\sigma(x) (1-f_\sigma(x))$ and $\frac{df_{tanh}(x)}{dx} = 1 - f_{tanh}(x)^2$ for $\alpha = 1$.\\
The sigmoid logistic function has the property of being a smoothed \textit{differentiable} threshold function. Hence, we can derive a Least (Mean) Square algorithm, by computing the gradient of the mean square loss function as for the linear units (also for a classifier).\\
From $\sigma(x) = x^Tw$ to $\sigma(x) = f_\sigma(x^Tw)$ where $f_\sigma$ is a logistic function. Find $w$ that minimizes the residual sum of squares $$E(w) = \sum_p (d_p - \sigma(x_p))^2 = \sum_p (d_p - f_\sigma(x_p^Tw))^2$$
$$\frac{\partial E(w)}{\partial w_j} = -2\sum_p^l (x_p)_j (d_p - f_\sigma(x_p^Tw))f'_\sigma(x_p^Tw) =$$ $$= -2\sum_p^l(x_p)_j\delta_pf'_\sigma(\hbox{net}(x_p))\hbox{ for 1 unit and patterns }p = 1\ldots l$$
\paragraph{Gradient descent algorithm} 
The same as for linear units using the new delta rule $w_{new} = w + \eta\cdot\delta_p\cdot x_p$ where $\delta_p = (d_p - f_\sigma(net(x_p)))f'_\sigma(net(x_p)) = (d_p-out(x_p))f'_\sigma$
\paragraph{Neural Network} In an MLP architecture: units connected by weighted links, organized in layers:
\begin{list}{}{}
	\item input layer, source of the input $x$. Copies the source external input patterns $x$, without computing $net$ and $f$.
	\item hidden layer, projects onto another hidden or an output layer, computing.
\end{list}
Can be viewed as network of units o flexible function:
$$h(x) = f_k\left(\sum_j w_{kj}f_j\left(\sum_i w_{ji}x_i\right)\right)$$
As for notation, given
\begin{center}
	\includegraphics[scale=0.6]{7.png}
\end{center}
We have that the index $t$ denotes a generic unit, can be either $k$ or $j$. $u$ denotes a generic input component, either $i$ or $j$.\\
$x$ is a generic input from an external source (input vector) or from other units according to the position of the unit in the network. If we load the pattern $x$ in the input layer, we can use the notation with $o$ for both the inputs and the hidden units outputs. Hence, inside the network, the input to each unit $t$ from any source $u$ (through the connection $w_{tu}$) is typically denoted as $o_u$.
\subparagraph{Architectures}
\begin{list}{}{} %TODO ML-21-NN-part1-v.0.1.pdf p.55
	\item \textbf{Feedforward NN} Standard architecture of the NNs. Direction: input $\longrightarrow$ output\\
	\item \textbf{Recurrent NN} adds feedback loops connections to the network topology. These self-loop connections provide the network with dynamical properties, leaving a "memory" of the past computations inside the model. This allows us to extend the representation capability of the model to the processing of sequences and structured data.
\end{list}
\subparagraph{Flexibility} The hypothesis space is a \textbf{continuous space} of all the functions that can be represented by assigning the weight values of the given architecture. Depending on the class of values produced by the output units (discrete or continuous), the model can deal, respectively, with classification tasks (sigmoidal output $f$) or regression tasks (linear output $f$). Also multi-class and multi-regression, with multiple output units.
\subparagraph{Universal approximation} The flexibility is theoretically grounded (Cybenko 1989, Hornik et al. 1993\ldots). In short, a single hidden-layer network with logistic activation functions can approximate (arbitrary well) every continuous function, provided enough units in the hidden layer.\\
A MLP network can approximate (arbitrarily well) every input-output mapping, provided enough units in the hidden layers.\\\\
Existence theorem: given $\epsilon$, $\exists\:h(x)\:|\:|f(x) - h(x)|< \epsilon\:\:\forall\:x$ in the hypercube.\\\\
With this fundamental result (MLP can represent \textit{any} function), two issues arise: how to learn by neural network and how to decide its architecture.\\
The \textbf{expressive power} of a NN is strongly influenced by the number of units and their configuration (architecture). The number of units can be related to the discussion of the VC-dim, specifically: the network capabilities are influenced by the number of parameters $w^*$, which is proportional to the number of units. Further studies also report the dependencies on their value sizes.\\\\
Es: weights = 0 $\Rightarrow$ minimal VC-dim, small weights $\Rightarrow$ linear part of the activation function,\\high values weights $\Rightarrow$ more complex model.\\
The universal approximation theorem is a fundamental contribution, showing that one hidden layer is sufficient in general, but it doesn't assure that a "small number" of units would do the work. For many function families it's possible to find boundaries on that number, but also cases for which a single hidden layer network would require an exponential number of units (in $n$ input dimension).\\
More layers can help, but is it possible to efficiently train deep networks?
\subparagraph{Learning Algorithm} The learning algorithm allows adapting the weights $w$ of the model in order to obtain the best approximation of the target function. Often realized in terms of minimization of and error/loss function on the training dataset. Same problem as other models: given a set of $l$ training examples $(x_p, d_p)$ and a loss measure $L$ (ex. the MSE $L(h(x_p), d_p) = (d_p - h_w(x_p))^2$), find the weight vector $w$ that minimizes the expected error on the training data $$E(w) = R_{emp} = \frac{1}{l}\sum_{p=1}^l L(h(x_p), d_p)$$
\textbf{Credit assignment problem}: which credit to the hidden units? Not easy when the error signal is not directly measurable: we don't know the error (delta) or the desired response for the hidden units, which is useful for changing their weights. Supposed too difficult, but the backpropagation algorithm brought a renaissance of the NN field.\\
\textbf{Loading problem}, NP-complete: given a network and a set of examples, is there a set of weights so that the network will be consistent with the examples?\\\\
In practice, networks can be trained in a reasonable amount of time, although optimal solutions are not guaranteed. How to solve? Key steps:
\begin{list}{}{}
	\item Credit assignment problem: how to change the hidden layer weights?
	\item Gradient descent approach can be extended to MLP (provided that loss and activations are differentiable functions, to find the delta for every unit in the network)
\end{list}
\paragraph{Backpropagation algorithm} We need a differentiable loss, differentiable activation functions and a network to follow the information flow.\\
Find $w$ by computing the gradient of the error function $$E(w) = R_{emp} = \frac{1}{l}\sum_{p=1}^l (h(x_p) - d_p)^2$$
It has nice properties: easy because of the \textbf{compositional} form of the model, and keeps track only of the quantities local of each unit (local variables), so the \textbf{modularity} of the units is preserved.
\begin{lstlisting}[style=myPython]
def backprop():
	# 1. Initialize all the weights w in the network and eta
	# 2. Compute out and e_tot
	while e_tot < epsilon:  # with epsilon desired value or other criteria
		for w_i in w:
			d_w_i = - (gradient of e_tot respect to w_i)  # step (1)
			w_new = w + eta*d_w_i + ...  # step (2)
		# Compute out and e_tot
	end
\end{lstlisting}
\subparagraph{Step (1)} $$\Delta w = - \frac{\partial E_{tot}}{\partial w} = - \sum_p \frac{\partial E_p}{\partial w} = \sum_p \Delta_p w$$
\paragraph{Issues in training neural networks} Heuristic guidelines in setting the backward propagation (backprop) algorithm. Generally, the models are over-parametrized, the optimization problem is not convex and is potentially unstable. We will discuss few of the issues. A good interpretation is to see backprop as a path trough the loss/weight space. The path depends on: data, neural network, starting point (initial weight values), rate of convergence, final point (stopping rule). This defines a control for the search over the hypothesis space. The basic algorithm, once again, is:
\begin{enumerate}
	\item Start with weight vector $w_{initial}$ and fix $0 < \eta < 1$
	\item Compute $\Delta w = -$ gradient of $E(w) = -\frac{\partial E(w)}{\partial w}$ (or for each $w_i$)
	\item Compute $w_{new} = w + \eta\cdot\Delta w$ (or for each $w_i$)\\
	$\eta$ is the step size or \textbf{learning rate}
	\item Repeat from $2$ until convergence or $E(w)$ sufficiently small
\end{enumerate}
But now the $\Delta w =-\frac{\partial E(w)}{w}$ were obtained through backprop derivation/algorithm for any weight in the network. At step 2, to compute the error, we first apply inputs to the network computing an output (\textbf{forward phase}), then we retro-propagate the deltas for the gradient (\textbf{backward phase}). So how to choose the $w_{initial}, \eta$ and the convergence?
\subparagraph{Starting values} In the basic algorithm, $w_{initial}$. The weights are initialized with random values close to zero. To be avoided: all zero, high values or all equal (symmetry), because this hampers the training. For standardized data, values $in [-0.7, +0.7]$. There are other heuristics: $range \cdot \frac{2}{fanin}$ with $fanin$ being the number of inputs to a hidden unit, or orthogonal matrices\ldots
\subparagraph{Multiple minima} The loss is not convex, has local minima. This affects the results, which depends on the starting weight values, hence: try a number of random starting configurations (5-10 or more \textbf{training runs} or \textbf{trials}). Useful taking the mean results (mean of errors) and looking at the variance to evaluate the model and then, if only one response is needed: we can choose the solution giving the lowest (penalized) validation error or we can take advantage of different end points and outputting a mean of the outputs (\textbf{committee approach}).\\
A "good" local minima is often sufficient: in ML we don't need the global or local minimum on $R_{emp}$, as we are searching the minimum of $R$ (which we can't compute). Often we stop early, in a point of non-zero gradient so being neither a local nor a global minima for the training error. The neural network builds a variable size hypothesis space, so VC-dim increases during training and the training error decreases toward zero (or global minimum) while the neural network becomes too complex. We \textbf{stop before this condition of overtraining}, avoiding overfitting.
\subparagraph{Online/batch} Batch version: sum all the gradients of each pattern over an epoch and then update the weights ($\Delta w$ after each epoch of $l$ patterns). Online/stochastic: upgrade $w$ for each pattern $p$, making progress with each example it sees. Faster but need smaller $\eta$.\begin{multicols}{2}
\textbf{Batch}
\begin{enumerate}
	\item Start with weight vector $w_{initial}$ and fix $0 < \eta < 1$
	\item Compute $\Delta w = -$ gradient of $E(w)$ on the entire training set (\textbf{epoch})
	\item Compute $w_{new} = w + \eta\cdot\Delta w$ (or for each $w_i$)
	\item Repeat from $2$ until convergence
\end{enumerate}
More accurate estimation of gradient
\columnbreak
\textbf{Online}
\begin{enumerate}
	\item Start with weight vector $w_{initial}$ and fix $0 < \eta < 1$
	\item For each pattern $p$\begin{enumerate}
		\item Compute $\Delta_p w = -$ gradient of $E(w)$
		\item Compute $w_{new} = w + \eta\cdot\Delta_p w$
	$\eta$ is the step size or \textbf{learning rate}
	\end{enumerate}
	\item Repeat from $2$ until convergence
\end{enumerate}
Since the gradient of a single data point can be considered a noisy approximation to the overall gradient, this is also called stochastic gradient descent.
\end{multicols}
Many variations exists, for example stochastic gradient descent with minibatch.
\subparagraph{Learning rate} With batch training we have more accurate estimation of gradient, higher $\eta$. With online we have training faster but potentially unstable, lower $\eta$. So high vs low $\eta \Leftarrow$ fast but unstable vs slow but stable. Typically $\eta \in [0.01, 0.5]$.\\
The learning curve, plotting the errors during training, allows to check the behavior in the early phases of the model design. Of course the absolute value depends also on model capability and other hyperparameters, but $\eta$ plays a big role in the curve quality. It's useful to have the mean of the gradients over the epoch: uniform approach (Least \textbf{Mean} Square). Some improvements: momentum (Nesterov), variable and adaptive learning rates, varying depending on the layers (in deep networks)\ldots\\
With momentum it becomes $\Delta w_{new} = -\eta\frac{\partial E(w)}{w} \underline{+ \alpha\Delta w_{old}}$, saving $\Delta w_{new}$ in $\Delta w_{old}$ for the next step. Becomes faster in plateaus but damps in oscillations (inertia effect, allows higher $\eta$)\\
Can be used in online by considering the previous example, $\Delta w_{p-1}$ as $\Delta w_{old}$.\\
It smooths the gradient over different examples. A variant is to evaluate the gradient after the momentum is applied (so using $\overline{w} = w + \alpha\Delta w_{old}$), improves the rate of convergence for the batch mode (not online!).\\
The variable learning rate starts high and decays linearly for each step until iteration $\tau$, using $\alpha = \frac{s}{\tau}$ with $s$ current step so that $\eta_s = (1-\alpha)\eta_0 + \alpha\eta_\tau$, then stops and uses a fixed small $\eta_\tau$. Set up as $\eta_\tau = 1\%$ of $\eta_0$ and $\tau$ as few hundred steps. $\eta_0$ same no instability/no stuck trade off.\\
With adaptive learning rate, it's automatically adapted during training possibly avoiding/reducing the fine tuning phase via hyperparameters selection.
% ML-21-NN-part2-v0.12.pdf p.42
\subparagraph{Stopping criteria} When to stop training? Basic: error. The best metric if we know the tolerance of data. Other: max tolerance instead of mean, number of misclassified, no more relevant weight changes or no more significant error decreasing.\\
In any case stop after too many epochs, but avoid stopping at an arbitrary fixed number of epochs, and not necessarily stop with very low training error.\\
Control of complexity is the main aim to achieve best generalization capability.
\subparagraph{Overfitting and regularization} Typically, stopping at the global minimum of $R_{emp}(w)$ is likely to be an overfitting solution. The control of complexity is our main aim to achieve the best generalization capability. For instance, we need to add some \textbf{regularization}: can be achieve directly (\textbf{penalty term}) or indirectly (\textbf{early stopping}). Model selection with cross validation on empirical data to find the trade-off.\\
In neural networks, we start learning with small random weights (breaking the symmetry!). As optimization proceeds, hidden units tend to saturate, increasing the effective number of free parameters (hence increasing the VC-dim). As we discussed, this is a variable-sized hypothesis space (changes during training).
\begin{center}
	\includegraphics[scale=0.5]{8.png}
\end{center}
How to act on the overtraining?
\begin{enumerate}
	\item Early stopping: using a validation set to determine when to stop (vague indication: when the validation error increases, so use more than one epoch before estimating (\textbf{patience}))\\
	Since the effective number of parameters grows during the training, halting the process effectively limits the complexity.
	\item Regularization on the loss: we can optimize the loss considering the weights values.\\
	Related to Tikhonov, so well principled approach: add a penalty term to the error function: $Loss(w) = \sum_p (d_p - f(x_p))^2 \underline{+\lambda||w||^2}$ with $||w||^2 = \sum_i w_i^2$\\
	The effect is a weight decay, basically $w_{new} = w + \eta\cdot\Delta w - 2\lambda w$.\\
	$\lambda$ is the \textbf{regularization parameter}, generally very low (0.01) and selected in the model selection phase. Applied on the linear model it's the ridge regression.\\
	More sophisticated penalty terms have been developed (ex: weight elimination, Haykin).
	Misunderstandings:
	\begin{list}{}{}
		\item Regularization is not a technique to control the stability of the training convergence but controls the \textbf{complexity of the model}, measured by VC-dim and related to the number of weights and values of the weights in the neural networks
		\item Early stopping needs a validation set to decide when to stop, which sacrifices some data. The regularization is a principled approach, as it allows the VL curve to follow the TR curve so that early stopping is not needed.\\
		But you can use both!
	\end{list}
	Typically the bias ($w_0$) is omitted because its inclusion causes the results to be not independent from target shift/scaling. May be included with its own regularization coefficient.\\
	Typically it's applied in the batch version. So for online/mini batch we need to take into account possible effects over many steps, so it's better to use $\lambda\cdot\frac{mb}{l}$ with $l$ being the number of total patterns.\\
	Other techniques: \textbf{dropout}.
	\item Pruning methods
\end{enumerate}
\subparagraph{Number of units} This is related to the control of complexity but also to the input dimension and the size of the TR set. In general, this is a model selection issue. The number of units, along with the regularization parameters, can be selected with the model selection phase (cross-validation).\\
Too few units $\Rightarrow$ underfitting, viceversa too many units $\Rightarrow$ overfitting. The number can be high with proper regularization.
\begin{list}{}{}
	\item \textbf{Constructive approaches}: the learning algorithm decides the number of hidden units, starting with small networks and adding new units.\\
	Incremental approach: algorithms that build a network starting with a minimal configuration and add new units and connections during training. Examples: Tower, Tiling Upstart for classification and Cascade Correlation for both regression and classification.\begin{list}{}{}
		\item The \textbf{Cascade Correlation} algorithm starts with N0, a network without hidden units, which is trained and evaluated. If N0 cannot solve the problem, go to N1: a hidden unit is added such that the correlation between the output of the unit and the residual error of N0 is maximized (by training its weights).\\
		After training, the weights of the new unit are frozen and the remaining weights are restrained. If the obtained network N1 cannot solve the problem, new hidden units are progressively added, which are connected with all the inputs and previously installed units. The process continues until the residual errors of the output layer satisfy a specified stopping criteria.\\
		This method dynamically builds up a neural network and terminates once a sufficient amount of hidden units has been found to solve the given problem. Specifically, it works by interleaving the minimization of the total error function (LMS) by simple backpropagation training of the output layer and the maximization of the (non-normalized) correlation (the covariance) of the new inserted hidden (candidate) units with the residual error. With $E_{p,k} = (o_{p,k} - d_{p,k})$ residual error, with $o_{p,k}$ output, $p$ pattern and $k$ output unit
		$$S = \sum_k\:\vline\sum_p(o_p - mean_p(o_{p,k}))(E_{p,k} - mean_p(E_{p,k}))\vline$$
		$$\frac{\partial S}{\partial w_j} = \sum_k sign(S_k)\sum_p(E_{p,k} - mean_p(E_{p,k}))f'(net_{p,h})I_{p,j}\:\:\hbox{with }h\hbox{ candidate index}$$
		\begin{center}
			\includegraphics[scale=0.5]{9.png}
		\end{center}
		The role of hidden units is to reduce the residual output error (solves a specific sub-problem and becomes a permanent "feature detector").\\
		Typically, since the maximization of the correlation is obtained using a gradient descent technique on a surface with several maxima, a pool of hidden units is trained and the best one selected to avoid local maxima. It's also greedy: easy to converge may also find a minimal number of units but may lead to overfitting.
\end{list}
	\item \textbf{Pruning methods}: start with a large network and progressively delete weights or units.
\end{list}
\subparagraph{Input scaling and output representation} Preprocessing can have large effects: normalization (via standardization and rescaling), categorical inputs, handling missing data\ldots\\
For the output, one or more linear units for regression. For classification, one unit (binary classification) or one of $k$ (multioutput):
\begin{list}{}{}
	\item sigmoid (choose the threshold to assign to class)
	\item rejection zone
	\item one-of-$k$ encoding (winner class chosen by taking the highest value among the outputs)
	\item Often symmetric logistic (tanh) learn faster
	\item Softmax for 0/1 targets, sum to 1 can be interpreted as probability of belonging to class $i$ among $k$ classes.
\end{list}
\section{Model Selection and Model Assessment}
\subsection{Bias-Variance} A bias-variance decomposition provides an useful framework to understand the validation issue, showing how difficult is the estimation of a model's performance, again showing the need of a trade-off between fitting capabilities (\textbf{bias}) and model flexibility (\textbf{variance}).
\begin{center}
	\includegraphics[scale=0.75]{10.png}
\end{center}
\subsection{Motivations} We are looking for the best solution, with minimal test error, looking for a balance between fitting (accuracy on training data) and model complexity. The \textbf{training set is not a good estimate of test error}.\\
Assuming that we have a set of tuning parameters $\Theta$, implicit or explicit, that varies the model complexity, we wish to find the value of $\Theta$ that minimizes test error: \textbf{methods for estimating the expected error} for a model (or each model of a class of models, or event a set of models\ldots)
\subsection{Validation}
We can approximate the estimation analytically:
\begin{list}{}{}
	\item AIC, BIC, Bayesian Information Criterion (limited to linear models)
	\item MDL
	\item Structural Risk Minimization and VC-Dimension
\end{list}
In practice, we can approximate the estimation on the data set by resampling, a direct estimation of error via cross-validation (hold-out, K-fold\ldots), bootstrap\ldots
\paragraph{Two aims} \begin{list}{}{}
	\item \textbf{Model Selection}: estimating the performance of different learning models in order to choose the best one (to generalize), which includes looking for the best hyperparameters of the model.\\
	It \textbf{returns a model}.
	\item \textbf{Model Assessment}: after choosing the final model (or class of models), estimating/evaluating the generalization error on \textbf{new test data}.\\
	It \textbf{returns an estimation value}.
\end{list}
The gold rule is to keep a separation between goals and to use separate data sets: \textbf{hold out}, if data set is sufficient, for example 50\% TR, 25\% VL and 25\% TS (\textbf{disjoint sets!})
	\begin{center}
		\includegraphics[scale=0.5]{11.png}
	\end{center}
The TR is the \textbf{training set}, VL is the \textbf{validation}/selection \textbf{set} (used to perform the \textbf{model selection}) and the TS is the \textbf{test set} (used for the model assessment).\\
If the test set is used repeatedly in the design cycle we would be doing model selection, and not reliable assessment. \textbf{Blind test set} concept: \textit{if you see the solution it's not a test}. In that case the test error would be an overoptimistic evaluation of the true test error. It's very easy to obtain very high classification accuracy over random tasks even when using the test set only implicitly.
\begin{center}
	\includegraphics[scale=0.5]{12.png}
\end{center}
\paragraph{Grid Search} For the model selection: hyperparameters can be set by searching in a hyperparameters space. Try every hyperparameter-value combination and record the result. The best result will corresponds to the best hyperparameter values.\\
The cost can be high (cartesian product between sets of values per each hyperparameter: what if 6 hyperparameters each with tens of possible different values?), so can be useful to fix some hyperparameter value before: more levels of grid search, where we do a first coarse grid search to find good intervals of values, then a finer grid-search can be performed over smaller intervals.
\paragraph{K-fold Cross Validation} Split the data set $D$ into $k$ mutually exclusive subsets $D_1,\ldots, D_k$, we train on $D - D_i$ and test it on $D_i$. No unique model, but we get a variance (std. dev.) over different folds for the class of models or algorithm.
\begin{center}
	\includegraphics[scale=0.5]{3.png}
\end{center}
The issue is to find $k$ and that it can be very computationally expensive but in can be combined with validation set,  double-$k$-fold cross-validation\ldots
\paragraph{Double cross-validation} After dividing the data set into $k$ mutually exclusive subsets $D_1,\ldots, D_k$, for each $D_i$: do another \textbf{internal} cross-validation using the other $k-1$ subsets.
\begin{center}
	\includegraphics[scale=0.75]{13.png}
\end{center}
It doesn't provide a model but an estimation of the risk, because can provide a different model per external step cycle.
\paragraph{Example with $k$-fold CV} Split data in TR and TS.\\
For the \textbf{model selection}, use $k$-fold CV over TR, obtaining new TR and VL in each fold, to find the best hyperparameters of the model.\\
\textbf{Train} the final model on the whole, original, TR.\\
Perform the \textbf{model assessment} by evaluating it on the external TS.\\\\
There are more combination and more ways of doing this.
\paragraph{Particular cases} 
\subparagraph{Lucky/unlucky sampling} Can we avoid to be sensible to the particular partitioning of the examples?
\begin{list}{}{}
	\item \textbf{Stratification}: the process of grouping examples into relatively homogeneous subgroups. For example, partitioning in a way that every class (in a classification task) is represented in approximately the same proportions as in the full data set.\\
	\item  \textbf{Repeated} hold-out or $k$-fold CV: repeating the splitting with different random sampling, average the results to yield an overall estimation.
\end{list}
\subparagraph{Very few data} In this case it's difficult to say if the sampling is representative or not. We can use stratification.\\
To avoid (or to be considered during evaluation): missing classes or features in training data, special classes, known outliers that can affect the results\ldots\\
Also the blind test set can be misleading if: is from a different distribution, measured with a different scale/tolerance, uncleaned, unprocessed\ldots
\subparagraph{When to stop?} Best to avoid stopping the NN training by fixing an arbitrary number of epochs: if it's too small it may be to early (underfitting), too high may be too late (overfitting), may not old for all the configuration in a cross-validation\ldots\\
Also selecting the number of epochs by model selection it's not the best practice: better than a fixed number, but still not accurate.
\subparagraph{Early Stopping} Should be part of the model selection as well. Tricky: uses part of the data just to decide when to stop, also complex with CV.\\
How to select the best model with early stopping for the final retraining (ex: different stop point for each validation set in a CV)? You could take the average number of epochs, but varying the data for retraining after model selection could lead to a different (best) stop point. So it's better to consider the average of the TR error at the best point and use it to gain the same level of fitting on the retraining. Else you need a VL every time you train.
\subparagraph{Random initialization} Different $w_{initial} \Rightarrow$ different models: how to chose one for the final model? You can compute the mean and variance of error/accuracy across the different trials, so that a random case would be in this range.
\section{Statistical Learning Theory}
\subsection{VC-dim} The VC-dim is used to provide the analytical bound we need to evaluate $H$. It's a measure of complexity (that is to say, capacity/expressive power/flexibility) of a class of hypothesis.\\
The bound is expressed not in terms of $|H|$ but in terms of distinct instances that can be completely discriminated using $H$. Intuitively (for classification): how much can $H$ discriminate points? The maximum number of points that can be correctly classified/learned without error for all the possible labelings.
\paragraph{Shattering} Given $X$ instance/input space, $n$ size of the instance space (number of instances) and $l$ for the number of data available. $H$ is the hypothesis space.\\
In the case of binary classification, there are $2^n$ possible \textbf{dichotomies} (partitions or labeling of the $n$ points in $\{-1,+1\}$): a particular dichotomy is represented in $H$ if there exists a hypothesis $h\in H$ that realizes the dichotomy.\\
\textbf{Definition}: $H$ shatters $X \Leftrightarrow H$ can represent all the possible dichotomies on $X$ (0 errors). the points in $X$ can be separated by an $h\in H$ in al the possible ways, and for every possible dichotomy of $X$ there exists a consistent hypothesis $h\in H$
\subparagraph{Example} \begin{multicols}{2}
\begin{center}
	\includegraphics[scale=0.5]{14.png}
\end{center}
\columnbreak
3 points in $R^2$, $H$ as a set of lines: $h(x) = sign(wx + w_0)$ with $x\in R^2$\\
A dichotomy is a particular labeling in $\{-1,+1\}$ of the points. This specific dichotomy can be represented in $H$: there exist a line that correctly separate the points (in pic)
\end{multicols}
\paragraph{VC-Dimension} \textbf{Definition}: the VC dimension of a class of functions $H$ is the maximum cardinality of a set (configuration) of points in $X$ that can be shattered by $H$.\\
$VC(H) = p \Rightarrow H$ shatters \textbf{at least one} set of $p$ points $\wedge H$ \textbf{cannot shatter any} set of $p+1$ points.\\
If arbitrarily large but finite sets of $X$ can be shattered by $H$ then $VC(H) = \infty$\\\\
$VC(H)\geq 3$ shown before, note that not all the possible configurations of $3$ points can be shattered (example follows), but it's sufficient to find one configuration of three points which is separable for every labeling.
\begin{center}
	\includegraphics[scale=0.5]{15.png}
\end{center}
VC-dim is related to the number of parameters, but it's not the same thing: we may add redundant free parameters, for example, and there exists models with one parameter and infinite VC-dim. For example, $k$-NN has infinite VC-dim.
\paragraph{Analytical Bound} With $N$ number of data $$R[h]\leq R_{emp}[h] + \epsilon\left(\hbox{VC-dim}, N, \delta\right)$$
with:
\begin{list}{}{}
	\item \textbf{Guaranteeded risk} $R_{emp}[h] + \epsilon\left(\hbox{VC-dim}, N, \delta\right)$
	\item \textbf{VC Confidence} $\epsilon\left(\hbox{VC-dim}, N, \delta\right)$
\end{list}
For example, for 0/1 loss $$\epsilon\left(\hbox{VC-dim}, N, \delta\right) = \sqrt{\frac{VC\left(\ln\frac{2N}{\hbox{VC-dim}}\right) - \ln\frac{\delta}{4}}{N}}$$ with probability at least $1-\delta$ for every VC-dim$< N$\\\\
There are different bound formulations for different classes of functions, of tasks\ldots\\
This gives us a way to estimate the error on future data based only on the training error and the VC-dim of $H$. The resulting bounds are the worst case scenario, because the hold for all but $1-\delta$ of the possible approximation function/training sets.
\paragraph{Remarks} For many reasonable hypothesis classes (ex: linear approximators), the VC-dim is linear in the number of free parameters of the hypothesis. This shows that to learn "well" we need a number of examples that is linear in the VC-dim.
\subsection{Structural Risk Minimization} SRM uses VC-dim as a controlling parameter for minimizing the generalization bound on $R$. Assuming finite VC-dim, we can define a nested structure of models-hypothesis spaces according to the VC-dim in the following way:\begin{list}{}{}
	\item $H_1\subseteq H_2\subseteq \ldots\subseteq H_n$
	\item $VC(H_1)\leq VC(H_2)\leq \ldots\leq VC(H_n)$
\end{list}
Some examples:
\begin{list}{}{}
	\item Neural Networks with increasing number of hidden units, but also the number of epochs
	\item Polynomial of increasing degree
	\item Increasing values for $c$, in $||w||<c$ for regularization
	\item Increasing number of nodes in a decision tree
\end{list}
\paragraph{Model selection} Growing VC-dim: empirical (training) error decreasing, VC confidence increasing.\\
SRM: find a trade-off in the bound and choose the model ($h$) with the best bound on the true risk.
\begin{center}
	\includegraphics[scale=0.5]{16.png}
\end{center}
\paragraph{Use of the bound} Provide a fundamental theoretical ground for principled ML: independently from specific model details or learning algorithms, highlighting the role of complexity control. The optimal choice of the model complexity (structure) provides the minimum expected risk (\textbf{inductive principle of SRM})\\
Also to provide a direction for new model development guided by SRM. As estimation of predictive errors is rarely used: the upper bound is overly pessimistic and may not be adequate for reliable evaluation of the generalization error (model assessment) and tighter bounds are under development, also is difficult to compute the VC-dim for specific classes of $H$.\\
Towards \textbf{principled approaches} less based on trial and error. For example, two practical approaches:
\begin{list}{}{}
	\item Choose appropriate structure/complexity, fix the model (hence the VC-dim) and minimize the TR error.\\
	Can be used in neural networks, however regularization by training heuristic can further introduce implicit SRM (early stopping\ldots Or SRM with Tikhonov regularization where we have minimum loss with $R_{emp} +$ complexity term, considering both the terms.
	\item Fix the TR error, automatically minimize the VC confidence (SVM)
\end{list}
\section{Support Vector Machines}
Linear machine, with maximization of the separation margin and structural risk minimization. Initially \textbf{hard margin SVM}, we assume to deal with linearly separable problems without errors in the data.\\
Example of linearly and non linearly separable problems:
\begin{center}
	\includegraphics[scale=0.5]{17.png}
\end{center}
\paragraph{Separating hyperplane} Given the training set $T = \{(x_i, d_i)\}_{i=1}^N$ we want to find an hyperplane of equation $w^Tx + b = 0$ to separate the examples and get $w^Tx_i + b \geq 0$ for $d_i = +1$ and $w^Tx_i + b < 0$ for $d_i = -1$\\
$g(x) = w^Tx + b$ is the discriminant function and $h(x) = sign(g(x))$ is the hypothesis.\\
An example of separation margin:
\begin{center}
	\includegraphics[scale=0.5]{18.png}
\end{center}
In this case the hyperplane has equal distance to both the closest negative and positive examples. The separation margin ($p$) is evaluated as the double of the distance between the linear hyperplane and the closest data point (a "safe zone"). Not all hyperplanes solving the task are equal: the margin changes, bigger or smaller. The \textbf{optimal hyperplane is the hyperplane which maximizes $p$} $w_O^Tx + b_O = 0$ with $_O$ being "optimal".\\
We will find $p = \frac{2}{||w||}$, so maximize $p\Leftrightarrow$ minimize $||w||$.
\paragraph{Support Vector} We can rescale $w$ and $b$ so that the closest points to the separating hyperplane statisfy $|g(x_i)| = |w^Tx_i + b| = 1$, so we can write $$w^Tx_i + b \geq 1\hbox{ if }d_i = +1$$ $$w^Tx_i + b < 1\hbox{ if }d_i = -1$$ which is compact form is $$d_i(w^Tx_i + b) \geq 1\:\:\forall\:i=1,\ldots,N$$
A \textbf{support vector} $x^{(s)}$ satisfies the previous equation exactly $$d^{(s)}(w^Tx^{(s)}+b) = 1$$ so the \textbf{support vectors are the closest data points to the hyperplane} and "lay on the margin".
\begin{center}
	\includegraphics[scale=0.5]{19.png}
\end{center}
\pagebreak
\paragraph{Computing the distance} With the discriminant function $g(x) = w^Tx + b$, recalling that $w_O$ is a vector orthogonal to the hyperplane. Let's denote with $r$ the distance between $x$ and the optimal hyperplane.
\begin{center}
	\includegraphics[scale=0.5]{20.png}
\end{center}
$$x=x_p+r\frac{w_O}{||w_O||}$$
$$g(x) = g\left(x_p + r\frac{w_O}{||w_O||}\right) ) w_O^Tx_o + b_O + w_O^Tr\frac{w_O}{||w_O||} = g(x_p) + rw_O^T\frac{w_O}{||w_O||} = r\frac{||w_O||^2}{||w_O||} = r||w_O||$$
$$\hbox{thus }r=\frac{g(x)}{||w_O||}$$
\paragraph{Computing the margin} Consider the distance between the hyperplane and a positive support vector $x^{(s)}$, for example $$r\hbox{ for }x^{(s)} = \frac{g(x^{(s)})}{||w_O||} = \frac{1}{||w_O||} = \frac{p}{2}$$
$$\Rightarrow p = \frac{2}{||w_O||}$$
So the optimum hyperplane maximizes $p\Leftrightarrow$ minimizes $||w||$
\paragraph{Quadratic Optimization Problem} The formal derivation of the SVM solution requires techniques in the constrained optimization framework, and we assume that quadratic programming is solved elsewhere.\\
For the hard margin SVM, the quadratic optimization problem asks to find the optimum values of $w$ and $b$ in order to maximize the margin.
\subparagraph{Primal Form} Given the training examples $T = \{(x_i, d_i)\}_{i=1}^N$, find the optimum values of $w$ and $b$ which minimizes $$\psi(w) = \frac{1}{2}w^Tw$$ satisfying the constraints $$d_i(w^Tx_i+b)\geq 1\:\:\:\forall\:i=1,\ldots,N$$
The objective function $\psi(w)$ is quadratic and convex in $w$, while the constraints are linear in $w$: solving this problem scales (the computational cost) with the size of the input space $m$.\\
The Lagrangian multipliers method is used: constructing the Lagrangian function corresponding to the quadratic optimization problem $$J(w, b,\alpha) = \frac{1}{2} w^Tw - \sum_{i=1}^N\alpha_i\left(d_i\left(w^Tx_i+b\right)-1\right)$$ with $\alpha_i \geq 0$ the $N$ Lagrangian multipliers. Each term in the sum correspond to a constraint in the primal problem. $J$ must be minimized with respect to $w$ and $b$, and maximized with respect to $\alpha$, and the solution correspond to a saddle point in $J$.\\\\
We will find $$w_O = \sum_{i=1}^N \alpha_{O,i}d_ix_i$$ thus the optimal hyperplane is expressed as $$w^T_Ox+b_O = 0 \Leftrightarrow \sum_{i=1}^N\alpha_{O,i}d_ix_i^Tx + b_O = 0$$
The optimal conditions will be:
\begin{list}{}{}
	\item Minimize $J$ with respect to $w \Rightarrow\frac{\partial J}{\partial w} = 0 \Rightarrow w = \sum_{i=1}^N \alpha_id_ix_i$
	\item Minimize $J$ with respect to $b \Rightarrow\frac{\partial J}{\partial b} = 0 \Rightarrow w = \sum_{i=1}^N \alpha_id_i$
\end{list}
And we may substitute these in $J$ to study the dual form.
\subparagraph{Kuhn-Tucker Conditions} From the KT conditions follows that $$\alpha_i(d_i(w^Tx_i + b) - 1) = 0\:\:\:\:\forall\:i=1,\ldots, N$$ in the saddle point of $J$:
\begin{list}{}{}
	\item $\alpha_i > 0 \Rightarrow d_i(w^Tx_i+b) = 1$ and $x_i$ is a support vector
	\item $x_i$ isn't a support vector $\Rightarrow \alpha_i = 0$
\end{list}
Hence we can restrict the computation to $N_s$ so $w_O=\sum_{i=1}^{N_s} \alpha_{O,i}d_ix_i$: \textbf{the hyperplane depends solely on the support vectors}!\\\\
To obtain the Lagrangian multipliers $\{a_i\}_{i=1}^N$ we must solve the dual form: given the training examples $T = \{(x_i, d_i)\}_{i=1}^N$, find the optimum values of $\{a_i\}_{i=1}^N$ that maximizes $$Q(\alpha) = \sum_{i=1}^N \alpha_i - \frac{1}{2}\sum_{i=1}^N\sum_{j=1}^N\alpha_i\alpha_jd_id_jx_i^Tx_j$$ satisfying the constraints $$\alpha_i\geq 0\:\:\:\forall\:i=1,\ldots,N$$ $$\sum_{i=1}^N\alpha_id_i = 0$$
Then we compute $w_O = \sum_{i=1}^N\alpha_{O,i}d_ix_i$ and $b_O = 1-w_O^Tx^{(s)}$ corresponding to a positive support vector $x^{(s)}$, so $$b_O = = 1-\sum_{i=1}^N\alpha_{O,i}d_ix_i^Tx^{(s)}$$
So we don't need to explicitly compute $w_O$, but we only need the Lagrangian multipliers $\alpha_i$ (by solving the dual problem), then we compute the optimal bias $b_O$. The decision surface is given by $$w_O^Tx + b_O = 0 \Leftrightarrow\sum_{i=1}^N\alpha_{O,i}d_i x_i^Tx + b_O = 0$$
So given the input pattern $x$:
\begin{list}{}{}
	\item Compute $g(x) = \sum_{i=1}^N \alpha_{O,i}d_ix_i^Tx + b_O$
	\item Classify $x$ as the sign of $g(x)$
\end{list}
Note that it's not necessary to compute $w_O$, also the sum can be restricted to the number of support vector $N_s$
\subparagraph{How does this improve the generalization?} Minimizing the norm of $w$ is equivalent to minimizing the VC-dim and thus to minimizing the VC confidence $\epsilon$ in $$R[h] \leq R_{emp}[h] + \epsilon(\hbox{VC-dim}, N, \delta)$$
\subparagraph{Theorem (Vapnik)} Let $D$ be the diameter of the smallest ball around the data points $x_1,\ldots,x_N$. For the class of separating hyperplanes described by the equation $w^Tx+b = 0$, the upper bound to the VC-dim is $$\hbox{VC-dim} \leq min(\lceil\frac{D^2}{p^2}\rceil, m_O) + 1$$
\paragraph{An elegant approach} For linearly separable data there are many solutions. Vapnik proposed an "optimal separating hyperplane" maximizing the margin providing:
\begin{list}{}{}
	\item An unique solution with zero errors for the binary classifier
	\item An automatized approach to SRM that minimizes VC confidence (by maximizing the margin) as part of the training process, without hyper parameters in the linear separable case.
	\item The use of a solver in the class of constrained quadratic programming (instead of gradient descent) with a dual form (showing support vectors and dot product among the patterns)
	\item A solution focused on "selected" training data (support vectors)
\end{list}
For noisy or not linearly separable data, you can have a soft margin (support vector still on the border, but some data may fall closer to the hyperplane: at least one point violate $d_i(w^Tx_i+b)\geq 1$)
\paragraph{Soft margin SVM} We introduce $\xi_i\geq 0\:\:\:\forall\:i=1,\ldots, N$ called \textbf{slack variables}: $d_i(w^Tx_i+b)\geq 1-\xi_i$, so a support vector satisfies that exactly $d_i(w^Tx_i+b)\geq 1-\xi_i$
\begin{center}
	\includegraphics[scale=0.5]{21.png}
\end{center}
Note that Vapnik does not hold anymore. The primal problems becomes: given the training examples $T = \{(x_i, d_i)\}_{i=1}^N$, find the optimum values of $w$ and $b$ which minimizes $$\psi(w, \xi) = \frac{1}{2}w^Tw + C\sum_{i=1}^N\xi_i$$ satisfying the constraints $$d_i(w^Tx_i+b)\geq 1\:\:\:\forall\:i=1,\ldots,N$$ $$\xi_i\geq 0\:\:\:\forall\:i=1,	\ldots, N$$
We introduced $C$ as a regularization hyperparameter, losing the fully automated SRM. Low $C$ means many TR errors allowed (possible underfitting), while high $C$ means less or no TR errors allowed (smaller margin, possible overfitting)
\subparagraph{Kuhn-Tucker Conditions} The KT conditions are now defined as
\begin{list}{}{}
	\item $\forall\:i=1,\ldots,N\:\:\:\alpha_i(d_i(w^Tx_i + b)+\xi_i - 1) = 0$
	\item $\forall\:i=1,\ldots,N\:\:\:\mu_i\xi_i=0$ with $\mu_i$ Lagrangian multipliers introduced to enforce the non negativity of the slack variables.
\end{list}
We have $0<\alpha_i<C\Rightarrow\xi_i = 0$ on the margin and $\alpha_i=C\Rightarrow\xi_i > 0$ inside the margin.
\subparagraph{Solving the problem} We solve the dual problem to compute the $\{\alpha_i\}_{i=1}^N$, then we find $w_O$ and $b_O$ from $\{\alpha_{O,i}\}_{i=1}^N$ with $w_O = \sum_{i=1}^N\alpha_{O,i}d_ix_i$\\
Then, for a pattern $j$ such that $0<\alpha_j<C$, we have $$b_O = d_j-\sum_{i=1}^N\alpha_{O,i}d_ix_i^Tx_j$$
or an average of all the solutions for numerical stability. The non-zero Lagrangian multipliers correspond the the support vectors. We use it as before: $g(x) = \sum_{i=1}^N \alpha_{O,i}d_ix_iTx + b_O$ and $h(x) = sign(g(x))$
\subsection{High-Dimensional feature spaces} We can use:
\begin{list}{}{}
	\item Non-linear mapping of input patterns to a high-dimensional feature space\\
	\textbf{Cover's Theorem}: the patterns are linearly separable with high probability in the feature space under such conditions.
	\item Finding the optimal hyperplane to separate the patterns in the feature space.
\end{list}
However we know that using high dimensional feature spaces (large basis function expansion) can be computationally unfeasible and can lead to overfitting.\\
We will propose the kernel approach to implicitly manage the feature space while regularizing.
\paragraph{Kernel} Non linear function mapping $$\phi:R^{m_0}\rightarrow R^{m_1}$$ $$x\mapsto \phi(x)$$
The problem is formulated as before, but the training set is now $\{(\phi(x_i), d_i)\}_{i=1}^N$ and the hyper plane is now $w^T\phi(x) + b = 0$\\
The weight vector is a linear combination of the feature vectors
$$w=\sum_{i=1}^N\alpha_id_i\phi(x_i)$$ so the hyperplane equation is $$\sum_{i=1}^N\alpha_id_i\phi(x_i)^T\phi(x) = 0$$
Evaluating $\phi(x)$ could be intractable, but with certain conditions we don't need to evaluate it directly. We do not even need to know the feature space itself! This is possible using a function to compute directly the dot products in the feature spaces $k:R^{m_0}\times R^{m_0}\rightarrow R$, with $k$ called inner product kernel function: $k(x_i, x) = \phi(x_i)^T\phi(x)$, and it's symmetric meaning $k(x_i, x) = k(x, x_i)$\\
\subparagraph{Kernel Matrix} We can arrange the dot products in the feature space between the images of the input training patterns in a $N\times N$ matrix called kernel matrix $K = \{k(x_i, x_j)\}_{i,j=1}^N$, symmetrical.
\subparagraph{Mercer's Theorem} This property holds only for kernels with positive semi-definite kernel matrices. It's related to having non-negative eigenvalues in the kernel matrix.
\subparagraph{Properties} With $k_1,k_2$ kernels over $R^{m_0}\rightarrow R^{m_0}$, the following are also kernel functions:
\begin{list}{}{}
	\item $k_1(x,y)+k_2(x,y)$
	\item $\alpha k_1(x,y)$ with $\alpha\in R^+$
	\item $k_1(x,y)\cdot k_2(x,y)$
\end{list}
\paragraph{Wrapping up}
\begin{list}{}{}
	\item The training set is $T = \{(x_i,d_i)\}_{i=1}^N$
	\item We can choose the trade-off parameter $C$ and the kernel function $k$
	\item We find $\{\alpha_i\}_{i=1}^N$ by solving the optimization problem via quadratic programming algorithms\\
	Remember that the solution is spars, as every $\alpha_i$ corresponding to non-support vector is $0$
	\item The bias $b_O$ is computed knowing the Lagrangian multipliers and the kernel matrix
	\item Given an input pattern $x$ we compute $w^T\phi(x) = \sum_{i=1}^N\alpha_id_ik(x,x_i)$\\
	Fundamental: we don't need to compute $w$
	\item $x$ is classified as $sign(g(x)) = sign\left(\sum_{i=1}^N\alpha_id_ik(x,x_i)\right)$ with $g(x)$ called discriminant function
\end{list}
At \textbf{test phase} we have the Lagrangian multipliers and the kernel matrix. To classify an unseen input pattern $x$, we compute $\sum_{i=1}^N\alpha_id_ik(x,x_i)$ and classify $x$ and the sign of that computation, so $h(x) = sign\left(\sum_{i=1}^N\alpha_id_ik(x,x_i)\right)$
\paragraph{Examples of kernels}
\begin{list}{}{}
	\item \textbf{Polynomial Learning Machine} $k(x,x_i) = (x^Tx_i + 1)^p$ with $p$ user-specified parameter
	\item \textbf{Radial Basis Function} or \textbf{Gaussian Kernel} $k(x,x_i) = e^{-\frac{1}{2\sigma^2}||x-x_i||^2}$ with $\sigma^2$ user-specified parameter.\\
	Narrow peaked kernels with small $\sigma$, imply that the reply for $x_i$ is only $d_i$\\
	Feature space with an infinite number of dimensions
	\item \textbf{Two-layer perceptron} $k(x,x_i) = tanh(\beta_0x^Tx_i + \beta_1)$ where $\beta_0 > 0$ and $\beta_1 < 0$ are user-specified parameters\\
	Here the Mercer's Theorem holds only for some choices of $\beta_0,\beta_1$
\end{list}
\subsection{SVM for non-linear regression}
A regression problem requires to find $f$ such that $d=f(x)+v$ with a training set $T=\{(x_i,d_i)\}_{i=1}^N$ and with $v$ statistically independent from $x$\\
We estimate $d$ using a linear expansion of non-linear functions $\{\phi_j(x)\}_{j=0}^{m_1}$ so $y=h(x) = w^T\phi(x)$ where $w = (w_0 = b, w_1,\ldots,w_{m_1})^T$ and $\phi(x) = (\phi(x)_0 = 1, \phi(x)_1,\ldots, \phi(x)_{m_1})^T$
\paragraph{$\epsilon$-insensitive loss function} $L_\epsilon(d,y) = \left\{\begin{array}{l l}
|d - y| - \epsilon & \hbox{if }|d-y|\geq\epsilon\\
0 & \hbox{otherwhise}
\end{array} \right.$
\begin{center}
	\includegraphics[scale=0.5]{22.png}
\end{center}
\paragraph{Optimization problem} Introducing the non-negative slack variables $\xi_i'$ and $\xi_i\forall\:i=1,\ldots,N$ $$-\xi_i'-\epsilon \leq d_i-w^T\phi(x_i)\leq \epsilon + \xi_i$$
leading to the following constraints, all $\forall\:i=1,\ldots,N$
\begin{list}{}{}
	\item $d_i - w^T\phi(x_i)\leq \epsilon + \xi_i$
	\item $w^T\phi(x_i)-d_i\leq \epsilon + \xi_i'$
	\item $\xi_i\geq 0$
	\item $\xi_i'\geq 0$
\end{list}
The primal problem then is: given the training set $\{(x_i, d_i)\}_{i=1}^N$ find the optimal values of $w$ such that the following objective function is minimized $$\psi(w,\xi,\xi') = \frac{1}{2}w^Tw + C\sum_{i=1}^N\left(\xi_i+\xi_i'\right)$$ under the constraints, all $\forall\:i=1,\ldots,N$
\begin{list}{}{}
	\item $d_i - w^T\phi(x_i)\leq \epsilon + \xi_i$
	\item $w^T\phi(x_i)-d_i\leq \epsilon + \xi_i'$
	\item $\xi_i\geq 0$
	\item $\xi_i'\geq 0$
\end{list}
The dual problem yields the optimal $\{\alpha_i\}_{i=1}^N$ and $\{\alpha_i'\}_{i=1}^N$. With those, we can compute the optimal $w$ $$w=\sum_{i=1}^N(\alpha_i - \alpha_i')\phi(x_i) = \sum_{i=1}^N\gamma_i\phi(x_i)$$
with $\gamma_i = \alpha_i - \alpha_i'$: in this case support vectors correspond to non-zero values of $\gamma_i$\\
The estimate is defined as $h(x) = y = w^T\phi(x)$, and using the linear expansion for $w$ we get $$h(x) = \sum_{i=1}^N\gamma_i\phi(x_i)^T\phi(x) = \sum_{i=1}^N\gamma_i k(x_i, x)$$
\paragraph{Wrapping up}
\begin{list}{}{}
	\item \textbf{Important}: select values for the user-specified parameters $C$ and $\epsilon$
	\item \textbf{Important}: choose an inner product kernel function $k$
	\item Compute the kernel matrix $K$
	\item Solve the dual form and get the optimal values of the Lagrangian multipliers ($\{\gamma_i\}_{i=1}^N$)
	\item Compute the optimal value for the bias ($b$)
	\item Obtain the estimate function as a linear combination of dot products in a feature space we can ignore ($h(x) = \sum_{i=1}^N \gamma_ik(x_i, x)$)
\end{list}
The test: given an input pattern $x$, we estimate the value of the unknown function $f$ in that point using the estimate computed before $$h(x) = \sum_{i=1}^N\gamma_i k(x_i,x)$$
\paragraph{Summary of the main characteristics}
\subparagraph{Pros}
\begin{list}{}{}
	\item The regularization is embedded in the optimization problem (margin)
	\item Approximation of the theoretical structure risk minimization
	\item Convex problem (\textbf{training always finds a global minimum})
	\item Implicit feature transformation using kernels
\end{list}
\subparagraph{Cons}
\begin{list}{}{}
	\item Must choose the kernel and its parameters
	\item It's a batch algorithm
	\item Very large problems are computationally intractable\\
	Problems with more than 20000 examples are very difficult to solve with standard approaches. However many solution are proposed (including gradient descent approaches)
\end{list}
\subparagraph{In practice} There is no theory which guarantees that a given family of SVMs will have high accuracy on a given problem.\\
The nice property of hard-margin SVMs cannot be directly extended to soft-margin and kernels: the $C$ parameter and kernels can lead to infinite VC-dim of the SVM classifier.\\
Gaussian RBF SVMs of sufficiently small width can classify an arbitrarily large number of training points correctly (only 1 SV point, the closest one, will contribute to the solution), thus have infinite VC-dim. On the opposite, with large width of the gaussian all SV points are considered and you get a sort of "global average", low VC-dim.\\
$\Rightarrow$ controlling the width is controlling the VC-dim.\\\\
Rigorous selection of the kernel and the $C$ requires an estimation of the complexity (VC-dim). Hyperparameters affecting model complexity are used for model selection. In practice a careful empirical evaluation.\\\\
Nowadays, deep neural networks have largely outperformed previous records of SVMs on image and speech recognition tasks.
\subsection{Kernel Methods}
The kernel trick introduces the kernel methods, also without SVM. \textbf{Kernelization} of previous approaches, whenever you had a dot product inside your model or a similar measure enabling them to operate in a new implicit high-dimensional space just by specifying a kernel (and changing it: \textbf{the $K$ can change without changing the learning machine}.
\begin{center}
	\includegraphics[scale=0.75]{23.png}
\end{center}
An SVM is largely characterized by the kernel: the best choice of kernel for a given problem is still a research issue.
\section{Bias-Variance} A training set is only one possible realizations from the universe of the data: different TR sets can provide different estimate. The expected error (on various TR) at a point $x$ is decomposed as:
\begin{list}{}{}
	\item \textbf{Bias}: quantify the discrepancy between true function and $h(x)$ (averaged on data)
	\item \textbf{Variance}: quantify the variability of the response of model $h$ for different realizations of the TR data
	\item \textbf{Noise}: error in the label
\end{list}
Let's assume the regression scenario with target $y$ and $L_2$ squared error loss. Suppose we have examples $(x,y)$ where the true function is $y = f(x) + \epsilon$ with $\epsilon$ being Gaussian noise with zero mean and std. dev. $\sigma$. In linear regression, given a set of examples $(x_i,y_i)$ with $i = 1,\ldots,l$, we fit a linear hypothesis $h(x) = wx + w_0$ to minimize sum-squared error over the training data $\sum_{i=1}^l (y_i - h(x_i))^2$\\
Because of the hypothesis class that we chose for some function $f$ (linear hypothesis), we have a \textbf{systematic prediction error}. Depending on the dataset that we have, the parameters $w$ that we find will be different.\\\\
Given a new data point $x$, what is the expected prediction error? Assume that the data points are drawn independent and identical distributed from a unique underlying probability distribution $P$. The goal is to compute, for an arbitrary new point $x$, $E_P[(y - h(x))^2]$ noting that there's a different $h$ and $y$ for each different "extracted" training set. $y$ is the value for $x$ that could be present in a data set, and the expectation is over \textbf{all training set} that are drawn according to $P$. We will decompose this expectation into three components: bias, variance and noise.
\subparagraph{Recall of statistics} With $Z$ random variable of possible values $z_i$ with $i=1,\ldots,l$ and probability distribution $P(Z)$
\begin{list}{}{}
	\item \textbf{Expected value} or \textbf{mean} of $Z$ is $$\overline{Z} = E_P[Z] = \sum_{i=1}^l z_i\cdot P(z_i)$$ with the sum replaced by an integral and the distribution by a density function if $Z$ is continuous.
	\item \textbf{Variance} of $Z$ is $$Var[Z] = E[(Z - E[Z])^2] = E[Z^2] - E[Z]^2$$ with $$E[Z^2] = E[Z]^2 + Var(Z)$$
\end{list}
\subsection{Bias-Variance Decomposition}
$$E_P[(y - h(x))^2] = E_P[h(x)^2 - 2yh(x) + y^2] = E_P[h(x)^2] + E_P[y^2] - 2E_P[y]E_P[h(x)]$$
Let $\overline{h}(x) = E_P[h(x)]$ denote the \textbf{mean prediction} of the hypothesis at $x$ when $h$ is trained with data drawn from $P$. By doing some calculations, we obtain $\begin{array}{r c l l}
E_P[(y-h(x))^2] & = & E_P[(h(x) - E_P[h(x)])^2] + & \hbox{variance}\\
 & & (E_P[h(x)] - f(x))^2 + & \hbox{bias}^2 \\
 & & E_P[(y - f(x))^2] & \hbox{noise}^2
\end{array}$\\
So expected error is Variance + Bias$^2$ + Noise$^2$\begin{list}{}{}
	\item \textbf{Bias}: quantify the discrepancy between true function and $h(x)$, with $h(x)$ averaged over different TR data (\textbf{systematic error})\\E.g. due to too small $H$, too rigid model.
	\item \textbf{Variance}: quantify the variability of the response of model $h$ for different realizations of the training data.\\Due to too high flexibility
	\item \textbf{Noise}: event the optimal solution can be wrong.\\E.g. if for a give n$x$ there are more than one possible $d$.\\
	It's irreducible, doesn't depend on the model.
\end{list}
\begin{center}
	\includegraphics[scale=0.5]{24.png}
\end{center}
\section{Ensemble Learning}
\begin{center}
	\includegraphics[scale=0.5]{25.png}
\end{center}
Take advantage of multiple models. In regression: simple average committee, mean square error of a committee\ldots In classification, take a vote over many classifier.
\subsection{Bagging} \paragraph{Bootstrap Aggregating} Combine many classifiers. Train $n$ classifiers on different subsets of TR and differentiate each training using bootstrap (resampling with replacement). Thing to bias-variance: high variance model can perform well on average. In other therms: the average of $h$ reduce the variance. In regression use the mean, in classification use the vote of the $n$ classifiers.
\subsection{Boosting} If the models have the same errors we have no advantage on esembling.\\
Differentiate each training concentrating on errors (more weight to difficult instances, e.g. more likely to be included in the TR of the next classifier, for example: train the 1st classifier, then train the 2nd with more weight on the instances misclassified before\ldots\\
Combine the results by output weights (weighted vote for classifiers, with more weight to low error classifiers).\\
If not stopped, boosting will learn to classify correctly all instances from TR, providing an incremental approach to construct complex models. Can suffer from noisy data (which are weighted more)
\subsection{Feature Selection} Find a selection of features that are more informative for the problem at hands. Benefits for: dimensionality reduction and filtering of irrelevant information and noise, interpretability\ldots\\
Computationally hard (many possible subsets of features, retraining\ldots) typically an heuristic search of the best subset by greedy or other optimization techniques.
\end{document}