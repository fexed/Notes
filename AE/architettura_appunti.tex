\documentclass[10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{textcomp}
\graphicspath{ {./img/} }
\usepackage{listings}
\usepackage{makecell}
\lstdefinestyle{customasm}{
  belowcaptionskip=1\baselineskip,
  frame=line,
  xleftmargin=\parindent,
  language=[x86masm]Assembler,
  basicstyle=\ttfamily,
  commentstyle=\itshape\color{purple!40!black},
}
\lstset{escapechar=@,style=customasm}
\lstnewenvironment{C}
  {\lstset{language=C++,frame=none}}
  {}
\begin{document}
\title{Architettura degli Elaboratori}
\author{Appunti: Simone Pepi\\Stesura in \LaTeX: Federico Matteoni}
\date{ }
\renewcommand*\contentsname{Indice}

\maketitle
\begin{multicols}{2}
\tableofcontents
\end{multicols}
\pagebreak
\section{Introduzione}
Appunti del corso di \textbf{Architettura degli Elaboratori} a cura di \textbf{Federico Matteoni} e \textbf{Simone Pepi}.\\\\
Prof.: \textbf{Maurizio Bonuccelli}, maurizio.angelo.bonuccelli@unipi.it\\
\begin{list}{-}{Riferimenti web:}
\item \emph{http://pages.di.unipi.it/bonuccelli/aeb.html}
\item \emph{didawiki.cli.di.unipi.it/doku.php/informatica/ae/start}
\end{list}
Ricevimento: Martedì 10-12, stanza 294 DE\\
Esame: \textbf{scritto} (\textit{closed book}) e \textbf{orale}. I compitini sono validi solo per la sessione invernale (gen-feb)\\
\begin{list}{-}{Libri}
\item  M. Vanneschi \textit{Architettura degli Elaboratori}, Pisa UniversitY Press
\item D. A. Patterson \textit{Computer Organization \& Design - The Hardware/Software Interface}
\end{list}
\section{Cosa riguarda il corso}
Consiste in come sono fatti pc internamento da un punto di vista di sottosistemi senza scendere nei dettagli elettrici.
\begin{list}{-}{Il corso è diviso in quattro parti:}
\item Fondamenti e strutturazione firmware (I Compitino)
\item Macchina assembler (D-RISC) e processi
\item Architetture General-Purpose
\item Architetture parallele (II Compitino)
\end{list}
\chapter{Fondamenti di strutturazione}
\section{Struttura a livelli}
\paragraph{Dividere} Per dedicarci allo studio di un sistema complesso spesso è utile \textbf{dividerlo in pezzi}. Nel caso di un sistema di elaborazione, in alcuni casi è \textbf{interessante avere una visione vicina alla struttura fisica} in termini di componenti hardware. In altri casi è \textbf{interessante avere una visione astratta del sistema} per poterne osservare le funzionalità e le strutture più adatte alla specifica applicazione.
\paragraph{Astrarre} Da questa necessità deriva la possibilità di strutturare un sistema a vari \textbf{livelli di astrazione} che non descrivono una reale struttura fisica, ma è \textbf{utile per ragioni specifiche} quali:
\begin{list}{}{}
\item Saper riconoscere \textbf{quale metodo di progettazione strutturata} viene seguito o conviene seguire (\textbf{top-down}, \textbf{bottom-up}, \textbf{middle-out})
\item Saper riconoscere \textbf{se i vari livelli rispettano una relazione gerarchica} oppure se non esiste alcun tipo di ordinamento
\item Essere in grado di \textbf{valutare a quali livelli conviene descrivere e implementare} determinate funzioni del sistema
\end{list}
\section{Macchine Virtuali}
\paragraph{Sistema di elaborazione} \textbf{Le funzionalità di un sistema di elaborazione} nel suo complesso possono essere \textbf{ripartite su un certo numero di livelli} che vengono definite \textbf{macchine virtuali}. \begin{list}{-}{La suddivisione può seguire \textbf{due approcci} fondamentali:}
	\item \textbf{Linguistico}: stabilisce i livelli in base ai linguaggi usati
	\item \textbf{Funzionale}: stabilisce i livelli in base a cosa fanno
\end{list}
I vari livelli sono schematizzati come in figura:
\begin{center}
\rule{5cm}{1pt} Interfaccia \rule{5cm}{1pt}\\
MV$_{i}$	R$_{i}$ = {risorse} + L$_{i}$ = {linguaggi}\\
\rule{5cm}{1pt} Interfaccia \rule{5cm}{1pt}\\
MV$_{i-1}$	R$_{i-1}$ = {risorse} + L$_{i-1}$ = {linguaggi}\\
\rule{5cm}{1pt} Interfaccia \rule{5cm}{1pt}\\
\end{center}
MV$_{i}$ realizza politica P$_{i}$ con linguaggio L$_{i}$ e risorse R$_{i}$.\\
MV$_{i}$ utilizza le funzionalità che il livello MV$_{i-1}$ (cioè le sue primitive) fornisce \textbf{attraverso l'interfaccia}.\\
L'interfaccia definita è fondamentale per poter rendere possibile la collaborazione tra le macchine virtuali, e permettere così ai linguaggi di MV$_{i}$ di sfruttare funzionalità e meccanismi di MV$_{i-1}$.\\
Le macchine virtuali godono delle \textbf{seguenti proprietà}:
\begin{list}{}{}
	\item L'\textbf{insieme} degli oggetti o risorse \textbf{R}$_{i}$ di MV$_{i}$ \textbf{è accessibile soltanto da parte dei meccanismi di L}$_{i}$ 
	\item Al livello MV$_{i}$ \textbf{non sono note le politiche adottate dai livelli inferiori}
\end{list}
\paragraph{Supporto a tempo di esecuzione} Anche detto \textbf{Runtime Support}, è l'\textbf{insieme dei livelli sottostanti}.\\
Nell'esempio, MV$_{i}$ ha come runtime support i livelli MV$_{i-1}$ \ldots MV$_{0}$.
\paragraph{Virtualizzazione ed Emulazione} Con \textbf{virtualizzazione} o astrazione intendiamo il \textbf{processo secondo cui un livello MV$_{i}$ usa funzionalità dei livelli superiori}.\\
Con \textbf{emulazione} o concretizzazione intendiamo il \textbf{processo secondo cui un livello MV$_{i}$ usa funzionalità dei livelli inferiori}.
\paragraph{Modularità} Tutte queste funzionalità sono \textbf{alla base della strutturazione di sistemi con elevata modularità}, \textbf{modificabilità}, \textbf{portabilità}, \textbf{manutenibilità} e \textbf{testabilità}.
\subsection{Le Macchine Virtuali}
\begin{list}{}{}
\item \textbf{MV$_{4}$} Applicazioni\\L$_{4}$: Java, C, ML\ldots\\R$_{4}$: oggetti astratti, costrutti, tipi di dato definibili dall'utente
\item \rule{5cm}{1pt} \textit{Interfaccia}: chiamate di sistema \rule{5cm}{1pt}
\item \textbf{MV$_{3}$} Sistema Operativo\\L$_{3}$: C, linguaggi di programmazione concorrente, linguaggi sequenziali con librerie che implementano meccanismi di concorrenza\\R$_{3}$: variabili condivise, risorse condivise, oggetti astratti usati per la cooperazione tra processi e thread
\item \rule{5cm}{1pt} \textit{Interfaccia}: istruzioni assembler \rule{5cm}{1pt}
\item \textbf{MV$_{2}$} Macchina assembler\\L$_{2}$: assembler (D-RISC)\\R$_{2}$: registri, memoria, canali di comunicazione
\item \rule{5cm}{1pt} \textit{Interfaccia}: istruzioni firmware per l'assembler \rule{2.8cm}{1pt}
\item \textbf{MV$_{1}$} Firmware\\L$_{1}$: microlinguaggio\\R$_{1}$: sommatore, commutatore, registri, strutture di interconnessione intra-unità e inter-unità
\item \rule{5cm}{1pt} \textit{Interfaccia}: hardware \rule{6.7cm}{1pt}
\item \textbf{MV$_{0}$} Hardware\\L$_{0}$: \textit{funzionamento dei circuiti elettronici}\\R$_{0}$: circuiti elettronici elementari (AND, OR, NOT), collegamenti fisici, reti logiche
\end{list}
Il corso riguarderà principalmente i livelli MV$_{2}$ $\rightarrow$ MV$_{0}$ inclusi, comprese le istruzioni assembler.\\
Il livello firmware sarà fatto da \textbf{memoria}, \textbf{processore} e \textbf{dispositivi I/O}. I dispositivi di I/O comunicano bilateralmente con la memoria e il processore comunica bilateralmente con memoria. Opzionalmente, i dispositivi di I/O comunicano bilateralmente direttamente con il processore. Questa è l'\textbf{architettura standard}, presentata in maniera \textbf{estremamente semplicistica}.\\
Vedremo nel dettaglio il processore e la memoria, non i dispositivi di I/O perché troppo complessi.
\pagebreak
\subsection{Struttura Interna}
\paragraph{Da Verticale a Orizzontale} Fin'ora abbiamo parlato delle macchine virtuali in senso \textbf{verticale}, adesso vogliamo trovare un modo concettualmente uniforme -- \textbf{orizzontale} -- per poter studiare i livelli \textbf{al loro interno}.
\paragraph{Sistema di Elaborazione} Una volta scelta la struttura verticale di un sistema, dobbiamo capire come funziona l'interno di ciascun livello per poter far funzionare tutto il sistema, cioè dobbiamo capire \textbf{come funziona il sistema di elaborazione di un livello} composto da due componenti:
\begin{list}{}{}
	\item \textbf{Moduli di Elaborazione}, \textbf{ad ognuno} dei quali \textbf{è affidata l'elaborazione di un sottoinsieme di operazioni} del livello.
	\item \textbf{Struttura di Interconnessione}, \textbf{con la quale i moduli} di elaborazione del livello \textbf{cooperano e comunicano tra loro}.\\
	Essa può essere di due tipi:
\end{list}
	(\begin{center}\includegraphics[scale=0.6]{strutturainterna.png}
\end{center}
\paragraph{Moduli di Elaborazione} Un modulo di elaborazione è definito come \textbf{un'entità autonoma e sequenziale}.
\subparagraph{Autonomia} L'autonomia è \textbf{data dal fatto che ogni modulo} di elaborazione \textbf{esegue un proprio controllo in maniera indipendente da altri moduli}.\\
Esso dunque \textbf{definisce le proprie strutture dati, operazioni elementari e interfacce} verso altri moduli.
\subparagraph{Sequenzialità} La sequenzialità è \textbf{data dal fatto che ogni modulo} di elaborazione \textbf{ha un singolo luogo di controllo}: \textbf{la sua attività è descritta} da un algoritmo di controllo costituito \textbf{da una lista sequenziale di comandi}.\\
La sequenzialità \textbf{non implica che un modulo non possa fare uso di forme di elaborazione concorrenti o parallele}. Alcuni o tutti i comandi di una lista sequenziale possono essere costituiti da una o più operazioni elementari \textbf{eseguite simultaneamente}.
\subsection{Parallelismo}
\paragraph{Sovrapporre} Poiché i moduli sono autonomi fra loro, sono in grado di \textbf{operare indipendentemente l'uno dall'altro}. \textbf{Le loro attività possono quindi essere sovrapposte nel tempo} eccetto quando, per ragioni legate alla sincronizzazione, alcuni di loro devono attendere il verificarsi di certi eventi dipendenti dall'elaborazione di altri.\\
In alcuni \textbf{casi limite} seppur realistici, il \textbf{funzionamento di tutti i moduli è rigidamente sequenziale}.\\\\
Tutto questo \textbf{vale per qualsiasi livello} o Macchina Virtuale, quindi sia per firmware che hardware.
\pagebreak
\subsection{Modelli di Cooperazione}
Dato un sistema di elaborazione a un certo livello, \textbf{i vari moduli} presenti \textbf{possono cooperare secondo due modalità}:
\begin{list}{}{}
	\item \textbf{Ambiente Globale}: esiste un \textbf{insieme di oggetti comuni accessibili da tutti i moduli} che devono cooperare tra loro, e \textbf{tutti i moduli possono operare su tale insieme}
	\item \textbf{Ambiente Locale}: \textbf{i moduli non condividono nulla}, quindi \textbf{non esiste alcun oggetto condivisi tra i moduli}. La \textbf{cooperazione avviene tramite scambio di messaggi}.
\end{list}
\begin{center}
\includegraphics[scale=0.5]{modellicoop.png}
\end{center}
\pagebreak
\section{Compilazione vs Interpretazione}
\paragraph{Programmi} L'obiettivo di un calcolatore è \textbf{rendere possibile l'esecuzione di programmi} con una certa qualità di servizio. I programmi vengono \textbf{progettati mediante linguaggi di alto livello}, quindi \textbf{occorre operare una traduzione da linguaggio di alto livello a linguaggio assembler}.\\
Tale traduzione può essere effettuata tramite due ben note tecniche e loro combinazioni:
\begin{multicols}{2}
\textbf{Compilatore}: è \textbf{statico}.\\\textbf{Sostituisce l'intera sequenza del programma} sorgente con un sequenza di istruzioni assembler. Questa traduzione viene effettuata staticamente, vale a dire in fase di preparazione e \textbf{prima che il programma passi in esecuzione}.\\
Uno compilatore ha \textbf{completa visione del codice} e quindi \textbf{può ottimizzarlo}. La sua attività è analoga all'opera di un traduttore, che può leggersi il testo più volte per tradurlo alla perfezione.
\columnbreak

\textbf{Interprete}: è \textbf{dinamico}\\ Scandisce la sequenza \textbf{sostituendo ogni singolo comando} con una sequenza na di istruzioni assembler. La traduzione è effettuata dinamicamente, cioè \textbf{a tempo di esecuzione}, quindi non può ottimizzare. Il firmware riceve un'istruzione alla volta, quindi la interpreta.\\
Il suo svantaggio è che il \textbf{tempo di interpretazione viene pagato ogni volta che lancio il programma} e che \textbf{non può ottimizzare non avendo una visione globale} del programma.
\end{multicols}
Entrambe servono per tradurre il \textbf{codice sorgente} nel \textbf{programma oggetto} o \textbf{eseguibile}. L'esecuzione è quindi \textbf{più veloce in un programma compilato} rispetto ad un programma interpretato.
\begin{center}
\texttt{ADD R1, R2, R3} $\longrightarrow$ \textit{compilatore} $\longrightarrow$ \textbf{OBJ} $\longrightarrow$ Interprete Firmware (interfaccia tra MV ASM e MV FW)
\end{center}
Intuitivamente, dall'istruzione ad alto livello viene \textbf{compilato un programma oggetto OBJ} il quale è un insieme di bit che \textbf{viene interpretato dall'interprete firmware}.
\paragraph{Esempio} \textbf{Suppongo programmi}:\\
\begin{center}
	\begin{multicols}{2}
	\textbf{A}
	\begin{verbatim}
for i=0; i++; i<n
    A[i] = A[i] + B[i];
	\end{verbatim}

	\columnbreak
	\textbf{B}
	\begin{verbatim}
for i=0; i++; i<n
    B[i] = B[i] + C;
	\end{verbatim}
	\end{multicols}
\end{center}
Ricevendo i due blocchi di istruzioni, il \textbf{compilatore riconosce che sono diverse e le compila in modo diverso}. Però in entrambi i casi sono del tipo \textit{oggetto = somma due oggetti}, quindi produce una sequenza di istruzioni analoga (a meno di registri e dati, ovviamente).
Parte del secondo pezzo di codice, ad esempio, verrà tradotto in questa maniera:
\begin{multicols}{2}
\begin{list}{}{}
\item \texttt{LOAD R$_{base}$, R$_{I}$, R$_{1}$}
\item \texttt{ADD R$_{1}$, R$_{2}$, R$_{1}$}
\item \texttt{STORE R$_{base}$, R$_{I}$, R$_{1}$}
\item \texttt{INC R$_{I}$}
\item \texttt{IF$<$ R$_{I}$, R$_{N}$, LOOP}
\end{list}
\columnbreak
\begin{list}{}{}
\item \texttt{M[R[base] + R[I]] $\rightarrow$ R[1]}
\item \texttt{R[1] + R[2] $\rightarrow$ R[1]}
\item \texttt{R[1]  $\rightarrow$ M[R[base] + R[I]]}
\item \texttt{R[I] + 1 $\rightarrow$ R[I]}
\end{list}
\textbf{Microlinguaggio corrispondente}
\end{multicols}
\chapter{MV0 -- Hardware}
\section{Reti Logiche}
L'\textbf{implementazione a livello hardware di funzioni "pure"} dà luogo alle \textbf{Reti Combinatorie}.\\
L'\textbf{implementazione} a livello hardware \textbf{di funzioni "con stato"} dà luogo alle \textbf{Reti Sequenziali}.
\paragraph{Famiglia} Entrambe definiscono la famiglia delle \textbf{Reti Logiche} che permettono di \textbf{realizzare il livello hardware di un sistema di elaborazione}.
\section{Reti Combinatorie}
Una \textbf{rete combinatoria è} una rete logica \textbf{con \texttt{n} ingressi binari X$_{1}$ \ldots X$_{n}$ e \texttt{m} uscite binarie Z$_{1}$ \ldots Z$_{m}$}. Ad \textbf{ogni combinazione di valori in entrata corrisponde una ed una sola combinazione di valori in uscita}. La corrispondenza è definita secondo la funzione implementata dalla rete combinatoria.\\
Indichiamo X$_{1}$ \ldots X$_{n}$ e Z$_{1}$ \ldots Z$_{m}$ come \textbf{variabili logiche} di ingresso ed uscita. \textbf{Tutte le combinazioni possibili} delle variabili logiche \textbf{sono dette stati} di ingresso -- con 2$^n$ possibilità -- e di uscita -- 2$^m$ possibilità.\\\\
Pe rdescrivere le proprietà e la struttura interna delle reti combinatorie si usa un'\textbf{algebra isomorfa a quella logica}, chiama \textbf{Algebra Booleana}.
\subsection{Algebra Booleana}
L'algebra booleana è computata su \textbf{due valori} e \textbf{tre operatori}:
\begin{center}
\begin{multicols}{2}
\texttt{false}\\\texttt{true}\\
\columnbreak
\texttt{AND}\\\texttt{OR}\\\texttt{NOT}
\end{multicols}
\end{center}
Esistono anche altri operatori, derivati dai tre precedenti: \texttt{XOR}, \texttt{NAND}, \texttt{NOR} ecc..
\paragraph{Proprietà} Vale la proprietà distributiva anche per la somma rispetto alla moltiplicazione, oltre il viceversa, quindi: \texttt{A(B+C) = AB + AC}, ma anche \texttt{A + BC = (A + B)(A + C)}.\\
\begin{list}{-}{Inoltre si hanno le cosiddette \textbf{proprietà di DeMorgan}:}
\item \texttt{$\overline{A + B}$ = $\overline{A}$ * $\overline{B}$}
\item \texttt{$\overline{AB}$ = $\overline{A}$ + $\overline{B}$}
\end{list}
\begin{multicols}{3}
\subsubsection{AND}
Anche detta \textbf{moltiplicazione logica}.\\
\begin{tabular}{cc|c}
X & Y & Z \\
0 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 0 \\
1 & 1 & 1 \\
\end{tabular}
\columnbreak
\subsubsection{OR}
Anche detta \textbf{somma logica}.\\
\begin{tabular}{cc|c}
X & Y & Z \\
0 & 0 & 0 \\
0 & 1 & 1 \\
1 & 0 & 1 \\
1 & 1 & 1 \\
\end{tabular}
\columnbreak
\subsubsection{NOT}
Anche detta \textbf{negazione logica}.\\
\begin{tabular}{c|c}
Y & Z \\
0 & 1 \\
1 & 0 \\
\end{tabular}
\end{multicols}
Per costruire una \textbf{rete combinatoria} esistono varie tecniche. Quella che useremo si chiama \textbf{somma di prodotti}.
\subsection{Tecnica della Somma di Prodotti, o codifica degli 1}
\paragraph{La tecnica nel dettaglio} Partendo dalla \textbf{tabella di verità}, identifico le uscite che valgono 1. Di quelle uscite, \textbf{moltiplico (\texttt{AND})} tra loro le entrate \textbf{sulla stessa riga}, \textbf{nego le entrate che valgono 0} e \textbf{sommo (\texttt{OR}) tra loro le diverse righe}.
\paragraph{Un esempio con la somma algebrica} Partendo dalla seguente tabella di verità.\\
\begin{multicols}{2}
\begin{tabular}{cc|c|c}
X & Y & Z & R \\
0 & 0 & 0 & 0 \\
0 & 1 & 1 & 0 \\
1 & 0 & 1 & 0 \\
1 & 1 & 0 & 1\\
\end{tabular}
\columnbreak
\\Sfruttando la tecnica descritta sopra ottengo le seguenti espressioni per le due uscite:
\begin{list}{}{}
\item \texttt{Z = $\overline{X}$ * Y + X * $\overline{Y}$}
\item \texttt{R = X * Y}
\end{list}
\end{multicols}
Alternativamente, posso anche realizzare la \textbf{funzione complementare}, ovver fare il solito procedimento ma per le uscite che valgono 0 per poi negarle.\\
\begin{multicols}{2}
\begin{tabular}{cc|c|c}
X & Y & $\overline{Z}$ & R \\
0 & 0 & 1 & 0 \\
0 & 1 & 0 & 0 \\
1 & 0 & 0 & 0 \\
1 & 1 & 1 & 1\\
\end{tabular}
\columnbreak
\begin{list}{}{}
\item \texttt{Z = $\overline{\overline{X} * \overline{Y} + X * Y}$}
\item \texttt{R = X * Y}
\end{list}
\end{multicols}
\paragraph{Esempio}
\begin{center}
\begin{tabular}{cccc|c}
S1 & S2 & X & Y & S1* \\
0 & 0 & 0 & 0 & 0\\
0 & 0 & 0 & 1 & 0\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 1 & 1 & 1\\
0 & 1 & 0 & 0 & 0\\
0 & 1 & 0 & 1 & 0\\
0 & 1 & 1 & 0 & 0\\
0 & 1 & 1 & 1 & 1\\
1 & 0 & 0 & 0 & 0\\
1 & 0 & 0 & 1 & 1\\
1 & 0 & 1 & 0 & 1\\
1 & 0 & 1 & 1 & 1\\
1 & 1 & 0 & 0 & 0\\
1 & 1 & 0 & 1 & 1\\
1 & 1 & 1 & 0 & 1\\
1 & 1 & 1 & 1 & 1\\
\end{tabular}
\end{center}
S1* = $\overline{S1}$*$\overline{S2}$*X*Y + $\overline{S1}$*S2*X*Y + S1*$\overline{S2}$*$\overline{X}$*$\overline{Y}$ + S1*$\overline{S2}$*X*$\overline{Y}$ + S1*$\overline{S2}$*X*Y + S1*S2*$\overline{X}$*Y + S1*S2*X*$\overline{Y}$ + S1*S2*X*Y
\pagebreak
\subsection{Porte Logiche}
Una volta ricavata l'espressione logica dalla tabella di verità, è \textbf{immediato realizzare lo schema logico utilizzando le componenti hardware elementari}, dette anche \textbf{porte logiche}:
\begin{center}
\includegraphics[scale=0.9]{portelogiche.png}
\end{center}
Ogni porta logica AND e OR \textbf{comporta un ritardo nel calcolo di 1 T$_p$}. Inoltre, ogni AND e OR può avere \textbf{massimo 8 ingressi}, quindi se ho più di 8 segnali in ingresso devo avere \textit{almeno} due livelli: un livello con tante porte logiche quando n/8 con n numero di segnali in ingresso, e \textit{almeno} un livello in cui "unire" i segnali in uscita in una porta logica analoga.
\subsection{Componenti Standard}
Di seguito sono le specifiche di alcune \textbf{reti combinatorie} che verranno supposte come \textbf{standard}, ovvero come componenti utilizzabili come blocchi elementari nella progettazione di strutture più complesse.
\subsubsection{Commutatore}
	\includegraphics[scale=0.7]{commutatore.png}
\subsubsection{Selezionatore}
	\includegraphics[scale=0.7]{confrontatore.png}
\subsubsection{Confrontatore}
	\includegraphics[scale=0.7]{selezionatore.png}
\subsubsection{ALU}
	\includegraphics[scale=0.7]{ALU.png}
\subsection{Ritardo di Stabilizzazione}
\paragraph{Prestazioni} Per valutare le prestazioni di un sistema, occorre saper \textbf{valutare le prestazioni delle reti combinatorie}. Ogni rete reale è \textbf{caratterizzata da un ritardo T$_r$}, necessario affinché \textbf{a seguito di una variazione dello stato d'ingresso si produca la corrispondente variazione dello stato in uscita}.\\
\textbf{Solo dopo} questo tempo si dice che \textbf{la rete è stabilizzata}.
\paragraph{T$_p$} Per una porta logica indichiamo con T$_p$ \textbf{il ritardo di stabilizzazione} -- ad oggi è di circa 10$^{-2}$ millisecondi.\\
Supponiamo che le \textbf{porte NOT} abbiano un \textbf{ritardo nullo}, pari a 0 T$_p$, mentre per le \textbf{porte AND/OR} il valore T$_p$ dipende dal numero di ingressi n della porta. Per n $\leq$ 8 supponiamo che le porte AND/OR abbiano un \textbf{ritardo di stabilizzazione di 1 T$_p$}.\\
Il costo in T$_p$ sarà quindi pari ai livelli di AND/OR presenti. Ad esempio, se ho una tabella di verità con n termini ed m variabili, avrò log$_8$n livelli di OR e log$_8$m livelli di AND. Il costo in T$_p$ sarà quindi = (log$_8$n + log$_8$m) T$_p$
\subsection{Registri e memorie}
\pagebreak
\section{Reti Sequenziali}
Una \textbf{rete sequenziale} è un oggetto con \textbf{un ingresso ed una uscita}, capace di \textbf{mantenere uno stato interno} -- ecco perché si parla di funzioni con stato. A livello hardware, possiamo identificare una rete sequenziale con un \textbf{automa a stati finiti}.
\paragraph{ASF} Un \textbf{automa a stati finiti} è caratterizzato da:
\begin{list}{}{}
	\item n variabili di ingresso $\Rightarrow$ h = 2$^n$ \textbf{stati di ingresso} X$_1$\ldots X$_h$
	\item m variabili di uscita $\Rightarrow$ k = 2$^m$ \textbf{stati di uscita} Z$_1$\ldots Z$_k$
	\item r variabili logiche dello stato interno $\Rightarrow$ p = 2$^r$ \textbf{stati interni} S$_1$\ldots S$_p$
	\item una \textbf{funzione di transizione} dello stato interno $\sigma$: X x S $\rightarrow$ S che \textbf{definisce il passaggio tra gli stati}
	\item una \textbf{funzione delle uscite} $\omega$: X x S $\rightarrow$ Z che \textbf{calcola le uscite}
\end{list}
Una \textbf{rete sequenziale} è quindi \textbf{composta da due reti combinatorie $\sigma$ e $\omega$}, che rispettivamente calcolano la variazione dello stato e l'uscita, \textbf{e da un registro R} che contiene lo stato interno.
\subsection{Modello di Mealy}
\begin{multicols}{2}
\includegraphics[scale=0.5]{mealy.png}
\columnbreak

Considerando il comportamento al tempo t, lo \textbf{stato interno successivo S(t+1)} dipende sia dallo stato di ingresso al tempo t, cioè X(t), sia dallo stato interno attuale S(t).\\
\texttt{S(t+1) = $\sigma$(X(t), S(t))}\\\\

Lo \textbf{stato di uscita al tempo t, Z(t)}, dipende sia dallo stato di ingresso X(t) sia dallo stato interno attuale S(t).\\
\texttt{Z(t) = $\omega$(X(t), S(t))}\\\\
\includegraphics[scale=0.8]{clockmemo.png}
\end{multicols}
\subsection{Modello di Moore}
\begin{multicols}{2}
\includegraphics[scale=0.5]{moore.png}
\columnbreak

In maniera analoga al modello di Mealy, lo \textbf{stato interno successivo S(t+1)} dipende sia dallo stato di ingresso al tempo t, cioè X(t), sia dallo stato interno attuale S(t).\\
\texttt{S(t+1) = $\sigma$(X(t), S(t))}\\\\

Lo \textbf{stato di uscita al tempo t, Z(t)}, dipende solo dallo stato interno attuale S(t).\\
\texttt{Z(t) = $\omega$(S(t))}\\\\
\includegraphics[scale=0.8]{clockmemo.png}
\end{multicols}
\pagebreak
\subsection{Reti Sequenziali di tipo Sincrono}
Vediamo adesso come si comportano nel tempo le reti sequenziali e spieghiamo perché adotteremo quelle di tipo sincrono. Come riferimento usiamo una rete di Mealy.
\paragraph{Spezzare} Abbiamo detto che lo stato al tempo successivo S(t + 1) dipende sia dall'ingresso X sia dallo stato interno attuale S(t), cioè \texttt{S(t+1) = $\sigma$(X(t), S(t))}.\\
Il registro R funge come un "cancello temporizzato" che \textbf{spezza la sequenza temporale degli eventi}.
\begin{multicols}{2}
Se il registro R non fosse presente, si verificherebbe la situazione in figura. In questo esempio, la porta logica o il componente $\sigma$ \textbf{potrebbero non stabilizzarsi mai}.\\
Se per esempio mettiamo una porta AND con due variabili in ingresso che nega il proprio risultato, tale rete tenderà a non stabilizzarsi mai ma a produrre una sequenza infinita di 0 e 1 in uscita.\\
Quindi devo avere necessariamente \textbf{un meccanismo che mi possa aiutare a determinare il valore dell'uscita} al tempo t, t + 1\ldots\\\\
Questo strumento è il resgistro impulsato, dove \textbf{la scrittura è scandita dal ciclo di clock}.
\columnbreak

\begin{center}
\includegraphics[scale=0.6]{esempiosincrono.png}
\end{center}
\end{multicols}
\paragraph{Modo Sincrono} Questo modo di lavorare delle reti sequenziali con un registro impulsato che funge da cancello temporizzato grazie al ciclo di clock si chiama \textbf{Modo Sincrono}.
\paragraph{Quando variare} Cerchiamo ora di capire quando devono variare gli ingressi e \textbf{per quanto tempo devono avere tale valore}.\\
Supponiamo di avere gli ingressi X$_0$ = 0 al tempo t, X$_1$ = 1 al tempo t + 1 e X$_2$ = 0 al tempo t + 2, e supponiamo che t$_\omega$ = t$_\sigma$ = 2t.
\begin{multicols}{2}
Se l'ingresso X variasse in un punto non precisato del ciclo di clock è probabile che $\omega$ e $\sigma$ non abbiano il \textbf{tempo necessario per produrre un risultato} e quindi avrei un \textbf{comportamento indefinito}.\\\\\\\\\\
In questo caso, cambiando il valore X all'inizio del ciclo di clock do il tempo necessario a $\sigma$ e $\omega$ di produrre un risultato stabile, ma al prossimo impulso del ciclo di clock (t + 2) leggerò di nuovo X = 1, che non è l'input corretto al tempo t + 2.\\\\\\\\\\\\
Questa è la soluzione giusta per il nostro esempio, che rispetta tutte le condizioni da noi elencate.
\includegraphics[scale=0.5]{sincronoesempi.png}
\end{multicols}
Per far funzionare le nostre reti, il ciclo di clock deve essere tale che T = MAX(t$_\sigma$, t$_\omega$) + $\delta$. Le reti funzionano anche con T $>$ MAX(t$_\sigma$, t$_\omega$) + $\delta$, ma avrei del \textbf{tempo perso} poiché la rete non opera, \textbf{aspetta solo che il clock sia alto per andare a scrivere nel registro}.
\pagebreak
\subsection{Reti Sequenziali a Componenti Standard}
Per poter sintetizzare una rete devo prima \textbf{decidere} se \textbf{implementare un modello di Mealy o di Moore}, \textbf{derivare le tabelle di verità} di $\omega$ e $\sigma$ e dire \textbf{quanti bit} ha il registro R.\\
Fatto questo, \textbf{ricavare le reti combinatorie} e \textbf{capire quanto valga il ciclo di clock} T (con T = MAX(t$_\sigma$, t$_\omega$) + $\delta$) che fa funzionare l'intera rete sequenziale.
\paragraph{Sintesi Classica} ASF $\longrightarrow$ Mealy o Moore $\rightarrow$ Tabelle verità, bit di R $\Rightarrow$ Reti combinatorie $\Rightarrow$ Ciclo di clock
\paragraph{Componenti Standard} In realtà per sintetizzare le reti sequenziali non usiamo questo procedimento di sintesi, ma bensì \textbf{usiamo i componenti standard}. Per esempio, prendiamo una rete che vuole calcolare il numero di persone presenti dentro una stanza con capienza massima 100 persone.
\paragraph{Con la sintesi classica} R ha bisogno di 7 bit per contare da 0 a 100.\\
Se andiamo, per esempio, a fare la tabella di verità per $\omega$, abbiamo ben 8 colonne negli ingressi, quindi 2$^8$ possibili combinazioni (righe).\\
Potrei avere 2$^8$/2 = 2$^7$ "uni" per colonna, di conseguenza un \textbf{numero considerevole di porte logiche}.\\
Diventa quindi praticamente impossibile sintetizzare questo esempio con il metodo classico. Procediamo con l'alternativa: l'utilizzo delle componenti standard.
\paragraph{Con le componenti standard} Procediamo col nostro esempio:\\
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=0.8]{esempiocompstd.png}
\end{center}
\columnbreak

In questo caso abbiamo usato il modello di Moore. Il \textbf{risultato è disponibile al prossimo impulso del ciclo di clock}.\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\

Qua invece è stato usato il modello di Mealy. In questo caso si vede bene come \textbf{la rete di Mealy sia più veloce}, poiché \textbf{il risultato è subito disponibile} prima del prossimo impulso del ciclo di clock: infatti Z non viene scritto in R prima di essere pubblicato.
\end{multicols}
\pagebreak
Di seguito un esempio di rete sequenziale a componenti standard più complesso.
\begin{center}
\includegraphics[scale=0.8]{compstdesempiobig.png}
\end{center}
\paragraph{Con sintesi classica} Avrei:
\begin{list}{}{}
	\item R = \{A, B\}, due registri da 32 bit $\Rightarrow$ 64 bit
	\item Ingressi: X + Y + $\alpha_{K1}$ + $\alpha_{K2}$ + $\alpha_{ALU}$ + $\beta_A$ + $\beta_B$ = 32 + 32 + 3 + 2 = 69
	\item Uscite: Z $\Rightarrow$ 32 bit
\end{list}
La tabella di verità di $\omega$, per esempio, avrebbe 69 colonne di ingressi, quindi 2$^{69}$ righe, \textbf{senza considerare gli ingressi di A e B}.\\
Il risultato è che è molto scomodo lavorare con una tabella di circa 5.9 * 10$^{20}$ righe.
\pagebreak
\chapter{MV1 -- Firmware}
\section{Unità Firmware}
Un sistema di elaborazione, a livello Firmware, è costituito da un certo numero di \textbf{Unità Firmware} che interagiscono fra loro mediante un sistema di interconnessione. Le UF sono capaci si svolgere un certo numero di operazioni esterne.
\paragraph{Unità Firmware} Una \textbf{unità firmware} è un \textbf{modulo di elaborazione autonomo} -- cioè capace di controllare la propria operazione in modo del tutto indipendente -- \textbf{e sequenziale} -- cioè dal funzionamento descritto da un programma sequenziale -- \textbf{capace di eseguire delle operazioni esterne} -- istruzioni \textbf{assembler}.
\paragraph{Struttura di interconnessione} Tipicamente la struttura di interconnessione tra unità firmware è \textbf{punto-a-punto} quindi a \textbf{collegamenti dedicati}.
\subsection{PC e PO}
Per capire bene cosa sono e a cosa servono le parti controllo (PC) e operativa (PO), vediamo un semplice esempio di come arriviamo a strutturare un'unità firmware.
\begin{center}
\includegraphics[scale=0.6]{ufesempio.png}
\end{center}
Il nostro obiettivo è quello di realizzare una unità capace di produrre un risultato Z a partire dalle variabili in input, \textbf{fornendogli solo le istruzioni per l'operazione da implementare} e \textbf{capace di gestire tutte le variabili di controllo} ($\alpha$, $\beta$) \textbf{in maniera autonoma}.\\
L'unità firmware è quindi l'unione di due oggetti:
\begin{list}{}{}
\item \textbf{Parte Controllo} che "comanda" l'operazione da eseguire
\item \textbf{Parte Operativa} che "esegue" l'operazione
\end{list}
\pagebreak
\subsubsection{Ciclo di Clock}
\begin{center}
\includegraphics[scale=1]{ufclock.png}
\end{center}
PO e PC sono\textbf{reti sequenziali impulsate} dallo stesso segnale di clock, quindi aventi lo stesso ciclo di clock. Il \textbf{ciclo di clock dell'unità firmware} viene determinato in modo da \textbf{permettere la stabilizzazione di entrambe le reti per l'esecuzione di una qualsiasi microisitruzione}.\\
Questo modello di programmazione è \textbf{sincrono}.\\\\
\textbf{Schematizzazione} del diagramma del ciclo di clock sopra:
\begin{list}{}{}
	\item $\omega_{PO}$: prepara i valori che servono alla PC per decidere cosa fare
	\item $\omega_{PC}$: prepara \{$\alpha$, $\beta$\} che implementano l'operazione richiesta
	\item $\sigma_{PC}$: decido il prossimo stato interno della PC $\rightarrow$ scrivo il registro R della PC
	\item $\sigma_{PO}$: eseguo l'operazione pianificata $\rightarrow$ scrivo i nuovi valori nei registri che compongono lo stato interno della PO
\end{list}
\subsubsection{Parte Operativa, Moore}
\textbf{Rete Sequenziale} progettata con \textbf{componenti standard} che provvede all'esecuzione di \textbf{istruzioni} tramite commutatori, selettori, ALU e registri.
\subsubsection{Parte Controllo, Mealy}
\textbf{Rete Sequenziale} progettata tramite \textbf{sintesi classica} che provvede a determinare le \textbf{variabili di controllo} $\alpha$ e $\beta$ per la parte operativa.
\subsubsection{Mealy o Moore?}
Essendo entrambe due reti sequenziali bisogna decidere quale modello usare. Analizziamo le varie combinazioni di modelli.
\paragraph{Mealy--Mealy} Se uso un modello Mealy--Mealy le uscite di $\omega_{PO}$ vanno direttamente nella $\omega_{PC}$ e le uscite di $\omega_{PC}$ ritornano in $\omega_{PO}$. Non ho un registro che ferma il ciclo continuo tra $\omega_{PC}$  e $\omega_{PO}$, quindi non riuscirò mai a stabilizzare i segnali che si scambiano PO e PC.\\
Viene naturale pensare di farle entrambe Mealy--Mealy poiché, come visto in precedenza, il modello di Mealy è più veloce di quello di Moore.
\paragraph{Almeno una Moore} Concludiamo che almeno una tra PC e PO deve essere di Moore per poter stabilizzare l'intera UF, ma quale?\\
La risposta corretta è usare il \textbf{modello di Mealy per la Parte Controllo} e il \textbf{modello di Moore per la Parte Operativa} in modo da avere dei \textbf{comandi veloci} ed una \textbf{esecuzione più lenta} rispetto alla PC.
\paragraph{Il contrario?} Non scegliamo un modello Moore per PC e Mealy per PO perché è \textbf{illogico avere un esecutore veloce che deve aspettare un controllore lento}: inutile avere una macchina molto veloce se inserisco i comandi molto lentamente.\\
Anche un modello Moore--Moore non è comodo da usare, seppure funzionante correttamente, perché avrei entrambe le parti lente.
\pagebreak
\begin{multicols}{2}
	\begin{center}
		\includegraphics[scale=0.5]{pcposchema.png}
	\end{center}
	\columnbreak
	\begin{center}
		\includegraphics[scale=1]{ufschema.png}
	\end{center}
\end{multicols}
\paragraph{Condizione di Correttezza} $\Rightarrow$ PO di Moore\\
Le uscite della Parte Operativa, cioè le variabili di condizionamento, dipendono esclusivamente dallo stato interno di PO, cioè \textbf{tutte le variabili di condizionamento devono essere prodotte senza usare alcun $\alpha$}
\subsection{Procedimento Formale}
Schematizzazione dei passaggi del procedimento formale per la costruzione e l'analisi di una rete sequenziale.
\begin{enumerate}
	\item Descrizione a parole delle operazioni esterne
	\item Programma scritto in $\mu$-linguaggio
	\item Componenti
	\begin{list}{}{}
		\item R$_{PO}$: capire quali sono i registri di stato della PO
		\item $\omega_{PO}$, $\sigma_{PO}$: capire le funzioni che mi servono nella PO
		\item R$_{PC}$: capire cosa è lo stato della PC
		\item $\omega_{PC}$, $\sigma_{PC}$: capire cosa calcolare nella PC
		\item $\longrightarrow$ T = t($\omega_{PO}$) + MAX\{t($\omega_{PC}$) + t($\sigma_{PO}$), t($\omega_{PC}$)\} + $\delta$
	\end{list}
\end{enumerate}
\pagebreak
\chapter{$\mu$-linguaggio}
Si formalizza un linguaggio chiamato \textbf{$\mu$-linguaggio} che permetta di \textbf{derivare formalmente com'è fatta la PC e la PO di una certa UF}.
\section{Istruzioni}
Nel $\mu$-linguaggio sono presenti solamente \textbf{due tipi di istruzione}:
\begin{list}{}{}
	\item \texttt{n. $\mu$op$_1$, \ldots, $\mu$op$_k$, m}\\
	Le \texttt{op} sono \textbf{operazioni di trasferimento tra registri}. Le varie \texttt{op} \textbf{separate da una virgola sono eseguite contemporaneamente} -- cioè nello stesso ciclo di clock.\\
	\texttt{m} finale indica \textbf{a quale istruzione andare dopo aver eseguito questa istruzione}, la \texttt{n}.
	\item \texttt{n. (condizione = T) $\mu$op$_1$, \ldots, $\mu$op$_k$, m'\\
	(condizione = F) $\mu$op$_1$, \ldots, $\mu$op$_h$, m''}\\
	Le \textbf{condizioni} sono \textbf{date in termini di variabili di condizionamento}. Possono essere messe in sequenza, venendo \textbf{valutate in sequenza}.\\
	Posso considerare solo \textbf{variabili booleane} o \textbf{espressioni di cui mi interessa solo il risultato} senza memorizzarlo.
\end{list}
\paragraph{Esempio} Vediamo un esempio di come scrivere un programma in $\mu$-linguaggio. Prendiamo come esempio la divisione fra interi.\\
\begin{multicols}{2}
\begin{center}
\textbf{Linguaggio pseudo-C}
\begin{verbatim}
Q = 0
while (A >= B) {
    Q = Q + 1
    A = A - B
}
R = A
\end{verbatim}
\end{center}
\columnbreak
\begin{center}
\textbf{$\mu$-linguaggio}
\begin{lstlisting}
0.  0 -> Q, 1
1.  (segno(A - B) = 0) nop, 2
    (= 1)              nop, 4
2.  Q + 1 -> Q, 3
3.  A - B -> A, 1
4.  A -> R, 0
\end{lstlisting}
\end{center}
\end{multicols}
Ogni $\mu$-istruzione è \textbf{eseguita esattamente in un ciclo di clock}. Nell'esempio, per eseguire il programma avrò bisogno di almeno 5 cicli di clock, a meno di iterazioni interne.
\pagebreak
\section{Ottimizzazione del codice}
Dopo aver scritto il $\mu$-codice possiamo provare ad \textbf{ottimizzarlo}, cioè \textbf{ridurre il numero di cicli di clock necessari ad eseguirlo}.\\
Un'ottimizzazione possibile dell'esempio precedente è la seguente.
\begin{multicols}{2}
\begin{center}
Prima
\begin{lstlisting}
0.  0 -> Q, 1
1.  (segno(A - B) = 0) nop, 2
    (= 1)              nop, 4
2.  Q + 1 -> Q, 3
3.  A - B -> A, 1
4.  A -> R, 0
\end{lstlisting}
\end{center}
\columnbreak
\begin{center}
Dopo
\begin{lstlisting}
0.  0 -> Q, A - B -> T, A - B -> A, 1
1.  (T0 = 0) Q + 1 -> Q, A - B -> A, A - B -> T, 1
    (= 1)    A -> R, 0
\end{lstlisting}
\end{center}
\end{multicols}
Considero un registro T dove memorizzo il risultato di A - B. Di quel registro T, considero T$_0$ -- cioè il bit più significativo -- per il segno.\\
Inoltre elimino le \texttt{nop}, che sono \textit{tempo sprecato}.
\subsection{Condizioni di Bernstein}
Per eseguire le ottimizzazioni sul $\mu$-codice, dobbiamo \textbf{seguire le Condizioni di Bernstein}. Tali condizioni forniscono delle regole per verificare se due o più $\mu$-operazioni possono essere eseguite nella medesima $\mu$-istruzione.
\paragraph{Le Condizioni} Per capire se\\
\texttt{i. $\mu$op$_A$, i+1\\
i+1. $\mu$op$_B$, k}\\
è equivalente a\\
\texttt{i. $\mu$op$_A$, $\mu$op$_B$, k}\\
Bisogna \textbf{valutare il dominio R(op)} -- registri \textbf{letti} da op -- e \textbf{il codominio W(op)} -- registri \textbf{scritti} da op -- delle $\mu$-operazioni.\\
Nell'esempio precedente:
\begin{list}{}{}
	\item R(A - B $\rightarrow$ T) = \{A, B\}
	\item W(A - B $\rightarrow$ T) = \{T\}
	\item R(Q + 1 $\rightarrow$ Q) = \{Q\}
	\item W(Q + 1 $\rightarrow$ Q) = \{Q\}
\end{list}
Le \textbf{condizioni da verificare} sono:
\begin{list}{}{}
	\item W($\mu$op$_A$) $\cap$ R($\mu$op$_B$) = $\emptyset$\\
	\textbf{Dipendenza}: non posso mettere insieme $\mu$-operazioni tali che la prima scrive in un registro letto dalla seconda.
	\item W($\mu$op$_A$) $\cap$ W($\mu$op$_B$) = $\emptyset$\\
	\textbf{Dipendenza di output}: non posso scrivere nello stesso registro con due $\mu$-operazioni diverse nella stessa $\mu$-istruzione.
\end{list}
\subsection{Variabili di Condizionamento}
Le \textbf{variabili di condizionamento} possono essere così categorizzate:
\begin{list}{}{}
	\item \textbf{Semplici}: indicano le uscite di registri \textbf{senza trasformazioni}\\
	$\longrightarrow$ t$_{\omega PO}$ = 0
	\item \textbf{Complesse}: indicano \textbf{trasformazioni delle uscite di registri} fatte tramite reti combinatorie prive di ingressi di controllo.\\
	Esempio: \texttt{segno(A - B)}, \texttt{OR(A)}\\
	$\longrightarrow$ t$_{\omega PO}$ = k t$_p$
\end{list}
\pagebreak
\subsection{Tempo medio di elaborazione}
Il \textbf{tempo medio di elaborazione} di una UF viene valutato come: \texttt{$T = \sum_{i = 0}^{n - 1}(p_i * k_i) $}\\
Dove:
\begin{list}{}{}
	\item $k_i$ è il numero medio di cicli di clock necessari per eseguire una generica operazione i
	\item $p_i$ è la probabilità di eseguire tale operazione
\end{list}
Quando non sono note le $p_i$, si assume che tutte le sottosequenze siano equiprobabili. Calcoliamo quindi T come media aritmetica dei $k_i$, oppure si cerca di stimare se possibile una distribuzione probabilistica attendibile.
\subsection{Riflessioni finali sull'ottimizzazione}
Bisogna prestare particolare attenzione quando si ottimizza il $\mu$-codice. Ridurre il numero di $\mu$-istruzioni ($k_i$) non è sempre qualcosa di buono.\\
Talvolta, \textbf{unire due o più $\mu$-istruzioni obbliga ad aumentare il ciclo di clock T} per consentire al $\mu$-programma di eseguirle tutte. Questa modifica, che si applica a \textbf{tutte} le $\mu$-istruzioni, potrebbe aumentare il tempo medio di elaborazione T, rendendo il programma complessivamente più lento.\\\\
Concludendo, le ottimizzazioni che si possono fare sono:
\begin{list}{}{}
	\item Eliminare le \texttt{nop}, tranne quelle di attesa per operazioni esterne
	\item Raggruppare le $\mu$-operazioni, attraverso le condizioni di Bernstein
	\item Raggruppare le condizioni logiche
\end{list}
\section{Controllo Residuo}
Per diminuire ulteriormente la complessità della PC possiamo \textbf{delegare alla PO alcune delle decisioni che dovrebbe prendere la PC}.\\
Vediamo alcuni esempi:
\begin{list}{}{}
	\item Leggere il k-esimo bit di un registro R di n bit
	\begin{multicols}{2}
	\begin{center}
	\textbf{Soluzione classica}\\
	\includegraphics[scale=0.5]{contrres_es1.png}
	\end{center}
	\columnbreak
	\begin{center}
	\textbf{Controllo residuo}\\
	\includegraphics[scale=0.5]{contrres_es1b.png}
	\end{center}
	Risparmio complessità della PC e riduco il traffico di dati da PO a PC
	\end{multicols}
	\pagebreak
	\item Supponiamo una $\mu$-istruzione da eseguire a seconda di una certa condizione, ad esempio: \texttt{(A$_0$ = 0) B + C $\rightarrow$ D}
	\begin{multicols}{2}
	\begin{center}
	\textbf{Soluzione classica}\\
	\includegraphics[scale=0.5]{contrres_es2.png}
	\end{center}
	\columnbreak
	\begin{center}
	\textbf{Controllo residuo}\\
	\includegraphics[scale=0.5]{contrres_es2b.png}
	\end{center}
	\end{multicols}
	\item \texttt{(segno(A - B) = 0) B - A $\rightarrow$ C\\(= 1) B + A $\rightarrow$ C}
	\begin{multicols}{2}
	\begin{center}
	\textbf{Soluzione classica}\\
	\includegraphics[scale=0.5]{contrres_es3.png}
	\end{center}
	\columnbreak
	\begin{center}
	\textbf{Controllo residuo}\\
	\includegraphics[scale=0.5]{contrres_es3b.png}
	\end{center}
	\end{multicols}
\end{list}
\section{Comunicazioni}
Con \textbf{comunicazioni} si intendono le \textbf{comunicazioni fra unità firmware e mondo esterno} e viceversa.\\
Nell'esempio preso in esame, della divisione fra A e B interi con Q ed R risultati, \textbf{A e B provengono dal mondo esterno e Q ed R sono comunicati verso di esso}.
\begin{center}
A,B $\longrightarrow$ UF $\longrightarrow$ Q,R
\end{center}
\paragraph{Categorie} Le comunicazioni sono \textbf{classificate in due categorie}:
\begin{list}{}{}
	\item \textbf{Simmetriche/Asimmetriche}\\
	Simmetriche: un solo mittente, un solo destinatario (\textbf{uno-a-uno})\\
	Asimmetriche: asimmetria in ingresso (\textbf{più mittenti}) o in uscita (\textbf{più destinatari})
	\item \textbf{Sincrone/Asincrone}\\
	Sincrone: la comunicazione avviene "\textbf{istantaneamente}"\\
	Asincrone: il destinatario legge il messaggio \textbf{dopo del tempo} (es. e-mail)
\end{list}
\pagebreak
\subsection{Protocollo a Livelli}
\textbf{Simmetrico e asincrono}, il protocollo a livelli \textbf{funziona aggiungendo ai registri} XOUT di UF$_1$ e XIN di UF$_2$ \textbf{altri due registri da 1 bit ciascuno}, che \textbf{indicano quando avviene la comunicazione}: \textbf{ACK e RDY}
\begin{center}
\includegraphics[scale=0.7]{ufprotliv.png}
\end{center}
Di seguito i passi del funzionamento del protocollo:
\begin{list}{}{}
	\item 1. UF$_1$ scrive XOUT e il primo registro da 1 bit
	\begin{multicols}{2}
		\begin{center}
		Situazione iniziale\\
		\begin{tabular}{ c c c }
			 0 &   & 0 \\ 
			 1 & $\longrightarrow$ & 0
		\end{tabular}
		\end{center}
		\columnbreak
		\begin{center}
		Situazione finale\\
		\begin{tabular}{ c c c }
			 0 &   & 0 \\ 
			 1 & $\longrightarrow$ & 1
		\end{tabular}
		\end{center}
		1 in RDY di UF$_2$ significa che ci sono dati significativi in XIN
	\end{multicols}
	\item 2. UF$_2$ utilizza XIN e comunica che ha finito scrivendo nel proprio registro di OUT da 1 bit
	\begin{multicols}{2}
		\begin{center}
		Situazione iniziale\\
		\begin{tabular}{ c c c }
			 0 & $\longleftarrow$ & 1 \\ 
			 1 & $\longrightarrow$ & 0
		\end{tabular}
		\end{center}
		\columnbreak
		\begin{center}
		Situazione finale\\
		\begin{tabular}{ c c c }
			 1 & $\longleftarrow$ & 1 \\ 
			 1 & $\longrightarrow$ & 1
		\end{tabular}
		\end{center}
	\end{multicols}
	\item 3/4. Ritorno alla situazione iniziale con tutti i registri da un bit a 0
	\begin{multicols}{4}
		\begin{center}
		\begin{tabular}{ c c c }
			 1 & $\longleftarrow$ & 1 \\ 
			 0 & $\longrightarrow$ & 1
		\end{tabular}
		\end{center}
		\columnbreak
		\begin{center}
		\begin{tabular}{ c c c }
			 1 & $\longleftarrow$ & 1 \\ 
			 0 & $\longrightarrow$ & 0
		\end{tabular}
		\end{center}
		\columnbreak
		\begin{center}
		\begin{tabular}{ c c c }
			 1 & $\longleftarrow$ & 0 \\ 
			 0 & $\longrightarrow$ & 0
		\end{tabular}
		\end{center}
		\columnbreak
		\begin{center}
		\begin{tabular}{ c c c }
			 0 & $\longleftarrow$ & 0 \\ 
			 0 & $\longrightarrow$ & 0
		\end{tabular}\\
		Si può riniziare
		\end{center}
	\end{multicols}
\end{list}
Vediamo i cicli di clock necessari per usare questo protocollo:
\begin{enumerate}
	\item UF$_1$ scrive 1
	\item UF$_2$ vede 1 $\rightarrow$ \ldots UF$_2$ agisce\ldots $\rightarrow$ UF$_2$ scrive 1
	\item UF$_1$ vede 1 di ritorno $\rightarrow$ UF$_1$ scrive 0
	\item UF$_2$ vede 0 $\rightarrow$ UF$_2$ scrive 0\\
	$\Rightarrow$ Condizioni iniziali: \textbf{4 cicli di clock}
\end{enumerate}
Se le due UF hanno clock sfasati uso lo stesso ragionamento, probabilmente finendo per dover usare più cicli di clock.
\paragraph{Nel programma} Come rendere questo meccanismo nel $\mu$-codice? Proviamo a scriverlo per UF$_2$:
\begin{lstlisting}
0.  (RDY = 0) nop, 0
    (= 1)     A -> TEMPA, B -> TEMPB, 1 -> ACK, 1
1.  (RDY = 1) nop, 0
    (= 0)     0 -> ACK, <proseguo con altro>
\end{lstlisting}
\pagebreak
Questo protocollo è particolarmente semplice e necessita \textbf{pochissimo hardware}, ma \textbf{richiede troppi cicli di clock per comunicare}. Vediamo un'alternativa migliore.
\subsection{Protocollo a Transizione di Livello}
\textbf{Simmetrico e asincrono}, simile al protocollo a livelli ma \textbf{usa degli indicatori di transizione di livello}.
\begin{center}
\includegraphics[scale=0.7]{uftransliv.png}
\end{center}
\subparagraph{ACK} \textbf{Contatore in modulo 2}, quindi cambia stato (0 $\leftrightarrow$ 1) ogni volta che ci scrivo.
\begin{multicols}{2}
\includegraphics[scale=1]{ack.png}
\begin{center}
\begin{tabular}{ c c | c }
			 S & $\beta$ & S' \\
			 \hline
			 0 & 0 & 0 \\
			 0 & 1 & 1 \\
			 1 & 0 & 1 \\
			 1 & 1 & 0
\end{tabular}
\end{center}
\end{multicols}
\subparagraph{RDY} Risultato di un confrontatore fra un contatore modulo 2 e un registro in ingresso.
\begin{multicols}{2}
\includegraphics[scale=1]{rdy.png}
\end{multicols}
\paragraph{Funzionamento} Dal punto di vista di UF$_2$.\\
Quando arriva un segnale da UF$_1$, la rete RDY diventa 1, quindi si può lavorare con i dati ricevuti.\\
Per comunicare a UF$_1$ che l'operazione è conclusa, mando un $\beta_{ACK}$ = 1 al mio ACK, che diventerà 1 anche in uscita.\\\\
Questo procedimento è il medesimo del protocollo a livelli visto in precedenza. Quello che cambia è come mi riporto nelle condizioni iniziali.\\
Dopo aver lavorato con XIN, chiamo \texttt{reset RDY} che \textbf{riporta la rete RDY a 0}, quindi pronta ad accogliere un nuovo messaggio.\\
In poche parole, torno alle impostazioni iniziali nello stesso momento in cui ricevo il messaggio.
\pagebreak
\subsubsection{Esempio}
Per capire meglio vediamo come esempio quello in esame, la divisione fra A e B interi con risultati Q, R.
\begin{multicols}{2}
\includegraphics[scale=1]{transliv_es.png}
\columnbreak
\begin{center}
\begin{lstlisting}
0.  (RDYIN = 0) nop, 0
    (= 1)       A -> TA, B -> TB, set ACKIN,
                reset RDYIN, 1
1.  0 -> TQ, 2
2.  (segno(TA - TB), ACKOUT = 0-) TQ + 1 -> TQ,
                                  TA - TB -> TA, 2
    (= 11) TA -> R, TQ -> Q, set RDYOUT, reset ACKOUT ...
\end{lstlisting}
\end{center}
Possiamo notare che UF$_1$ manda i segnali A e B e \textbf{non ha altre operazioni mentre attende la risposta Q, R da UF$_2$}. Questa situazione si chiama \textbf{protocollo domanda/risposta} e solo in questo caso basta una coppia di indicatori di transizione.
\begin{center}
\begin{lstlisting}
UF1
0.  ... -> A, set RDY, 1
1.  (ACK = 0) nop, 1
    (= 1)     B -> ..., reset ACK

    
UF2
0.  (RDY = 0) nop, 0
    (= 1) A -> ..., 1
... elaborazione di UF2 ...
n.  ... -> B, set ACK, reset RDY, 0
\end{lstlisting}
\end{center}
\end{multicols}
\pagebreak
\subsection{Comunicazioni asincrone a n posizioni}
In questo caso \textbf{non esiste una soluzione basata su semplici interfacce nelle due unità comunicati}, ma è \textbf{necessario introdurre una terza unità} (chiamata \textbf{unità buffer}) che \textbf{implementi una coda FIFO a n posizioni}.\\
Il \textbf{mittente può spedire al più n messaggi senza che il destinatario effettui ricezioni}.
\begin{center}
\includegraphics[scale=0.7]{asynnpos.png}
\end{center}
\paragraph{1} Se ci sono messaggi in memoria M, manda un messaggio a UF$_2$
\paragraph{2} Se c'è posto in memoria M, riceve un messaggio da UF$_1$ e lo memorizza in M
\paragraph{} Se valgono entrambe ed M è vuota, allora il messaggio in ingresso da UF$_1$ viene passato direttamente ad UF$_2$
\paragraph{U$_{buffer}$} Il codice di U$_{buffer}$ avrà come \textbf{variabili di condizionamento RDY da UF$_1$}, \textbf{ACK da UF$_2$}, \textbf{condizione di memoria piena} e \textbf{condizione di memoria vuota}.\\
Le due condizioni di memoria piena/vuota possono essere gestite tramite un semplice contatore. SE per esempio abbiamo una memoria M con 2$^k$ posizioni, useremo un contatore da k + 1 bit:
\begin{list}{}{}
	\item Memoria vuota \texttt{OR(CONT) = 0}
	\item Memoria piena \texttt{CONT$_0$ = 1}
\end{list}
$\Rightarrow$ \texttt{0. (RDY, ACK, OR(CONT), CONT$_0$, \ldots}\\\\
Il buffer implementa una politica \textbf{FIFO}: \textbf{il primo messaggio inviato} da UF$_1$ \textbf{deve essere il primo messaggio letto} da UF$_2$
\subsection{Comunicazioni asimmetriche}
\paragraph{BUS} Per poter parlare delle comunicazioni asimmetriche è necessario introdurre il concetto di BUS. Un \textbf{BUS è un insieme di linee per trasportare dati} (es. da 32 bit), \textbf{l'indirizzo} (es. log$_2$ n bit) \textbf{e una linea da 1 bit che indica l'operazione da svolgere}.
\begin{center}
\includegraphics[scale=0.9]{bus.png}
\end{center}
\pagebreak
Il grosso svantaggio di questo tipo di comunicazione è che \textbf{le unità possono comunicare solamente una alla volta}. Ho bisogno quindi di un \textbf{meccanismo di arbitraggio} che regola l'ordine di comunicazione delle varie unità.\\
Vedremo due tipi di arbitraggio: \textbf{centralizzato} e \textbf{distribuito}.\\
Inoltre avremo a disposizione anche un protocollo d'interazione Unità --- BUS:\\richiesta da U$_i$ di uso del BUS $\rightarrow$ comunicazione (uso del BUS) $\rightarrow$ rilascio della risorsa.\\\\
Rimane da sottolineare il \textbf{problema della sincronizzazione}. Abbiamo bisogno di indicatori a livello per poter comunicare in maniera asimmetrica con un BUS.
\subsubsection{Arbitri Centralizzati}
\paragraph{Arbitro Centralizzato a Richieste Indipendenti} La unità hanno un \textbf{collegamento diretto con l'arbitro}
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=0.5]{arcentrrichind.png}
\end{center}
\columnbreak
L'arbitro AR \textbf{considera le richieste}, fra di esse \textbf{individura l'unità} U$_i$ vincitrice \textbf{secondo una certa politica} P e \textbf{assegna la richiesta} all'unità individuata. Dopodiché, attende il rilascio della risorsa assegnata.
\end{multicols}
\paragraph{Arbitro Centralizzato Daisy Chaining} Abbiamo un numero minore di ingressi ma la stessa politica del caso precedente.
\begin{center}
\includegraphics[scale=0.5]{arcentrdaisy.png}
\end{center}
Il vantaggio di questo arbitro è la sua semplicità: due ingressi ed una uscita.\\
L'arbitro, se c'è richiesta, manda il segnale di disponibilità alla prima unità. Il segnale viene passato da unità ad unità, finché non viene trovata l'unità che ne ha fatto richiesta. A tal punto, tale unità manda all'arbitro il bit di occupato, ed inizia ad utilizzare la risorsa. Infine, l'arbitro attende che il bit di occupato torni a 0.
\paragraph{Arbitro Centralizzato Polling} L'arbitro interroga le varie unità per sapere se hanno bisogno della risorsa BUS, ed assegna le risorse secondo una certa politica.
\paragraph{Aribtro Centralizzato a Divisione di Tempo} Questa soluzione consiste nell'assegnare l'accesso alla risorsa comune ad ogni unità U$_i$ per \textbf{istanti di tempo ben determinati}.
\pagebreak
\subsubsection{Arbitri Decentralizzati}
\paragraph{Arbitro Decentralizzato a Disciplina Circolare (Token Ring)}
\begin{center}
\includegraphics[scale=0.7]{ardectoken.png}
\end{center}
Se U$_i$ riceve 1 (token) in ingresso e deve utilizzare la risorsa allora la usa. In ogni caso, sia che debba utilizzarla sia che non debba farlo, passa il token in uscita alla prossima unità.
\paragraph{Non Deterministici} Usati soprattutto nelle reti wireless.
\begin{center}
\includegraphics[scale=0.65]{arnondet.png}
\end{center}
In questa tipologia di arbitro può succedere che due o più unità rilevino la disponibilità e inizino a trasmettere \textbf{contemporaneamente}.\\
Tale problema viene risolto con \textbf{un sistema per rilevare le collisioni}: ogni unità che trasmette ascolta ciò che l'arbitro trasmette. Se ciò che ascolta è il medesimo segnale che ha inviato allora l'invio del messaggio è andato a buon fine, altrimenti se ascolta un messaggio differente allora aspetta un lasso di tempo casuale per poi ritentare la trasmissione.
\section{Memoria Modulare}
\pagebreak
\chapter{Macchina Assembler}
In questa parte andremo a vedere alcuni moduli che sono \textbf{unità firmware a tutti gli effetti}, con una \textbf{certa struttura di interconnessione} e che \textbf{riescono ad eseguire istruzioni assembler ASM}. In sostanza, \textbf{i processori}.
\begin{center}
\includegraphics[scale=0.7]{macchinaassemblerschema.png}
\end{center}
\section{CPU}
\paragraph{Processore} Una \textbf{unità firmware in grado di eseguire operazioni esterne che sono istruzioni del linguaggio ASM}.
\paragraph{Cache} Una \textbf{memoria piccola ma molto veloce}, che \textbf{contiene il sottoinsieme della memoria principale M che permette di eseguire un certo programma}.
\paragraph{MMU} La \textbf{Memory Management Unit} è il \textbf{componente che permette di tradurre gli indirizzi logici} generati dal processore \textbf{in indirizzi fisici}
\paragraph{Logicamente} Dal punto di vista logico, un \textbf{processore P fa un ciclo infinito in cui legge l'istruzione all'indirizzo IC} (Instruction Counter) \textbf{o PC} (Program Counter), \textbf{la decodifica} e \textbf{la esegue}, poi \textbf{aggiorna IC e gestisce le interruzioni} (eventi \textbf{generati dal sottosistema I/O})
\begin{center}
\begin{lstlisting}
while(true) {
	FETCH	;legge istruzione a indirizzo IC/PC
	DECODE	;decodifica istruzione
	EXECUTE	;esegue istruzione
		;aggiorna IC/PC
	INT	;gestisci interruzioni/eccezioni
}
\end{lstlisting}
\end{center}
\pagebreak
\section{Istruzioni ASM}
\paragraph{Dati} Le istruzioni assembler (ASM) operano sostanzialmente su \textbf{due tipi di dato}:
\begin{list}{}{}
	\item \textbf{Registri Generali} RG: \textbf{pochi}, \textbf{velocissimi}, lunghi una parola, a doppia porta (permettono di leggere due locazioni e scrivere una locazione nello stesso ciclo di clock)
	\item \textbf{Locazioni di memoria}: \textbf{molte}, \textbf{lente}, lunghe una parola, \textbf{esterne al processore} che interagisce con M tramite un meccanismo domanda/risposta.
\end{list}
\paragraph{Istruzioni} I tipi di istruzioni presenti sono:
\begin{list}{}{}
	\item \textbf{Operative}: somma, shift\ldots
	\item \textbf{Accesso} alla memoria: lettura (M $\rightarrow$ RG) o scrittura (RG $\rightarrow$ M)
	\item \textbf{Salto Condizionale}, con condizione sui RG
	\item \textbf{Salto Incondizionale}
\end{list}
\section{Programmi e processi}
Assumeremo che \textbf{ogni programma ASM veda uno Spazio di Indirizzamento detto Memoria Virtuale} (MV), schematizzato di seguito.
\begin{multicols}{2}
\paragraph{MV} La \textbf{memoria virtuale} la vediamo come un \textbf{vettore} le cui \textbf{posizioni vanno da un indirizzo 0 ad un indirizzo MAX} (indirizzi logici), ognuno dei quali corrisponde ad \textbf{una parola} (32/64 bit).\\
Lo spazio viene organizzato secondo lo schema a fianco.
\columnbreak
\begin{center}
\includegraphics[scale=0.6]{progproc.png}
\end{center}
\end{multicols}
\paragraph{Da programma a processo} Quando un \textbf{programma entra in esecuzione diventa un processo}. Di seguito uno schema che racconta le fasi di un programma C che diventa un processo.
\begin{center}
\includegraphics[scale=0.6]{prog2proc.png}
\end{center}
\pagebreak
\section{Spazio di Indirizzamento Logico e Memoria Virtuale}
\paragraph{Indirizzi} Gli \textbf{indirizzi generati} dal processore, durante l'esecuzione di un processo, \textbf{non sono indirizzi della memoria principale} -- cioè \textbf{indirizzi fisici} -- ma \textbf{indirizzi logici} cioè \textbf{riferiti ad un'astrazione della memoria del processo detta Memoria Virtuale} (MV).\\
L'\textbf{insieme degli indirizzi logici di un processo è detto Spazio Logico di Indirizzamento}.\\
Il codice eseguibile del processo/programma, generato dal compilatore, è quindi riferito alla MV, ed il processore genera \textbf{indirizzi logici sia per il codice che per i dati}.
\paragraph{Rilocazione} Quando un processo viene \textbf{creato e caricato}, ad esso viene \textbf{allocata una porzione della memoria principale} la cui \textbf{ampiezza} e i cui \textbf{indirizzi non coincidono con quelli della MV} del processo, ma viene \textbf{stabilita una corrispondenza tra indirizzi logici} della MV \textbf{e indirizzi fisici} della M. Questa funzione, detta \textbf{funzione di rilocazione o traduzione} dell'indirizzo, è \textbf{implementata come una tabella associata al processo della Tabella di Rilocazione}.\\
Il processo di traduzione deve essere \textbf{molto veloce} ed è eseguito dalla \textbf{MMU} (Memory Management Unit).
\subsection{Modalità di Indirizzamento}
Vediamo come esprimere la locazione di un certo operando di un'istruzione ASM. Supponiamo di avere l'istruzione \texttt{INC X}, che incrementa un valore intero. Vediamo la \texttt{X} prima come un RG e poi come una M.
\begin{multicols}{2}
\paragraph{Registro} \texttt{RG[4]}, \texttt{R4}
\begin{center}
\includegraphics[scale=1]{indiriz_rg.png}
\end{center}
\columnbreak
\paragraph{Locazione di memoria} Ci sono diversi modi per esprimerlo:
\begin{list}{}{}
	\item \textbf{Assoluto}\\R$_i$ $\Rightarrow$ \texttt{M[RG[i]]}
	\item \textbf{Base + indice}\\R$_i$(R$_s$) $\Rightarrow$ \texttt{M[RG[i] + RG[s]]}
	\item \textbf{Relativo} ad IC/PC\\R$_i$ $\Rightarrow$ \texttt{M[IC + RG[i]]}
\end{list}
\end{multicols}
Le modalità di indirizzamento \textbf{registro} e \textbf{locazione assoluta} sono \textbf{molto simili}: la differenza sta in come si usa il registro, nel primo caso direttamente come dato e nel secondo come indirizzo di memoria.
\begin{center}
\begin{tabular}{c | c}
\textbf{Istruzioni ASM} & \textbf{Operandi} \\
\hline
Operative & Registro \\
Memoria & Base + Indice \\
Stato & Assoluto/Relativo
\end{tabular}
\end{center}
\section{RISC vs CISC}
\paragraph{RISC} \textbf{Reduced} Instruction Set Computer
\paragraph{CISC} \textbf{Complex} Instruction Set Computer
\paragraph{Dilemma} Il progettista dell'architettura di un calcolatore si è sempre trovato di fronte al dilemma "\textit{set di istruzioni semplici}" (RISC) oppure "\textit{set di istruzioni complesse}" (RISC). L'esigenza di avere un set di istruzioni complesso nasce dal desiderio di avere una \textbf{corrispondenza il più possibile uno-a-uno tra le istruzioni della macchina ASM e i comandi e le strutture dati dei linguaggi ad alto livello}.\\
In linea di principio, l'approccio CISC dovrebbe \textbf{comportare un aumento di prestazioni rispetto al RISC}, in quanto, a parità di computazione, \textbf{ciò che in RISC viene espresso con una sequenza molto lunga di istruzioni ASM}, \textbf{in CISC è espresso con poche istruzioni}.
\paragraph{Vantaggi e Svantaggi} Molti dei vantaggi potenziali ottenibili con l'approccio CISC si sono rilevati meno ovvi di quanto sembri a prima vista, talvolta addirittura tramutandosi in svantaggi. Infatti \textbf{la complessità delle istruzioni e dei modi di indirizzamento può aumentare la lunghezza delle parole dell'istruzione e il numero di accessi in memoria}.\\
Stabilire il migliore è impossibile, ma a livello didattico conviene studiare l'approccio RISC, in particolare il D-RISC.
\pagebreak
\chapter{D-RISC}
Il \textbf{Didactic-Reduced Instruction Set Computer} è un \textbf{assembler didattico di tipo RISC}. Le sue caratteristiche sono le seguenti.
\subparagraph{Registri Generali} Sono presenti \textbf{64 registri general purpose} (\texttt{RG[0 .. 63]}), cioè in cui posso scrivere qualsiasi cosa, \textbf{da 32 bit}.\\
Il registro R$_0$ è \textbf{particolare} e \textbf{contiene sempre 0 al proprio interno}.
\subparagraph{Parole} Le parole all'interno dei registri \textbf{sono da 32 bit}.\\
Questa caratteristica non è vincolante, anche avendo parole da 64 bit la struttura della macchina non cambia.
\subparagraph{Istruzioni D-RISC} In generale il D-RISC contiene \textbf{questo set di istruzioni}:
\begin{list}{}{}
	\item \textbf{Operative}: operazioni artimetico-logiche fra registri
	\item \textbf{Load/Store}: caricano dati da/in memoria
	\item \textbf{Salto} condizionato/incondizionato
\end{list}
Delle istruzioni si vedranno sintassi e formato in memoria, cioè come vengono rappresentate mediante parole da 32 bit.
\subparagraph{Memoria} La \textbf{memoria principale è indirizzabile alla parola}.
\section{Istruzioni}
\subsection{Operative}
Comprendono le istruzioni aritmetiche su interi e logiche più comuni. Sono \textbf{tutte registro--registro}.
\subparagraph{Con Operandi e risultato in RG} \texttt{ADD op1, op2, op3}\\
Es. \texttt{ADD R$_5$, R$_{27}$, R$_3$}, semantica: \texttt{R[5] + R[27] $\rightarrow$ R[3]}.\\
In memoria: \begin{tabular}{| c | c | c | c | c |}
\hline
8 bit & 6 bit & 6 bit & 6 bit & 6 bit \\
\hline
ADD & R$_5$ & R$_{27}$ & R$_3$ & \\
\texttt{CODOP} & \texttt{op1} & \texttt{op2} & \texttt{op3} & \\
\hline
\end{tabular}
\subparagraph{Con uno dei due operandi immediato} \texttt{ADDI op1, op2, op3}\\
Es. \texttt{ADDI R$_5$, \#8, R$_{27}$}, semantica: \texttt{R[5] + 8 $\rightarrow$ R[27]}\\
In memoria: \begin{tabular}{| c | c | c | c |}
\hline
8 bit & 6 bit & 6 bit & 12 bit \\
\hline
ADD & R$_5$ & R$_{27}$ & \textit{costante in comp. a 2} \\
\hline
\end{tabular}
\paragraph{} Questi tipi di operazioni posso farle con \texttt{ADD}, \texttt{SUB}, \texttt{MUL}, \texttt{DIV}, \texttt{SHR}, \texttt{SHL}.
\subparagraph{Istruzioni logiche} \texttt{AND}, \texttt{OR} e \texttt{NOT}\\
Esempio: \texttt{AND R$_1$, R$_2$, R$_3$}, semantica \texttt{R[1] AND R[2] $\rightarrow$ R[3]}\\
In memoria: \begin{tabular}{| c | c | c | c | c |}
\hline
8 bit & 6 bit & 6 bit & 6 bit & 6 bit\\
\hline
AND & R$_1$ & R$_2$ & R$_3$  & \\
\hline
\end{tabular}
\pagebreak
\subsection{Load/Store}
Non sono singole $\mu$-istruzioni perchè c'è una comunicazione tra UF (processore--memoria). Sono le \textbf{uniche istruzioni memoria--registro del D-RISC}. Come visto in precedenza, l'\textbf{indirizzo logico in memoria è calcolato come somma del contenuto di due registri generali}, uno che funge da \textbf{base} e uno da \textbf{indice}.
\begin{multicols}{2}
\begin{center}
\textbf{Sintassi}\\
\texttt{LOAD R$_B$, R$_i$, R$_X$}\\
\texttt{STORE R$_B$, R$_i$, R$_X$}
\end{center}
\columnbreak
\begin{center}
\textbf{Semantica}\\
\texttt{M[R[B] + R[i]] $\rightarrow$ R[X]}\\
\texttt{R[X] $\rightarrow$ M[R[B] + R[i]]}
\end{center}
\end{multicols}
\begin{center}
\textbf{In memoria} (analogo per entrambe):  
\begin{tabular}{| c | c | c | c | c |}
\hline
8 bit & 6 bit & 6 bit & 6 bit & 6 bit\\
\hline
LOAD/STORE & R$_B$ & R$_i$ & R$_X$  & \\
\hline
\end{tabular}
\end{center}
Inoltre, possiamo avere un'\textbf{ulteriore istruzione che realizza lo scambio del contenuto tra una locazione di memoria e un RG}.

\begin{multicols}{2}
\begin{center}
\textbf{Sintassi}\\
\texttt{EXCHANGE R$_B$, R$_i$, R$_X$}
\end{center}
\columnbreak
\begin{center}
\textbf{Semantica}\\
\texttt{M[R[B] + R[i]] $\leftrightarrow$ R[X]}
\end{center}
\end{multicols}
\begin{center}
\textbf{In memoria}:  
\begin{tabular}{| c | c | c | c | c |}
\hline
8 bit & 6 bit & 6 bit & 6 bit & 6 bit\\
\hline
EXCHANGE & R$_B$ & R$_i$ & R$_X$  & \\
\hline
\end{tabular}
\end{center}
\subsection{Salto Condizionato}
Permettono di \textbf{saltare un numero preciso di istruzioni} a seconda della condizione imposta.
\subparagraph{Confronto fra due RG} Usando gli operatori logici $>$, $<$, $=$, $\neq$, $\leq$, $\geq$\\
Sintassi, esempio con $>$: \texttt{IF$_>$, R$_i$, R$_s$, offset}\\
Semantica: se \texttt{R[i] $>$ R[s]} allora \texttt{IC + offset $\rightarrow$ IC}, altrimenti \texttt{IC + 1 $\rightarrow$ IC}.\\
\textbf{In memoria}:  
\begin{tabular}{| c | c | c | c |}
\hline
8 bit & 6 bit & 6 bit & 12 bit \\
\hline
IF$_>$ & R$_i$ & R$_s$ & offset \\
\hline
\end{tabular}
\subparagraph{Confronto fra un RG e una costante} Sempre usando gli operatori logici.\\
Sintassi, esempio con $>$: \texttt{IF$_{> 0}$, R$_i$, offset}\\
Semantica: se \texttt{R[i] $>$ 0} allora \texttt{IC + offset $\rightarrow$ IC}, altrimenti \texttt{IC + 1 $\rightarrow$ IC}.\\
\textbf{In memoria}:  
\begin{tabular}{| c | c | c |}
\hline
8 bit & 6 bit & 18 bit \\
\hline
IF$_{> 0}$ & R$_i$ & offset \\
\hline
\end{tabular}
\subsection{Salto Incodizionato}
Permettono di \textbf{saltare un numero preciso di istruzioni} senza valutare nessuna condizione.
\subparagraph{Salto con offset} \texttt{GOTO offset}\\
Semantica: \texttt{IC + offset $\rightarrow$ IC}\\
\textbf{In memoria}:  
\begin{tabular}{| c | c |}
\hline
8 bit & 24 bit \\
\hline
GOTO & offset \\
\hline
\end{tabular}
\subparagraph{Salto con RG} \texttt{GOTO R$_i$}\\
Semantica: \texttt{IC + R[i] $\rightarrow$ IC}\\
\textbf{Non c'è lo stack}, quindi è impossibile la ricorsione.
\subparagraph{Chiamata a procedura} \texttt{CALL R$_F$, R$_{RET}$}
\begin{list}{}{}
	\item R$_F$: indirizzo della funzione/procedura
	\item R$_{RET}$: indirizzo di ritorno all'uscita
\end{list}
Semantica: \texttt{R[F] $\rightarrow$ IC, IC + 1 $\rightarrow$ R[RET]}, operazioni fatte in contemporanea.\\
Solo per procedure non ricorsive!
\pagebreak
\section{Compilazione}
Vediamo una serie di esempi su come si compilano alcuni comandi presi dal linguaggio C.
\paragraph{Assegnamento}
\begin{multicols}{2}
\begin{center}
	\begin{C}
	int x;
	x = 0;
	x = 123;
	x = z;
	\end{C}
	\begin{C}
	int y[16];
	y[8] = 0;
	x = y[8];
	\end{C}
\end{center}
\columnbreak
\begin{center}
	\begin{lstlisting}
	Rx
	ADD R0, R0, Rx
	ADDI R0, #123, Rx
	;Possibile, 123 < 2^12
	ADD Rz, R0, Rx
	\end{lstlisting}
	\begin{lstlisting}
	Rby
	;Indirizzo di partenza del vettore
	STORE Rby, #8, R0
	LOAD Rby, #8, Rx
	\end{lstlisting}
\end{center}
\end{multicols}
\paragraph{Diramazione Condizionale}
\begin{multicols}{2}
\begin{center}
	\begin{C}
	if (e) {
		//body
	}
	\end{C}
	\begin{C}
	if (x == 0) {
		y = y + 1;
	}
	\end{C}
\end{center}
\columnbreak
\begin{center}
	\begin{lstlisting}
		IF not(e), cont
		;compilazione body
	cont:	;resto del programma
	\end{lstlisting}
	\begin{lstlisting}
		IF!=0 Rx, cont
		ADDI Rx, #1, Ry
	cont:	;resto del programma
	\end{lstlisting}
\end{center}
\end{multicols}
\paragraph{Diramazione Condizionale con else}
\begin{multicols}{2}
\begin{center}
	\begin{C}
	if (e) {
		//body then
	} else {
		//body else
	}
	\end{C}
	\begin{C}
	if (x == 0) {
		y = y + 1;
	} else {
		y = x - 1;
	}
	\end{C}
\end{center}
\columnbreak
\begin{center}
	\begin{lstlisting}
		IF e, then
		;compilazione body else
		GOTO cont
	then:	;compilazione body then
	cont:	;resto del programma
	\end{lstlisting}
	\begin{lstlisting}
		IF=0 Rx, then
		SUB Rx, #1, Ry
		GOTO cont
	then:	ADD Rx, #1, Ry
	cont:	;resto del programma
	\end{lstlisting}
\end{center}
\end{multicols}
\pagebreak
\paragraph{Ciclo Indeterminato}
\begin{multicols}{2}
\begin{center}
	\begin{C}
	while (e) {
		//body
	}
	\end{C}
	\begin{C}
	while (x < n) {
		y = y + x;
		x = x * 2;
	}
	\end{C}
\end{center}
\columnbreak
\begin{center}
	\begin{lstlisting}
	while:	IF not(e), cont
		;compilazione body
		GOTO while
	cont:	;resto del programma
	\end{lstlisting}
	\begin{lstlisting}
	while:	IF>= Rx, Rn, cont
		ADD Ry, Rx, Ry
		MUL Rx, #2, Rx
		GOTO while
	cont:	;resto del programma
	\end{lstlisting}
\end{center}
\end{multicols}
\paragraph{Ciclo Determinato}
\begin{multicols}{2}
\begin{center}
	\begin{C}
	for (iniz; cond; incr) {
		//body
	}
	\end{C}
	\begin{C}
	for (i = 0; i < n; i++) {
		x[i] = 0;
	}
	\end{C}
\end{center}
\columnbreak
\begin{center}
	\begin{lstlisting}
	;assumo di fare almeno un'iterazione
		;compilazione iniz
	for:	;compilazione body
		;compilazione incr
		IF cond, for
	cont:	;resto del programma
	\end{lstlisting}
	\begin{lstlisting}
	for:	STORE Rbx, Ri, R0
		ADD Ri, #1, Ri
		IF< Ri, Rn, for
	cont:	;resto del programma
	\end{lstlisting}
	Si può ottimizzare con la \textbf{tecnica dell'unrolling}: se n è pari, posso fare cicli con \texttt{i += 2}, se è multiplo di 10 con \texttt{i += 10}\ldots
\end{center}
\end{multicols}
\paragraph{Prodotto fra vettori}
\begin{multicols}{2}
\begin{center}
	\begin{C}
	sum = 0;
	for (i = 0; i < n; i++) {
		sum += x[i] * y[i]
	}
	\end{C}
\end{center}
\columnbreak
\begin{center}
	\begin{lstlisting}
		ADD R0, R0, Rsum
		ADD R0, R0, Ri
	for:	LOAD Rbx, Ri, Rxi
		LOAD Rby, Ri, Ryi
		MUL Rxi, Ryi, Rmul
		ADD Rsum, Rmul, Rsum
		ADD Ri, #1, Ri
		IF< Ri, Rn, for
	\end{lstlisting}
\end{center}
\end{multicols}
\pagebreak
\section{Processore come UF}
Vediamo come \textbf{schematizzare un processore} (P) \textbf{come una generica Unità Firmware}.
\paragraph{DMA} \textbf{Direct Memory Access}, BUS che consente alle unità di I/O di \textbf{comunicare direttamente con la memoria M}.
\begin{center}
\includegraphics[scale=1]{procuf.png}
\end{center}
\paragraph{Processore} Possiamo definire un processore come un'\textbf{unità firmware che esegue un ciclo infinito in cui si eseguono le operazioni esterne}. Le operazioni del processore possono essere descritte, in un linguaggio più "informatico", come segue:
\begin{C}
while(true) {
	fetch ISTR (IC/PC) //LOAD di una istr, ricordiamo che PC=IC stessa cosa
	decode ISTR
	exec ISTR
	trattamento interruzioni
}
\end{C}
Che diventa l'\textbf{interprete ASM in $\mu$-codice a livello firmware}.\\
Un processore può essere semplicisticamente schematizzato come segue:
\begin{center}
\includegraphics[scale=1]{procschema.png}
\end{center}
\pagebreak
\subsection{Interfaccia verso la memoria}
Di seguito lo schema dell'\textbf{interfaccia a transizione di livello del processore verso la memoria} su collegamenti dedicati.
\begin{center}
\includegraphics[scale=1.2]{interfaccia_pm.png}
\end{center}
Su altre fonti possiamo trovare due coppie di sincronizzatori (ACK e RDY), ma si assume che l'interfaccia sia a domanda/risposta, quindi possiamo usare una coppia sola.
\subsection{Interfaccia verso UNINT}
Di seguito lo schema dell'\textbf{interfaccia del processore verso l'unità UNINT}.
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=1.2]{interfaccia_punint.png}
\end{center}
\columnbreak
Se \textbf{durante l'esecuzione} del ciclo infinito \textbf{si riceve\\INT = 1}, allora \textbf{il processore deve trattare l'interruzione}: \textbf{manda ACK$_{INT}$} e \textbf{fa due LOAD} per ottenere il numero di unità I/O che ha richiesto l'interruzione e il motivo. Successivamente \textbf{invoca il driver corretto per il dispositivo}.
\end{multicols}
\pagebreak
\section{Interprete Firmware}
Scriviamo una prima versione dell'interprete firmware con il $\mu$-linguaggio, un estratto:
\begin{lstlisting}
0.	fetch -> OP, IC -> IND, set RDYM, 1

1.	(ACKM, OR(ESITO) = 0-) nop, 1
	(= 10) DATAIN -> IR, reset ACKM, 2
	;                ^^ IR = Instruction Register
	(= 11) ..., trattecc
	;           ^^^^^^^^ Trattamento eccezioni

2.	(IR.CODOP, INT = "ADD", 0) R[IR.RA] + R[IR.RB] -> R[IR.RC], IC + 1 -> IC, 0
	(= "ADD", 1) R[IR.RA] + R[IR.RB] -> R[IR.RC], IC + 1 -> IC, trattint
	;                                  Trattamento interruzioni ^^^^^^^^
	(= "LOAD", 0) read -> OP, R[IR.RA] + R[IR.RB] -> IND, set RDYM, reset ACKM, 3
	(= "IF<", 0) segno(R[IR.RA] - R[IR.RB]) -> S, 4
	...

3.	(ACKM, OR(ESITO), INT = 0--) nop, 3
	(= 100) DATAIN -> R[IR.RC], reset ACKM, IC + 1 -> IC, 0
	(= 101) DATAIN -> R[IR.RC], reset ACKM, IC + 1 -> IC, trattint
	(= 110) ..., trattecc
	
4.	(S, INT = 10) IC + IR.offset -> IC, 0
	(= 11) IC + IR.offset -> IC, trattint
	(= 00) IC + 1 -> IC, 0
	(= 01) IC + 1 -> IC, trattint
...
\end{lstlisting}
\paragraph{IR} L'\textbf{Instruction Register} è da intendersi come un \textbf{registro interno}, cioè non d'interfaccia. In questo registro andiamo a \textbf{memorizzare tutti i bit che ci indicano l'istruzione da eseguire}.\\
Con \texttt{IR.CODOP} o \texttt{IR.RB} si intendono \textbf{i soli bit che indicano il codice operativo} (operazione), i \textbf{registri} o l'\textbf{etichetta per i salti}.
\paragraph{Problema} Il problema di questa prima versione dell'interprete firmware \textbf{riguarda la decodifica del Codice Operativo} che ogni volta viene mandato alla PC del processore per gestire i vari $\alpha$, $\beta$ della PO in modo da eseguire le operazioni corrette.\\
Essendo il codice operativo un \textbf{frammento di 8 bit}, possiamo avere \textbf{una $\mu$-istruzione con ben 256 frasi alternative} che comporta un \textbf{esponenziale aumento di complessità di progettazione}.
\paragraph{Soluzione} La soluzione al problema è quella di \textbf{avere una $\mu$-istruzione che usa il codice operativo per dire al processore di andare ad eseguire quella istruzione} di $mu$-codice \textbf{che ha come indirizzo il valore del codice operativo}.\\
Quindi il codice operativo viene mandato alla PC, che \textbf{decide quale sarà il suo stato interno} (e quindi \textbf{la prossima $\mu$-istruzione}) durante il prossimo ciclo di clock. Questa tecnica si chiama \textbf{Salto Forzato}, simile per principio al controllo residuo.\\\\
A pagina seguente il codice che sfrutta questa tecnica. Notare l'evidente ottimizzazione.
\pagebreak
\begin{lstlisting}
fch0.	fetch -> OP, IC -> IND, set RDYM, fch1

fch1.	(ACKM, OR(ESITO) = 0-) nop, fch1
	(= 10) DATAIN -> IR, DATAIN.COP -> RC ;decido la prossima istruzione
	
add0.	(INT = 0) R[IR.RA] + R[IR.RB] -> R[IR.RC], IC + 1 -> IC, fch0
	(= 1) R[IR.RA] + R[IR.RB] -> R[IR.RC], IC + 1 -> IC, trattint

load0.	read -> OP, R[IR.RA] + R[IR.RB] -> IND, set RDYM, load1

load1.	(ACKM, OR(ESITO), INT = 0--) nop, load1
	(= 100) DATAIN -> R[IR.RC], IC + 1 -> IC, fch0
	(= 101) DATAIN -> R[IR.RC], IC + 1 -> IC, trattint
	(= 110) ..., trattecc
	
if<0.	segno(R[IR.RA] - R[IR.RB]) -> S, if<1

if<1.	(S, INT = 10) IC + IR.offset -> IC, fch0
	(= 11) IC + IR.offset -> IC, trattint
	(= 00) IC + 1 -> IC, fch0
	(= 01) IC + 1 -> IC, trattint
\end{lstlisting}
In questa versione abbiamo almeno \textbf{una istruzione in $\mu$-codice per ogni istruzione ASM}.\\
Si può notare che la decisione di quale $\mu$-istruzione eseguire è \textbf{determininata da \texttt{DATAIN.COP -> RC}}, \textbf{dove andiamo a scrivere gli 8 bit che determinano il codice operativo} dell'istruzione ASM \textbf{direttamente nel registro di stato della PC (RC)}, in modo da determinare quale $\mu$-istruzione eseguire immediatamente dopo.
\subsection{Valutazione delle prestazioni}
Il \textbf{tempo effettivo di accesso in memoria t$_a$} è \textbf{maggiore del ciclo di clock del processore}: per questo è uno dei principali problemi da risolvere per il raggiungimento di prestazioni elevate.
\begin{center}
\includegraphics[scale=1]{valprest.png}
\end{center}
\begin{multicols}{2}
Con il $\mu$-programma visto prima, siamo in grado di dare una valutazione del tempo di elaborazione per le istruzioni che caratterizzano il processore:
\begin{list}{}{}
	\item t$_{fch0}$ + t$_{fch1}$ = 2$\tau$ + t$_a$ \textbf{chiamata e decodifica}
	\item t$_{add}$ = t$_{sub}$ = 1$\tau$
	\item t$_{saltocond}$ = 2$\tau$
	\item t$_{saltoincd}$ = 1$\tau$
	\item t$_{load}$ = 2$\tau$ + t$_a$
	\item t$_{store}$ = 3$\tau$ + t$_a$
	\item t$_{mul}$ = t$_{div}$ = 50$\tau$ per convenzione
\end{list}
t$_a$ = 2($\tau$ + t$_{tr}$) + $\tau_m$ = 72$\tau$ accesso alla memoria molto lento.\\

\columnbreak
Tempo medio di elaborazione T, con formula vista in precedenza \texttt{$T = \tau * \sum_{i = 0}^{n - 1}(p_i * k_i) $}\\
Le probabilità come seguono:
\begin{list}{}{}
	\item Aritm-Logiche corte 40\%
	\item Aritm-Logiche lunghe 10\%
	\item Load/Store 30\%
	\item Salto 20\%
\end{list}
Inoltre le prestazioni variano anche in relazione al tipo di RAM: l'accesso a memorie dinamiche costa 60--100$\tau$, mentre a quelle statiche costa 20--40$\tau$
\end{multicols}
\pagebreak
\chapter{Superamento dei Limiti del Processore Monolitico}
Nel processore visto fin'ora come un'\textbf{unica struttura firmware} troviamo \textbf{due problemi essenziali}: il lungo tempo di accesso alla memoria e il lungo tempo di esecuzione delle istruzioni.
\paragraph{Tempo di Accesso alla Memoria} Una soluzione al problema consiste nell'\textbf{introdurre un ulteriore livello di gerarchia di memoria} tra la memoria principale ed i registri generali.\\
Tale memoria, detta \textbf{memoria cache}, è \textbf{molto piccola}, \textbf{molto veloce} ma anche \textbf{molto costosa} da realizzare. La cache \textbf{conterrà un sottoinsieme delle locazioni della memoria principale}.
\paragraph{Tempo di Esecuzione delle Istruzioni} Nel processore monolitico le varie operazioni da compiere (fetch, decode, operandi, execute, int) vengono \textbf{eseguite sequenzialmente una alla volta}. Possiamo \textbf{ottimizzare} questo comportamento \textbf{in due modi}:
\begin{list}{}{}
	\item \textbf{Processori Pipeline} o \textbf{a catena di montaggio}. Si caratterizzano per il \textbf{parallelismo nell'interprete firmware}, cioè \textbf{avere più UF che lavorano in parallelo}, ognuna con un particolare compito.
	\begin{multicols}{2}
	Spenderò tempo inizialmente per \textbf{riempire la catena di montaggio} (2$\tau$ per comunicare tra le UF), ma dopo un certo tempo \textbf{produrrà risultati in un tempo sicuramente inferiore} rispetto al processore monolitico.
	\begin{center}
		\includegraphics[scale=1]{pipeline.png}
	\end{center}
	\end{multicols}
	\item \textbf{Processori Superscalari}. Il concetto alla base è di \textbf{avere più processori che lavorano in simultanea} avendo una \textbf{memoria di registri in comune}.
\end{list}
\section{Gerarchie di Memoria}
\paragraph{Idea} Il concetto di gerarchie di memoria è \textbf{centrale alla strutturazione dei sistemi} ad un qualunque livello di astrazione.\\
All'interno di uno stesso elaboratore esistono vari tipi di memoria con caratteristiche molto diverse tra loro, quindi esiste una \textbf{gerarchia di memoria} nella quale al \textbf{livello più alto} stanno i \textbf{dispositivi di memoria più capaci}, più \textbf{lenti} e \textbf{meno costosi}, mentre man mano che \textbf{si scende di livello} i supporti hanno \textbf{capacità sempre più piccola}, \textbf{tempo di accesso inferiore} e \textbf{costo per bit maggiore}.\\\\
In termini di ottimizzazione del rapporto prestazioni/costo, l'obiettivo è di raggiungere \textbf{prestazioni paragonabili} (appena inferiori) a quelle che avremo se \textbf{tutta la massa di informazioni del sistema fosse concentrata nel solo livello inferiore}, e ad un \textbf{costo paragonabile} (appena superiore) a quello che avremmo se \textbf{tutta la massa di informazioni del sistema fosse concentrata nel solo livello più alto}.
\pagebreak
\subsection{Paginazione}
\paragraph{Memoria Virtuale -- Memoria Principale} Ritorniamo al concetto di memoria virtuale di un processo mandato in esecuzione ed ampliamo i concetti usati.
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=1]{paging.png}
\end{center}
Ogni \textbf{indirizzo logico} è \textbf{diviso in due parti}:
\begin{list}{}{}
	\item pochi bit indicano l'\textbf{offset}
	\item i restanti indicano il numero di pagina logica
\end{list}
In questo modo \textbf{suddividiamo la MV in pagine tutte della medesima dimensione}, indirizzate dall'offset.\\
Tale meccanismo è \textbf{molto utile per la traduzione} di indirizzi logici in fisici. In particolar modo quando in memoria principale vengono caricati più processi con dimensioni differenti e con tempi di esecuzione differenti. Spesso accade che un processo viene \textbf{caricato in memoria in maniera frammentaria} (cioè non è detto che gli indirizzi i e i+1 corrispondano agli indirizzi fisici s e s+1). Quindi avere un ulteriore metodo per raggruppare gli indirizzi è molto comodo.
\end{multicols}
\paragraph{Tabella di rilocazione} Ogni processo ha quindi una \textbf{tabella di rilocazione} che \textbf{associa ad ogni numero di pagina logica il numero di pagina fisica corrispondente}.\\
Ogni riga della tabella è una parola di 32 bit, usati per indicare la pagina fisica, ed 1 bit per indicare se è stata caricata o no.
\paragraph{Tipologie} La paginazione può essere:
\begin{list}{}{}
	\item \textbf{Statica}: carico tutte le pagine del processo in memoria e \textbf{ogni posizione rimane tale finché il processo è in esecuzione}
	\item \textbf{Dinamica}: carico tutte le pagine del processo in memoria, ma \textbf{può capitare che alcune pagine vengano scaricate} -- cioè messe in memoria secondaria -- \textbf{e ricaricate successivamente}. Questo metodo permette di caricare \textbf{solo le pagine utili all'esecuzione} del processo, quindi di \textbf{ottimizzare l'uso della memoria principale}.\\
	La paginazione dinamica ha due proprietà su cui basa la propria efficacia:
	\begin{list}{}{}
		\item \textbf{Località}: se al tempo t accedo a indx è molto probabile che al tempo t' non lontano da t acceda anche a indx+k con k piccolo.
		\item \textbf{Riuso}: se al tempo t accedo a indx è probabile che in un istante t' non lontano da t acceda nuovamente a indx.
	\end{list}
\end{list}
\subsubsection{Working Set}
Con \textbf{working set} si intende l'\textbf{insieme delle pagine che permettono l'esecuzione del programma in un certo istante alla massima velocità minimizzando il numero di fault}, ovvero senza dover portare in memoria principale altre pagine che si trovano in memoria secondaria.
\pagebreak
\subsubsection{Dimensione della pagina}
Adesso non resta che da capire quanto devono essere grandi le pagine. Partiamo da alcune semplici osservazioni:
\subparagraph{Pagine Piccole} $\Rightarrow$ Tante pagine
\subparagraph{Dimensione pagina}
\begin{list}{}{}
	\item \textbf{cresce} $\Rightarrow$ diminuisce la probabilità di informazioni che probabilmente accederò fra un po' in memoria
	\item \textbf{decresce} $\Rightarrow$ aumenta la probabilità che fra un po' mi serva un altra pagina
\end{list}
\paragraph{Fault di pagina} Il \textbf{pagefault} è il \textbf{tentativo di accedere ad un indirizzo non presente in memoria principale}.
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=0.65]{pagefaultplot.png}
\end{center}
Dopo un certo limite le probabilità di fault tornano a crescere poiché con \textbf{pagine troppo grandi può accadere che per caricare nuove pagine vada a scartare pagine in memoria più utili di quella caricata}.\\\\
Il grafico è valido per capacità di memoria principale limitate.
\end{multicols}
Nella \textbf{paginazione dinamica}, quando ho un pagefault e quindi ho bisogno di caricare una nuova pagina in memoria principale, \textbf{se la memoria è satura} e quindi devo liberare spazio \textbf{viene scelta la pagina da scartare secondo regole euristiche LRU} (Least Recently Used).\\
Se si tiene un \textbf{contatore} sui \textbf{secondi dopo l'ultimo accesso di una pagina per stabilire la pagina vittima} si ha un \textbf{costo molto grande sia di tempo che di spazio} (i confronti).\\
Un approccio più pratico è quello di \textbf{marchaire con un bit le pagine ogni volta che le carichiamo in memoria e azzerare il contatore dopo un certo tempo prestabilito} (pochi istanti). Quando dobbiamo trovare la pagina da scartare non si fa altro che scorrere la memoria e cercare la prima pagina con il contatore da un bit a 0.
\pagebreak
\subsection{MMU}
La \textbf{traduzione dell'indirizzo} da logico a fisico e il \textbf{controllo della protezione} -- e l'eventuale generazione dei pagefault -- sono implementate ad hardware--firmware nella \textbf{Memory Management Unit}, facente parte della CPU.\\
\textbf{Al processore} P \textbf{non è visibile come avviene la traduzione} degli indirizzi, ma deve ovviamente aver noto l'esito di ogni accesso in memoria. Come già visto, tale \textbf{esito viene esplicitamente inviato a P dalla MMU per ogni richiesta} di accesso.\\
Non è realistico pensare che, viste le dimensioni, l'intera tabella di rilocazione del processo venga copiata all'interno della MMU. Occorre realizzare in hardware una \textbf{tabella accedibile per contenuto}: questo componente è chiamato \textbf{memoria associativa}
\begin{center}
\includegraphics[scale=1]{mmu.png}
\end{center}
\subsection{Memoria Cache}
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=1]{memhierarchy.png}
\end{center}
Per rendere i calcolatori più veloci, quindi rendere l'accesso in memoria principale più rapido, usiamo delle \textbf{memorie cache} che sono \textbf{molto più piccole della memoria principale ma anche molto più veloci}.\\
L'idea è quella di \textbf{memorizzare nella cache} C \textbf{solo il working set} di un programma per rendere la traduzione degli indirizzi molto veloce.\\\\
dim(M) $>>$ dim (C)\\
t$_{aM}$ $>>$ t$_{aC}$\\
M = O(Gb), t$_a$ = O(100$\tau$)\\
C = O(Kb)/O(Mb\\\\
\textbf{Livelli di cache}\\
C$_2$ = 2--16 Mb, t$_a$ = O(10$\tau$)\\
C$_1$ = 16--32 Kb, t$_a$ = O(2--3$\tau$)
\end{multicols}
\pagebreak
Nella cache memorizziamo parole da 32 bit della memoria principale facenti parte del working set del processo in uso.
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=0.9]{cacheline.png}
\end{center}
Quando il processo mi chiede di usare una parola non ancora caricata in cache, non solo viene memorizzata in cache tale parola ma anche le parole contigue che \textbf{probabilmente verranno usate negli istanti successivi} (principio di località).\\
Tale tecnica funziona poiché la memoria principale è una memoria interlacciata e mi consente in un solo ciclo di accedere a più indirizzi.
\end{multicols}
\subsubsection{Indirizzamento diretto}
In questo caso \textbf{ogni blocco di memoria principale può essere trasferito solo in un determinato blocco della cache}: esiste \textbf{uno ed un solo blocco} della cache in cui una certa informazioni \textbf{può risiedere}.
\begin{multicols}{2}
\begin{center}
	\includegraphics[scale=0.7]{ind.png}
\end{center}
\begin{list}{}{\textbf{Procedura logica per leggere nella cache la parola di IND}}
	\item Vado nella linea di indirizzo \texttt{\# linee}
	\item Controllo che \texttt{IND.TAG = C[\# linee]}
	\item Se è vero $\rightarrow$ uso \texttt{IND.offset} per prendere la parola cercata
	\item Se è falso $\rightarrow$ \textbf{fault di cache}
\end{list}
\columnbreak
\begin{center}
	\includegraphics[scale=1]{indirzz_dir.png}
\end{center}
\end{multicols}
\paragraph{Bit di modifica} In ogni \textbf{linea di cache} è presente anche un \textbf{bit di modifica} che \textbf{indica se una delle $\sigma$ parole è stata modificata}, così da sapere che devo salvare in memoria la linea aggiornata prima di cancellarla. Vediamo com'è implementata a livello firmware.
\begin{center}
%TODO immagine
immagine da fare
\end{center}
\paragraph{Vantaggi e svantaggi} i vantaggi di questo metodo sono \textbf{la velocità} e \textbf{la semplicità}.\\
Presenta uno svantaggio che può essere molto \textbf{gravoso} e deriva dalla \textbf{rigidità della legge di corrispondenza}: quando devo accedere più volte a coppie di informazioni che stanno in blocchi della memoria principale corrispondenti alo stesso blocco della cache (quindi con \texttt{\# linee} uguali), il numero di fault è molto elevato (\textbf{trashing})
\subsubsection{Indirizzamento completamente associativo}
Questo metodo \textbf{offre la massima flessibilità circa la corrispondenza tra le linee di M e quelle di C}: ogni blocco o linea di M può risiedere in qualsiasi blocco di C.
\begin{multicols}{2}
\begin{center}
%TODO immagine
immagine da fare
\end{center}
\columnbreak
Per vedere se ho l'indirizzo memorizzato enlla cache, considero l'\textbf{AND tra il risultato dei confronti}, se è 0 allora esiste il tag giusto, sennò \textbf{fault di cache}.
\end{multicols}
\begin{center}
%TODO immagine
immagine da fare
\end{center}
Offre la massima flessibilità al prezzo di un maggiore tempo di accesso e di un aumento di costo dovuto alla memoria associativa
\subsubsection{Indirizzamento associativo su insiemi}
Questo metodo \textbf{approssima} in maniera soddisfacente sia \textbf{la flessibilità} del metodo completamente associati, sia che \textbf{la semplicità} del metodo diretto. Ogni linea di memoria viene fatta \textbf{corrispondere ad un insieme determinato di linee cache, potendo essere allocato in una qualsiasi linea di tale insieme}.\\\\
\texttt{insieme = \{gruppo di linee/blocchi di cache di dimensione k\}}
\begin{center}
%TODO immagine
immagine da fare
\end{center}
Dall'indirizzo fisico, prendo i bit che mi indicano il \texttt{\# ins} e decido quale insieme andare a guardare. Dopo prendo i \texttt{TAG} dalle linee dell'insieme in esame e li confronto con \texttt{IND.TAG}, decidendo quale linea parendere o se c'è un \textbf{fault di cache}.\\
Successivamente con \texttt{IND.offset} decido quale delle $\sigma$ parole prendere.\\\\
Di seguito l'implementazione a livello firmware con k = 2, per questioni di semplicità dello schema.
\begin{center}
%TODO immagine
immagine da fare
\end{center}
\subsubsection{Livelli di Cache}
\begin{multicols}{2}
\begin{center}
	\includegraphics[scale=1]{lvcache.png}
\end{center}
\columnbreak
\begin{list}{}{}
	\item \textbf{3° Livello di cache} -- 10 Mb -- Dati e istruzioni in una sola cache
	\item \textbf{2° Livello di cache} -- 1 Mb -- Dati e istruzioni in una sola cache
	\item \textbf{1° Livello di cache} -- 10 Kb -- Una cache per istruzioni, una cache per dati 
\end{list}
\end{multicols}
Tutte le cache che lavorano su dati (CD, C$_2$, C$_3$) usano un \textbf{metodo associativo su insiemi}, mentre la cache istruzioni può essere ad indirizzamento diretto o associativo su insiemi.\\\\
Nel 1° livello ci sono \textbf{solo i dati del processo in esecuzione} (cache molto piccola), al cambio di contesto si cancella il contenuto della cache (logicamente, la rimozione fisica richiedete tantissimo tempo, cioè \textbf{i bit di presenza vengono settati a 0}).\\
A questo livello ho una \textbf{politica on-demand}: quando ho un fault rimpiazzo una linea di cache con la linea richiesta.\\\\
Nel 2° livello si potrebbe avere una politica di \textbf{pre-fetch}: se accedo ad un indirizzo tento di portare in cache anche la linea che contiene tale indirizzo + $\sigma$ se non è già presente (\textbf{precaricamento}).\\
Quindi abbiamo dati e codice di processi diversi con politica ondemand e prefetching.\\\\
Il 3° livello è analogo al secondo, ma più capiente.
\subsubsection{Modifiche in Cache}
Abbiamo \textbf{due strategie} per effettuare le modifiche in cache:
\begin{list}{}{}
	\item \textbf{Write Back}\\
	Ogni \texttt{STORE} \textbf{modifica l'indirizzo solo in cache}, \textbf{successivamente} -- quando dobbiamo liberare la linea modificata -- \textbf{la copiamo in memoria principale} o nel livello superiore della gerarchia di memoria.
	\item \textbf{Write Through}\\
	Ogni \texttt{STORE} \textbf{modifica il dato in cache e immediatamente in memoria principale} (o nel livello superiore della gerarchia).\\
	La \textbf{scrittura in cache} è \textbf{sincrona}, mentre nel livello superiore è asincrona.
\end{list}
\pagebreak
\paragraph{Un esempio} Vediamo come esempio l'inizializzazione di un array, per valutare le prestazioni dei due metodi.
\begin{multicols}{2}
\begin{C}
for (i = 0; i < n; i++) x[i] = 1;
\end{C}
\begin{lstlisting}
1. loop:	STORE Rbx, Ri, R1
2.		INC Ri
3.		IF< Ri, Rn, loop
4.		END...
\end{lstlisting}
\columnbreak
\textbf{Costo}\\
	1. 5$\tau$ + 2t$_a$\\
	2. 3$\tau$ + t$_a$\\
	3. 4$\tau$ + t$_a$\\
	= 12$\tau$ + t$_a$ = 20$\tau$
\end{multicols}
\begin{list}{}{Facendo l'ipotesi che lavoriamo su cache di primo livello C$_1$:}
	\item CD (Associativo su insiemi): 2$\tau$
	\item CI (Diretto): 2$\tau$
\end{list}
\paragraph{Write Through} Per il metodo write through, perché funzioni devo \textbf{verificare che la banda della \texttt{STORE} sia minore o uguale della banda del livello successivo della gerarchia di memoria}.
\paragraph{Write Back} Nel caso del write back \textbf{non ho bisogno di fare questa verifica} poiché \textbf{vado a scrivere nel livello successivo della gerarchia di memoria solo quando ho un fault} e quindi finché non è stato copiato il tutto il processore non riceve le istruzioni, quindi non genera ulteriori modifiche.
\subsubsection{Modello di costo}
\paragraph{Dal file ASM} \texttt{\# istruzioni * durata}\\
$\Rightarrow$ Tempo di completamento ideale (accessi in cache) = T$_{cid}$\\
$\Rightarrow$ Tempo di completamento reale T$_c$ = T$_{cid}$ + \#Fault * T$_{tranf}$.
\paragraph{Dove} T$_{tranf}$: tempo di trattamento del singolo fault, dipende unicamente dall'hardware.\\
\#Fault: dipende dal codice (working set) e in parte dall'hardware (capacità della cache).\\\
T$_{tranf}$ = 2(T$_{tr} + \tau$) + $\frac{\sigma}{m}\tau_m$ ( oppure, se T$_{tr}$ > $\tau_m$, = 2(T$_{tr} + \tau$) + $\frac{\sigma}{m}T_{tr}$ )\\
\textbf{Sempre proporzionale a $\frac{\sigma}{m}$}, con T$_{tr}$ tempo di trasferimento da cache a memoria e m le parole lette alla volta.
\section{I/O}
\begin{center}
	\includegraphics[scale=1]{procuf.png}
\end{center}
\pagebreak
Ogni unità I/O svolge il compito di interfacciare, nei confronti dell'Unità Centrale, un certo tipo di dispositivo periferico come: hard disk, stampanti, interfacce di rete\ldots.\\
\paragraph{Categorie} Possiamo dividere le unità I/O in due categorie:
\begin{list}{}{}
	\item \textbf{Carattere}: unità che trasferiscono \textbf{pochi byte} per ogni operazione effettuata, ad esempio il mouse
	\item \textbf{Blocchi}: unità che lavorano con una \textbf{grossa quantità di dati}, ad esempio hard disk o interfacce di rete.\\
	Per queste unità è previsto un \textbf{bus DMA} che gli \textbf{permette di lavorare con i dati della memoria principale indipendentemente dal lavoro che sta svolgendo il processore} in quel momento.
\end{list}
L'\textbf{interazione} tra processore e il \textbf{sottosistema I/O} avviene \textbf{mediante i segnali di interruzione}. Questo è dovuto al fatto che i tempi di reazione delle unità periferiche sono molto maggiori rispetto al tempo di reazione del processore P, quindi non vale la pena fare attendere P o fare polling di ogni dispositivo.
\subsection{Trasferimento dati}
Esistono due modelli principali che descrivono il trasferimento dei dati tra processore/memoria e le periferiche I/O: \textbf{Memory Mapped I/O} e \textbf{uso di istruzioni speciali}.\\\\
Nello spazio di memoria virtuale, una certa porzione viene dichiarata come "riservata" e la \textbf{mappo sull'I/O}.\\
Se questa mappatura avviene in maniera per cui io \textbf{posso direttamente usare l'indirizzo della MMU}, che interpreta l'indirizzo del sottosistema I/O allora \textbf{parliamo di Memory Mapped I/O}.\\
Se invece \textbf{uso delle istruzioni speciali} allora parliamo di istruzioni speciali, meccanismi di comunicazione tra processore le periferiche, che usano quindi\textbf{interfacce dedicate}.
\paragraph{Memory Mapped I/O} Per il MMI/O il meccanismo di comunicazione/esecuzione con I/O è il seguente.\\
Si inizia con delle \texttt{STORE} con \textbf{il comando da eseguire sulla periferica} con i suoi parametri, nella memoria virtuale che rappresenta la memoria dell'unità I/O.\\
Dopo il processore deve \textbf{ordinare il comando alla periferica}: questo avviene come una \texttt{STORE \#1} sul registo \texttt{RDY} della periferica.\\
Successivamente il processore attende una interruzione, nel mentre può eseguire altri processi. Quando avviene l'interruzione, si eseguono delle \texttt{LOAD} sulla memoria mappata dell'I/O per leggere i risultati.
\section{Trattamento delle Interruzioni}
Quando una periferica I/O richiede una interruzione, manda una \textbf{richiesta di interruzione all'unità \texttt{UNINT}}. Se l'\textbf{unità di arbitraggio} \texttt{UNINT} \textbf{accetta la richiesta}, manda il segnale \texttt{INT} (interruzione) al processore che provvede a mandare un \texttt{ACK}.\\
\texttt{UNINT} manda anche un segnale all'unità I/O che ha vinto l'arbitraggio, che è libera di mandare alla MMU due parole: \textbf{numero dell'unità} -- per capire quale unità ha richiesto l'interruzione -- e \textbf{la ragione dell'interruzione}.\\
La MMU, a questo punto, manda le due parole ricevute al processore come se fossero due LOAD.
\paragraph{Nell'interprete}
\begin{lstlisting}
trattint.	reset INT, set ACKINT, trattint1
trattint1.	(ACKM, OR(ESITO) = 00) nop, trattint1
		(= 10) DATAIN -> R[61], reset ACKM, SET RDYM, trattint2		*
trattint2.	(ACKM, OR(ESITO) = 00) nop, trattint2
		(= 10) DATAIN -> R[62], IC -> R[63], set RDYM, reset ACKM,
		R[60] -> IC, fch0 ** ;da qui fase ASM
\end{lstlisting}
\begin{list}{}{}
	\item \textbf{*} Ricevuto il codice dell'evento, questo viene passato alla Routine d'Interfacciamento Interruzioni via R[61].
	\item \textbf{**} Ricevuta la seconda parola del messaggio di I/O, questa viene passata alla Routine d'Interfacciamento Interruzioni via R[62]. Viene poi salvato l'indirizzo di ritorno dalla routine ed effettuato il salto alla routine stessa.
\end{list}
Quando ho una interruzione, \textbf{smetto di fare ciò che sto facendo} e vado ad eseguire il codice che tratta questa interruzione \textbf{eseguendo solo quel codice} che deve essere molto veloce.
\pagebreak
\subsection{DMA I/O}
\paragraph{Direct Memory Access} La memoria principale ha due interfacce: una che comunica con il processore e una che comunica con il bus DMA. Quindi M è un'unità attiva che \textbf{contiene un arbitro} per stabilire se comunicare con P o il Bus.\\
Un esempio con una periferica I/O come un hard disk. Come primo passo vengono memorizzati operazioni e parametri via MM I/O sulla periferica, dopo di memorizza "GO" in RDY della periferica.\\
Adesso la periferica I/O completa l'operazione, accede al bus DMA ed esegue il ciclo di trasferimento da M a M I/O. A questo punto richiede una interruzione ed attende che sia accolta.
\pagebreak
\chapter{Livello dei processi}
Un processo può essere immaginato \textbf{come l'esecuzione di un programma}. Una tipica situazione è quella in cui si \textbf{distinguono due sottoinsiemi} di processi:
\begin{list}{}{}
	\item \textbf{Processi di Sistema}\\
	Sono processi che \textbf{esistono permanentemente nel sistema} e sono \textbf{delegati alla gestione di risorse e servizi} nei confronti di richieste dalle applicazioni. Possono anche cooperare tra loro oltre che con le applicazioni.
	\item \textbf{Processi Applicativi}\\
	Derivano dalla \textbf{compilazione} e dalla \textbf{richiesta di esecuzione di programmi applicativi}. In generale \textbf{nascono e muoiono}.
\end{list}
L'interazione fra processi può avvenire in due modi distinti e molto differenti tra loro: a \textbf{scambio di messaggi} -- cooperano spedendo informazioni -- o a \textbf{memoria condivisa} -- più processi con accesso alle medesime locazioni di memoria, con opportune tecniche di traduzione degli indirizzi.
\section{Supporto a tempo di esecuzione}
Tale supporto definisce la macchina virtuale che, rispetto alla macchina sottostante (assembler, firmware, hardware) possiede in più il concetto di processo e di meccanismi di concorrenza e di cooperazione tra processi.
\paragraph{Descrittore di processo} Il \textbf{Process Control Block} (\textbf{PCB}) è l'insieme delle informazioni che permette al sistema di gestire il processo. All'interno troviamo i seguenti campi:
\begin{list}{}{}
	\item \textbf{Stato del processo}: indica se il processo è in esecuzione, attesa di una periferica o potrebbe essere in esecuzione ma in quel momento non può usare il processore.
	\item \textbf{Area Salvataggio dei registri}: area di memoria dove ci sono le struttura dati in cui, dal momento in cui decidiamo che un processo non ha più il controllo della CPU, copiamo il contenuto dei registri e dell'IC. Questo in modo da poter rimandare in esecuzione il processo dal punto dove è stato sospeso.
	\item \textbf{Puntatore alla tabella di rilocazione}: così che quando il processo è in esecuzione la MMU possa tradurre correttamente gli indirizzi logici del processo.
	\item \textbf{Riferimento alla lista dei processi "pronti"}
	\item \textbf{Riferimento al processo in esecuzione}
	\item \textbf{Riferimenti alle strutture condivise}: per interazioni fra processi a memoria condivisa.
\end{list}
\pagebreak
\section{Schedulazione a basso livello}
Con tali informazioni si organizza la \textbf{schedulazione a basso livello}, ovvero il \textbf{complesso di funzionalità per la gestione degli stati di avanzamento dei processi} e quindi la gestione della risorse processore.\\
Un tipico schema di avanzamento è il seguente:
\begin{center}
\includegraphics[scale=0.8]{processistati.png}
\end{center}
\begin{list}{}{}
	\item \textbf{Creazione}\\
	Crea un PCB\\
	Carica almeno la prima pagina di codice in memoria principale, modificando la tabella di rilocazione del processo\\
	Mette il PCB nella lista dei processi pronti
	\item \textbf{Ready} $\longrightarrow$ \textbf{Exec}\\
	Copia di registri e IC da PCB nei registri firmware\\
	Istruzione particolare per caricare IC\\
	Istruzione particolare che passa l'indirizzo della tabella di rilocazione alla MMU
	\item \textbf{Exec} $\longrightarrow$ \textbf{Ready}\\
	Terminazione attivata da un timer\\
	Salvataggio dei registri e IC nel PCB\\
	PCB viene messo in coda alla lista dei processi pronti
	\item \textbf{Exec} $\longrightarrow$ \textbf{Wait}\\
	Terminazione attivata da una richiesta I/O\\
	Salvataggio dei registri e IC nel PCB\\
	Registro il PCB come in attesa
	\item \textbf{Wait} $\longrightarrow$ \textbf{Ready}\\
	Transizione su completamento dell'operazione I/O\\
	Inserimento in fondo alla lista dei processi pronti
	\item \textbf{Terminazione}\\
	Cancellazione del PCB
	\item \textbf{Wait} $\longrightarrow$ \textbf{Exec} (\textbf{Priorità})\\
	Transizione attivata dal completamento dell'operazione I/O per un processo a priorità maggiore di quello in esecuzione.\\
	Copia dei registri e IC nei registri firmware e passaggio dell'indirizzo della tabella di rilocazione ad MMU
	\item \textbf{Exec} $\longrightarrow$ \textbf{Ready} (\textbf{Prerilascio})\\
	Mette il PCB \textbf{in testa} alla coda dei processi pronti.\\
	Salvataggio dei registri e IC nel PCB
\end{list}
\pagebreak
\section{Istruzione speciale Start-Process (D-RISC}
Questa istruzione \textbf{conclude la fase di commutazione di contesto}: prende due parametri:
\begin{list}{}{}
	\item R$_{IC}$, che contiene l'indirizzo della prima istruzione da eseguire quando il processo andrà in esecuzione
	\item R$_{tabRil}$ che contiene l'indirizzo della tabella di rilocazione del processo da inviare alla MMU
\end{list}
\begin{lstlisting}
startp0.	"startprocess" -> OP, R[IR.Rtabril] -> IND, set RDYM, startp1
startp1.	(ACKM, OR(ESITO) = 0-) nop, startp1.
		(= 10) R[IR.Ric] -> IC, fch0
		(= 11) R[IR.Ric] -> IC, trattint
\end{lstlisting}
Invio alla MMU le informazioni necessarie a riferire la tabella di rilocazione del processo che entra in esecuzione. Contemporaneamente ripristino il contenuto di IC all'indirizzo logico della prima istruzione da cui il processo deve riprendere o iniziare l'esecuzione.\\
Ciò permette di effettuare in maniera atomica tutte le azioni necessarie a iniziare l'esecuzione di un processo una volta che i registri generali sono stati ripristinati.
\section{Commutazione di contesto}
Trattiamo la \textbf{commutazione di contesto} che avviene alla \textbf{fine del quanto di tempo}.
\paragraph{Situazione} Abbiamo un processo ProcA \textbf{in esecuzione}. Quando il tempo di esecuzione finisce, il registro \textbf{INT diventerà 1} quindi si salterà al \textbf{trattamento dell'interruzione} (\texttt{trattint}): l'\textbf{IC corrente verrà salvato in R[63]} (fase firmware) ecc\ldots\\
Successivamente si passa alla \textbf{fase assembler} ASM, dove vengono eseguiti i seguenti passaggi:
\begin{list}{}{}
	\item \textbf{Salvare lo stato} di ProcA in esecuzione (una serie di \texttt{STORE} per da PCB+posizione\ldots)
	\item \textbf{Posizionare PCB$_A$ in coda} alla lista dei processi pronti
	\item \textbf{Ripristino lo stato del primo processo nella lista pronti} nei registri del processore (una serie di \texttt{LOAD})
	\item \textbf{Rimuovere il puntatore al processo nella lista pronti} e \textbf{aggiorno lo stato della EXEC}
	\item \textbf{Start-Process}, quindi passo IC e l'indirizzo della tabella di rilocazione del nuovo processo in esecuzione
\end{list}
\paragraph{Commuta Contesto} Tutti i passaggi prendono il nome di \textbf{commuta contesto}. Tale codice viene eseguito quando è sempre attiva la tabella di rilocazione di A, quindi il codice di commuta contensto deve esserer \textbf{presente nella memoria virtuale di A} -- e in tutti gli altri processi. Inoltre, \textbf{tutti i PCB devono essere accessibili a tutti i processi} per completare la commutazione di contesto.\\\\
Tutti i passaggi fin'ora descritti vengono \textbf{effettuati con la MMU che lavora mediante la tabella di rilocazione di A}. Sarà la \textbf{start-process ad occuparsi di cambiare la tabella di rilocazione con quella del nuovo processo}, da cui sorge un problema: \textbf{che tipo di indirizzo} (logico o fisico) \textbf{devo mandare come parametro alla start-process per la nuova tabella di rilocazione}?\\
Quando inizia a lavorare, la start-process deve \textbf{liberare la cache della MMU} (settando tutti i bit P a 0) per poter caricare i valori della nuova tabella di rilocazione. Quindi l'\textbf{indirizzo passato come parametro non può essere logico} perché in tale situazione \textbf{non c'è modo di tradurlo in indirizzo fisico dato che la cache della MMU è "vuota"}.\\
Quindi il parametro della start-process \textbf{deve essere un indirizzo fisico}.\\\\
Nella PCB è memorizzato l'indirizzo logico della tabella di rilocazione del processo, quindi \textbf{serve un meccanismo che traduca molto velocemente tale indirizzo in fisico}: prendo la pagina logica e con essa vado a chiedere dove si trova in memoria quella pagina, ci metto l'indirizzo di pagina fisica e rimando il tutto come indirizzo fisico alla start-process.
\pagebreak
\begin{center}
	\includegraphics[scale=0.7]{commconttabril.png}
\end{center}
\paragraph{Problema} Il problema di questo meccanismo è che \textbf{per capire dove sia l'indirizzo fisico} della tabella di rilocazione del nuovo processo, \textbf{devo convertire l'indirizzo logico con la tabella di rilocazione del nuovo processo} e \textbf{non con quella del vecchio processo}.\\
Quindi \textbf{da ogni processo deve essere accessibile l'indirizzo logico delle tabelle di rilocazione di ogni alto processo}.
\section{Condivisione indirizzi tra processi}
Vediamo come due o più processi possono \textbf{usare indirizzi in comune}.\\
I vari indirizzi logici dei vari processi devono essere tradotti nell'unico indirizzo fisico dove risiede realmente il dato in comune. Abbiamo \textbf{tre soluzioni} per ottenere quei dati:
\begin{list}{}{}
	\item \textbf{Gli indirizzi logici in A e B sono gli stessi} e corrispondono alla stessa locazione di memoria fisica. Come se \textit{TabRil(x)$_A$ = TabRil(x)$_B$}.
	\item \textbf{Gli indirizzi logici di x in A e B} sono diversi ma \textbf{puntano alla stessa locazione di memoria}.
	\item \textbf{Capability}: il \textbf{processo A comunica l'indirizzo fisico della variabile condivisa al processo B} in modo che possa accedere a tale variabile.\\
	Il processo A deve calcolare quindi l'indirizzo fisico di X e conosce R$_{baseX}$ (cioè pagina indirizzo di pagina logica ed offset), R$_{TabRil}$, R$_{PCB}$
\end{list}
\begin{lstlisting}
					;Rbasex = [IPL | offset]
SHR	Rbasex, # bit offset, R1	;[0000 | IPL]
LOAD	Rtabril, R1, R2			;[IPF | ////]
AND	Rbasex, #00..011..1, R3		;[0000 | offset] Tanti 1 quanti i bit offset
AND	R2, #11..100..0, R4		;[IPF | 0000]
ADD	R3m R4, Rindfis			;[IPF | offset]
;IPF = indirizzo pagina fisica
;IPL = indirizzo pagina logica
\end{lstlisting}
Con R$_{indfis}$ non posso fare nè \texttt{LOAD} nè \texttt{STORE} perché la MMU interpreterebbe tale indirizzo come logico, quindi lo andrebbe a tradurre, ma posso mandare tale indirizzo al processo B.\\
Il processo B \textbf{deve trattare R$_{indfis}$ come dato e non come indirizzo}, quindi dobbiamo trovare un modo per \textbf{convertire tale indirizzo fisico in logico seguendo la tabella di rilocazione del processo B}. La soluzione è quella di \textbf{scrivere i bit IPF di R$_{indfis}$ nella prima posizione libera della tabella di rilocazione di B}, in modo tale che accedendo a quella posizione la MMU possa correttamente tradurre l'indirizzo logico in fisico e accedere alla variabile condivisa.
\begin{center}
	\includegraphics[scale=0.75]{indcondivisi.png}
\end{center}
\pagebreak
\chapter{Elaborazione in parallelo}
Fin'ora abbiamo studiato il comportamento, costo e prestazioni di un \textbf{processore monolitico} che \textbf{esegue le istruzioni una dopo l'altra}.\\
Abbiamo anche detto che le prestazioni di un processore cono calcolate secondo la funzione P = f(K, $\tau$, t$_a$) dove K è il numero di cicli di clock necessari per eseguire una istruzione, $\tau$ è la lunghezza del ciclo di clock e t$_a$ è il tempo di accesso alla memoria principale.\\
Il tempo di accesso alla memoria è già stato "ottimizzato" introducendo la cache e la gerarchia di memoria. Cerchiamo adesso di \textbf{diminuire il K e in misura minore il $\tau$}, introducendo nuovi tipi di processore che riescono ad \textbf{operare in parallelo}.
\section{Forme di parallelismo}
\subsection{Pipeline}
La forma di parallelismo \textbf{pipeline} è definita come \textbf{una catena di Stadi Interconnessi fra loro}, che \textbf{lavorano ognuno producendo qualcosa che verrà preso in ingresso dallo stadio successivo}.\\
\textbf{Ogni stadio} del processore \textbf{ha un compito ben preciso} per arrivare ad un risultato.\\
\begin{list}{}{Definiamo alcune \textbf{metriche} per capire quali sono le prestazioni di questa tecnica:}
	\item \textbf{Latenza} -- L\\
	\textbf{Tempo per completare un calcolo} da quando inizia a quando finisce. Ogni stadio i ha una latenza L$_i$.
	\item \textbf{Tempo di servizio} -- T$_S$\\
	\textbf{Tempo che intercorre fra l'invio di due risultati successivi}. Nel processore monolitico L = T$_S$
	\item \textbf{Throughput} -- B = $\frac{1}{T_S}$
	\item L$_{pipe}$ = $\sum_{i=1}^{n stadi} (L_i + t_{comm})$, dove:
	\begin{list}{}{}
		\item L$_i$ è la latenza del singolo stadio
		\item t$_{comm}$ è la latenza di comunicazione
	\end{list}
	\item T$_{Spipe}$ = max\{T$_{Si}$\}
	\item \textbf{Tempo di completamento} -- T$_C$\\
	\textbf{Tempo fra l'arrivo del primo input e l'uscita dell'ultimo output}.
	\item T$_{Cpipe}$ $\simeq$ m * T$_S$, con m numero di task
\end{list}
La migliore condizione per una pipeline è \textbf{quando tutti gli stadi hanno lo stesso tempo di servizio}.
\paragraph{Efficienza} Su tutte queste metriche andiamo a misurare l'\textbf{efficienza}, ovvero il \textbf{rapporto tra il tempo ideale T$_{id}$ e il tempo misurato T(n)}.\\
$\varepsilon$(n) = $\frac{T_{id}(n)}{T(n)}$ --- Quando m $>>$ n allora $\varepsilon \rightarrow$ 1 ( $\varepsilon$ = 1 è ideale) --- T$_{id}$ = $\frac{T_{seq}}{n}$
\pagebreak
Se supponiamo di trasformare un modulo (sistema) sequenziale in una struttura parallela equivalente con grado di parallelismo n, ha senso parlare di \textbf{scalabilità} o \textbf{speed-up}: lo speed-up usa al numeratore il miglior tempo sequenziale, mentre la scalabilità usa il tempo di un gradi di parallelismo pari a 1.
\begin{center}
sp(n) = $\frac{T_{seq}}{T(n)}$ --- sc(n) = $\frac{T(1)}{T(n)}$\\
\includegraphics[scale=0.8]{spscefficienzaplots.png}\\
T$_{id}$ = $\frac{T_{seq}}{n}$\\$\varepsilon(n)$ = $\frac{T_{id}}{T(n)}$ = $\frac{T_{seq}}{n * T(n)}$ = $\frac{sp(n)}{n}$ $\Rightarrow$ sp(n) = $\varepsilon$(n) $*$ n
\end{center}
\subsection{Farm}
\paragraph{Replicazione Funzionale} Questa forma di parallelistmo lavora \textbf{dividendo i dati in ingresso su vari stadi in modo che essi lavorino parallelamente}.
\begin{center}
	\includegraphics[scale=1]{farm.png}
\end{center}
\begin{multicols}{2}
\begin{list}{}{}
	\item t$_f$ = latenza calcolo di f
	\item L$_{farm}$ = t$_{sched}$ + t$_f$ + t$_{collez}$
	\item T$_{Sfarm}$ = max\{t$_{sched}$, $\frac{t_f}{n}$, t$_{collez}$\}
	\item T$_{Cfarm}$ = (n - 1)$\frac{t_f}{n}$ + $\frac{m}{n} * t_f$
	\item $\Rightarrow$ T$_{Cfarm}$ = (n - 1)$T_S$ + $\frac{m}{n} * T_S$
\end{list}
	\includegraphics[scale=0.43]{farmgraph.png}
\end{multicols}
\paragraph{Rimozione di colli di bottiglia} Se nella pipeline ho uno stadio che impiega molto più tempo dei restati, posso dividere quello stadio in K worker usando una replicazione funzionale (farm) per abbassare il suo T$_S$ in modo da renderlo simile agli altri.
\pagebreak
\subsubsection{Teoria delle Code}
Un sistema a coda modella il comportamento di un \textbf{servente S} (\textbf{centro di servizio}) al quale \textbf{uno o più clienti C$_1$, \ldots, C$_n$ si rivolgono} attendendo in una \textbf{fila di attesa Q} di ricevere il servizio erogato.
\begin{center}
\includegraphics[scale=1]{coda.png}
\end{center}
\begin{list}{}{}
	\item t$_a$ \textbf{tempo di interarrivo}
	\item t$_S$ \textbf{tempo di servizio}
	\item 1 task ogni t$_a$ dalla coda\\
	1 task ogni t$_S$ dal sistema
	\item \textbf{Fattore di utilizzo} $p = \frac{t_S}{t_a}$\\
	L'obiettivo è avere questo valore $< 1$, altrimenti arriverebbero in cosa più task di quanti se ne riescano ad elaborare (\textbf{collo di bottiglia}).\\
	Chiaramente si intende un valore \textbf{molto vicino a 1}, altrimenti avrei \textbf{tempi dove non arrivano task ed il servente non processerebbe nulla} quindi uno spreco di risorse.
\end{list}
\begin{multicols}{2}
\begin{center}
\textbf{Serventi multipli}\\
\includegraphics[scale=1]{servmult.png}
\end{center}
\begin{center}
\textbf{Clienti multipli}\\
\includegraphics[scale=1]{clientimulti.png}
\end{center}
\end{multicols}
\pagebreak
\section{Processore}
\subsection{Processore Pipeline}
Il concetto alla base di questa architettura è la \textbf{parallelizzazione della CPU mediante la parallelizzazione dell'interprete firmware}, eseguito dal processore con la collaborazione delle altre unità della CPU stessa.\\
Per scopo didattico, useremo una visione semplificata dell'architettura composta da soli \textbf{quattro stadi}.
\begin{center}
\includegraphics[scale=1]{procpipeline.png}
\end{center}
\begin{list}{}{}
	\item \textbf{Instruction Memory} -- IM\\
	Unità memoria dedicata a \textbf{memorizzare solo istruzioni} e quindi a \textbf{compiere operazioni di fetch}.\\
	Dotata di MMU propria.
	\item \textbf{Instruction Unit} -- IU\\
	\textbf{Cuore vero e proprio} del processore, dove le \textbf{istruzioni che provengono da IM vengono decodificate ed eseguite}. Si collega alla IM per le istruzioni di salto.
	\item \textbf{Data Memory} -- DM\\
	Unità memoria dedicata a \textbf{memorizzare solo dati}. Il collegamento dalla IU è dato dalla decodifica di istruzioni di \texttt{LOAD} e \texttt{STORE}.
	\item \textbf{Execution Unit} -- EU\\
	Unità che si occupa di \textbf{eseguire istruzioni aritmetico-logiche}. Il collegamento dalla DM è dato per concludere le istruzioni di \texttt{LOAD}.
\end{list}
\pagebreak
\begin{multicols}{2}
\begin{center}
\includegraphics[scale=0.8]{procpipelineint.png}
\end{center}
La IM ha al suo interno la propria MMU$_I$, per la traduzione degli indirizzi delle istruzioni, e la cache istruzioni CI. Tale unità è collegata tramite la MMU$_I$ alla IU, per prelevare IC nel caso di istruzioni di salto, e alla \textbf{memory interface MINF} per dialogare con il sottosistema di memoria.\\
Analogamente la DM avrà una propria MMU$_D$ e una propria cache dati CD. Anch'essa è collegata tramite MMU$_D$ alla IU e a MINF.\\
La EU ha una copia dei registri per effettuare le operazioni aritmetico-logiche.\\
Anche IU ha una copia dei registri di EU (il più aggiornati possibile) per alcune operazioni di decodifica.
\end{multicols}
\subsubsection{Istruzioni operative}
Le \textbf{istruzioni operative compieranno sempre il giro IM $\rightarrow$ IU $\rightarrow$ EU}.\\
IM, usando la copia che ha di IC, manda una coppia \texttt{<istr, IC'>} -- dove \texttt{istr} è l'istruzione da eseguire e \texttt{IC'} indica da dove è stata presa e che serve per trattare le interruzioni di salto.\\
IU invia alla EU una \textbf{istruzione decodificata}, cioè una \textbf{tupla di valori} -- per esempio \texttt{ADD R$_1$, R$_2$, R$_3$} $\rightarrow$ \texttt{<+, 1, 2, 3>} dove 1, 2, 3 corrispondono a indirizzi di registri.\\
EU esegue l'istruzione e manda il risultato alla IU "avvertendola" che una copia di registri deve essere aggiornata con il risultato appena calcolato.
\subsubsection{Istruzioni di salto}
Le istruzioni di \textbf{salto incondizionato compieranno il giro IM $\rightarrow$ IU}.\\
IM manda \texttt{<istr, IC'>} -- per esemptio \texttt{GOTO R$_4$}.\\
IU và nella propria copia dei registri, accede al contenuto del registro R$_4$ e invia alla IM un messaggio dicendo che il nuovo IC deve essere il contenuto del registro, quindi la IM aggiorna IC.
\subsubsection{Meccanismi per sincronizzazione}
Partiamo da un esempio per capire il problema. Consideriamo il seguente set d'istruzioni:
\begin{center}
\includegraphics[scale=0.7]{pipelinesincprobl.png}
\end{center}
Si può notare che al passo 3 è presente un problema di sincronizzazione poiché, benché nel codice abbiamo una istruzione di salto, la IM manda alla IU l'istruzione successiva "quasi" ignorando il salto, mentre la IU manda alla IM il valore nuovo di IC.
\pagebreak

Per ogni registro affianchiamo un contatore C, il quale funge da indicatore della correttezza del contenuto del registro.\\
Inizialmente C = 0, quando dalla IU alla EU mando un'istruzione che scriverà in quel registro, incremento il contatore. Quando dalla EU si riceverà il risultato aggiornato lo decremento.\\
La IU leggerà il registro $\Leftrightarrow$ C == 0, altrimenti si blocca.
\subsubsection{Meccanismi di gestione della copia IC e IC'}
IU aggiorna costantemente il suo Instruction Counter, invia IC a IM ogni volta che questo viene aggiornato e scarta le istruzioni che arrivano da IM e che non hanno IC' della tupla uguale al proprio valore IC.
\subsubsection{Istruzioni LOAD}
Le istruzioni \texttt{LOAD} compiono il giro IM $\rightarrow$ IU $\rightarrow$ DM $\rightarrow$ EU $\rightarrow$ IU.\\
IM compie la fetch dell'istruzione di \texttt{LOAD} e la manda alla IU\\IU manda un ordine di caricamento alla DM \texttt{<load, ind>} -- dove \texttt{ind} è calcolato dalla IU con \texttt{R$_{base}$ + R$_{ind}$}. Contemporaneamente manda a EU un messaggio che indica di aver ordinato una \texttt{LOAD} sul registro R$_x$ \texttt{<load, R$_x$>}. Infine \textbf{aggiorna il contatore di R$_x$}.\\
DM compie la \texttt{LOAD} e la manda alla EU.\\
EU prende il valore mandato da DM e lo mette nel registro R$_x$. Manda il nuovo valore R$_x$ a IU e aggiorna il contatore di R$_x$.
\begin{multicols}{2}
\begin{lstlisting}
LOAD	R1, R2, R3
ADD	R3, R4, R5
\end{lstlisting}
Posso eseguire subito l'istruzione perché il contenuto di R$_3$ è preso dalla EU, quindi già aggiornato
\columnbreak

\begin{tabular}{c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & \\
	\hline
	\textbf{IM} & \texttt{LOAD} & \texttt{ADD} & & & & \\
	\hline
	\textbf{IU} & & \texttt{LOAD} & \texttt{ADD} & & & \\
	\hline
	\textbf{DM} & & & \texttt{LOAD} & $\searrow$ & & \\
	\hline
	\textbf{EU} & & & &\texttt{LOAD} & \texttt{ADD}  & \\
	\hline
\end{tabular}
\end{multicols}
\subsubsection{Istruzioni STORE}
Le istruzioni di \texttt{STORE} compiono il giro IM $\rightarrow$ IU $\rightarrow$ DM.\\
IM compie la fetch dell'istruzione di \texttt{STORE} e la manda alla IU.\\
IU manda un messggio \texttt{<STORE, ind, val>} alla DM che si occupa di concludere l'istruzione.
\begin{multicols}{2}
\begin{lstlisting}
STORE	R1, R2, R3
ADD	R3, R4, R5
\end{lstlisting}
\columnbreak

\begin{tabular}{c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & \\
	\hline
	\textbf{IM} & \texttt{STORE} & \texttt{ADD} & & & & \\
	\hline
	\textbf{IU} & & \texttt{STORE} & \texttt{ADD} & & & \\
	\hline
	\textbf{DM} & & & \texttt{STORE} & & & \\
	\hline
	\textbf{EU} & & & &\texttt{STORE} & \texttt{ADD}  & \\
	\hline
\end{tabular}
\end{multicols}
\pagebreak
\subsection{Dipendenze Logiche}
Una \textbf{istruzione I induce una dipendenza logica su una istruzione J quando I produce x sull'unità U$_a$ che è letta da J sull'unità U$_b$}, quindi scrittura e lettura avvengono su unità diverse. Un po' come la prima condizione di Bernstein ma su unità diverse.
\paragraph{Esempio} Decodifico una LOAD/STORE su IU usando un registro che sta per essere modificato nella EU.

\begin{multicols}{2}
\begin{lstlisting}
INC	Ri
LOAD	Rbase, Ri, R1
\end{lstlisting}
\texttt{INC} induce una dipendenza logica sulla \texttt{LOAD}: scrivo R$_i$ e lo leggo dopo, \texttt{INC} opera su EU e \texttt{LOAD} su IU.
\columnbreak

\begin{tabular}{c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & 6 \\
	\hline
	\textbf{IM} & \texttt{INC} & \texttt{LOAD} & & & & \\
	\hline
	\textbf{IU} & & \texttt{INC}* & \texttt{LOAD}** & \texttt{LOAD} & & \\
	\hline
	\textbf{DM} & & & & & \texttt{LOAD} & \\
	\hline
	\textbf{EU} & & & \texttt{INC} & & & \texttt{LOAD} \\
	\hline
\end{tabular}\\

$*$: il contatore del registro R$_i$ va a 1\\

$**$: \texttt{LOAD} tenta di leggere R$_i$ con contatore = 1, quindi il sistema si blocca.
\end{multicols}
\paragraph{Esempio} Decodifico un salto condizionato o incondizionato su un registro

\begin{multicols}{2}
\begin{lstlisting}
INC	Ri
IF<	Ri, Rn, loop
\end{lstlisting}
\columnbreak

\begin{tabular}{c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & 6 \\
	\hline
	\textbf{IM} & \texttt{INV} & \texttt{IF<} & & & & \\
	\hline
	\textbf{IU} & & \texttt{INC}* &\texttt{IF<}** & \texttt{IF<} & & \\
	\hline
	\textbf{DM} & & & & & & \\
	\hline
	\textbf{EU} & & & \texttt{INC} & & & \\
	\hline
\end{tabular}\\

$*$: il contatore del registro R$_i$ va a 1\\

$**$: \texttt{IF<} tenta di leggere R$_i$ con contatore = 1 per la valutazione della guardia, quindi il sistema si blocca.
\end{multicols}
Per tali dipendenze teniamo conto anche della ditanza k, cioè quanto distano due istruzioni con dipendenza logica -- negli esempi precedenti la distanza era pari a 1, quindi k = 1.\\
Solo le dipendenze logiche con k $\leq$ 2 possono avere un peso, e il peso può aumentare se c'è una \texttt{LOAD} nella sequenza di istruzioni che porta alla dipendenza.
\subsubsection{Effetti dei salti}
Partiamo da un semplice esempio per capire che effetto fatto i salti condizionati.
\begin{multicols}{2}
\begin{lstlisting}
	ADD	R1, R2, R3
	GOTO	eti
	SUB	R4, R5, R6
	INC ...
	...
eti:	LOAD	Rp, Ri, R7
	MUL ...
\end{lstlisting}
\columnbreak

\begin{tabular}{c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & 6 \\
	\hline
	\textbf{IM} & \texttt{ADD} & \texttt{GOTO} & \texttt{SUB} & \texttt{LOAD} & \texttt{MUL} & \\
	\hline
	\textbf{IU} & & \texttt{ADD} & \texttt{GOTO}$\nearrow$ & X\texttt{SUB}X & \texttt{LOAD} & \\
	\hline
	\textbf{DM} & & & & & & \\
	\hline
	\textbf{EU} & & & \texttt{ADD} & & & \\
	\hline
\end{tabular}
\end{multicols}
Cerchiamo di capire il meccanismo firmware che permette questo tipo di comportamento:\\
IM manda a IU \texttt{<istr, IC'>}\\
IU va nella copia dei registri e accede al contenuto del registro dove è indicato il nuovo valore di IC e invia alla IM un messaggio dicendo che il nuovo IC deve essere il contenuto di quel registro. Dopodiché aggiorna il proprio IC.\\
IU scarterà tutte le istruzioni che non hanno il valore di IC' uguale al proprio IC.
\pagebreak
\section{Ottimizzazione del codice D-RISC}
Nel caso del processore monolitico abbiamo visto in sostanza due tipi di ottimizzazioni, entrambe nella LOAD: il prefetch e la non-deallocazione. Questo perché il degrado delle prestazioni avveniva esclusivamente a causa dell'accesso alla memoria.\\
Nel processore pipeline possiamo operare in due modi: sui \textbf{salti} o sulle \textbf{dipendenze logiche} -- \textbf{principale causa} del degrado delle prestazioni.
\subsection{Inlining}
Questa tecnica cerca di \textbf{eliminare i salti relativi alle chiamate di procedura} o funzione. Ad esempio
\begin{multicols}{2}
\begin{C}
int inc(int x) {return x+1;}
int main() {... y[i] = inc(z); ...}
\end{C}
\columnbreak
\begin{lstlisting}
inc:	INC 	Rx
	GOTO	Rret
	
	ADD	Rz, R0, Rx
	CALL	Rinc, Rret
	STORE	Ry, Ri, Rx
\end{lstlisting}
\end{multicols}
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
	\hline
	\textbf{IM} & \texttt{CALL} & \texttt{STORE} & \texttt{INC} & \texttt{GOTO} & xxx & \texttt{STORE} & & & & \\
	\hline
	\textbf{IU} & \texttt{ADD} & \texttt{CALL}$\nearrow$ & X\texttt{STORE}X & \texttt{INC} & \texttt{GOTO}$\nearrow$ & XxxxX & \texttt{STORE} & & & \\
	\hline
	\textbf{DM} & & & & & & & & \texttt{STORE} & & \\
	\hline
	\textbf{EU} & & \texttt{ADD} & & & \texttt{INC} & & & & & \\
	\hline
\end{tabular}\\
5 istruzioni $\Rightarrow$ 7$\tau$
\end{center}
Usando la parola chiave \texttt{inline} davanti ad una procedura o funzione in fase di programmazione, il compilatore va a \textbf{sostituire la chiamata di funzione con il codice stesso della funzione}.
\begin{multicols}{2}
\begin{C}
inline int inc(int x) {return x+1;}
int main() {... y[i] = inc(z); ...}
\end{C}
\columnbreak
\begin{lstlisting}
	INC 	Rz
	STORE	Ry, Ri, Rz
\end{lstlisting}
\end{multicols}
Si scarta così il salto di chiamata a procedura ed il salto di ritorno, evitando due bolle sicure.
\subsection{Out Of Order}
La compilazione di un programma è un \textbf{processo statico} durante il quale \textbf{possiamo fare molte ottimizzazioni ma ci è impossibile prevedere il comportamento durante l'esecuzione} -- se i salti vengono fatti o meno\ldots\\
Molti processori adottano uno schema di implementazione chiamato \textbf{Out Of Order Execution/Decode}. Vediamo un esempio:
\begin{multicols}{2}
\begin{lstlisting}
INC	Ri
LOAD	Rp, Ri, R1
SUB	Ri, R2, Ri
\end{lstlisting}
\columnbreak

\begin{tabular}{c|c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
	\hline
	\textbf{IM} & \texttt{INC} & \texttt{LOAD} & & \texttt{SUB} & & & \\
	\hline
	\textbf{IU} & & \texttt{INC} & \texttt{LOAD}* & \texttt{LOAD} & \texttt{SUB} & & \\
	\hline
	\textbf{DM} & & & & & \texttt{LOAD} & $\searrow$ & \\
	\hline
	\textbf{EU} & & & \texttt{INC}$\nearrow$ & & & \texttt{LOAD} & \texttt{SUB} \\
	\hline
\end{tabular}\\
$*$ = bolla, attesa
\end{multicols}
Se avessi una IU che lavora \textbf{out of order} potrei \textbf{"accantonare" momentaneamente} l'istruzione che mi provoca il blocco/bolla e \textbf{tentare di eseguire l'istruzione successiva}.
\begin{multicols}{2}
\begin{lstlisting}
INC	Ri
LOAD	Rp, Ri, R1
SUB	Ri, R2, Ri
\end{lstlisting}
R(\texttt{LOAD}) = \{R$_p$, \textbf{R$_i$}\}\\
W(\texttt{LOAD}) = \{R$_i$\}\\
R(\texttt{SUB}) = \{R$_i$, R$_2$\}\\
W(\texttt{SUB}) = \{\textbf{R$_i$}\}\\
\columnbreak

\begin{tabular}{c|c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & 6 & 7 \\
	\hline
	\textbf{IM} & \texttt{INC} & \texttt{LOAD} & \texttt{SUB} & & & & \\
	\hline
	\textbf{IU} & & \texttt{INC} & \texttt{LOAD} & \texttt{SUB} & \texttt{LOAD} & & \\
	\hline
	\textbf{DM} & & & & & & \texttt{LOAD} & \\
	\hline
	\textbf{EU} & & & \texttt{INC}$\nearrow$ & & \texttt{SUB} & & \texttt{LOAD} \\
	\hline
\end{tabular}
\end{multicols}
In questo esempio, per implementare out of order, dovrei \textbf{avere un registro che mantenga la copia del valore vecchio di R$_i$} poiché l'intersezione tra l'insieme dei registri scritti da \texttt{SUB} e l'insieme dei registri letti da \texttt{LOAD} \textbf{non è vuota}.\\
Se, in questo caso, la \texttt{SUB} fosse andata a scrivere in un altro registro che non fosse R$_i$, allora avrei potuto implementare tale schema out of order senza problemi e senza copie momentanee di registri.\\
Questo modo permette di evitare bolle o blocchi e quindi rendere più performante il processore.
\subsection{Artimetiche lunghe con EU Master e Slave}
Per questo tipo di istruzioni, nel processore pipeline abbiamo una soluzione molto \textbf{semplice}: \textbf{espandere le EU con delle ALU specializzate che calcolano in pipeline i risultati delle artimetico-logiche lunghe}
\begin{center}
	\includegraphics[scale=0.88]{eumasterslave.png}
\end{center}
Quando la EU$_m$ riceve una istruzione aritmetico-logica lunga, \textbf{delega tale compito alla EU$*/$} (EU slave che fa moltiplicazioni e divisioni intere), mandando all'unità una istruzione del tipo \texttt{<$*$, R[a], R[b]>}.\\
Vediamo un esempio:
\begin{lstlisting}
ADD	R1, R2, R3
MUL	R4, R5, R6
MUL	R7, R8, R9
ADD	R6, R9, R10
\end{lstlisting}
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
	\hline
	\textbf{IM} & A & M$_1$ & M$_2$ & A$_2$ & & & & & & \\
	\hline
	\textbf{IU} & & A & M$_1$ & M$_2$ & A$_2$ & & & & & \\
	\hline
	\textbf{DM} & & & & & & & & & & \\
	\hline
	\textbf{EU$_m$} & & & A & M$_1$ & M$_2$ & A$_2*$ & $*$ & $*$ & $*$ & A$_2$ \\
	\hline
	\textbf{EU$_{*/}$} & & & & & M$_1$ & \makecell{M$_1^a$ \\ M$_2$} & \makecell{M$_1$ \\ M$_2$}& \makecell{M$_1\nearrow$ \\ M$_2$} & M$_2\nearrow$ & \\
	\hline
\end{tabular}\\
$^a$: lo stadio S$_1$ ha finito di calcolare M$_1$, quindi inizia M$_2$
\end{center}
Considereremo una EU$_m$ e 3 EU slave: una per $*/$ intere, una per $+-$ in virgola mobile e una per $*/$ in virgola mobile
\subsection{Loop Unrolling}
Questa tecnica cerca di \textbf{eliminare i salti relativi dovuti ai cicli} -- cioè ai loop.
\begin{multicols}{2}
\begin{C}
for (i = 0; i < n; i++)
	x[i] = a[i] + b[i];
\end{C}
\columnbreak
\begin{lstlisting}
loop:	LOAD
	LOAD
	ADD
	STORE
	IF<	loop
	...
\end{lstlisting}
\end{multicols}
\begin{center}
\begin{tabular}{c|c|c|c|c|c|}
	 & 1 & 2 & 3 & 4 & 5 \\
	\hline
	\textbf{IM} & \texttt{IF$<$} & ... & \texttt{LOAD} & & \\
	\hline
	\textbf{IU} & & \texttt{IF$<$}$\nearrow$ & X...X & \texttt{LOAD} & \\
	\hline
\end{tabular}
\end{center}
\pagebreak
Ad ogni iterazione del ciclo ho sicuramente una bolla dovuta ad un salto. \textbf{Sapendo il numero di iterazioni compiute} -- essendo un ciclo determinato -- \textbf{possiamo cercare di operare in questo modo}: \textbf{sapendo che n è pari}
\begin{C}
for (i = 0; i < n; i += 2) {
	x[i] = a[i] + b[i];
	x[i + 1] = a[i + 1] + b[i + 1];
}
\end{C}
In questa maniera ho \textbf{dimezzato il numero di salti dovuti al ciclo} e quindi ho ottimizzato le prestazioni.\\
\textbf{Molti compilatori utilizzano questa tecnica}.
\subsection{Delayed Branch}
Un'interessante \textbf{tecnica a tempo di compilazione} è quella chiamata \textbf{delayed branch}, che può essere considerata come un \textbf{caso particolare di spostamento del codice} cioè \textbf{basata sul concetto di sfruttare i tempi morti} introdotti dalle bolle \textbf{per effettuare del lavoro utile}.\\
Per capire come funziona vediamo un esempio:
\begin{lstlisting}
IF<	Ri, Rn, loop, delayed	;se il salto viene preso esegui comunque la ADD
ADD	R1, R2, R3		;e poi salta
\end{lstlisting}
Ovviamente posso fare questa operazione \textbf{solo se lo spostamento non cambia la semantica del programma}: \textbf{se scrive qualcosa nessuna delle istruzioni successive deve leggerlo} e \textbf{se legge qualcosa nessuna delle istruzioni successive deve scriverlo}. Valido sopratutto in casi di salti con i cicli.
\begin{multicols}{2}
\begin{lstlisting}
loop:	LOAD	Ra, Ri, R1
	LOAD	Rb, Ri, R2
	ADD	R1, R2, R3
	STORE	Rc, Ri, R3
	INC	Ri
	IF<	Ri, Rn, loop
\end{lstlisting}
\begin{center}
Diventa $\Rightarrow$
\end{center}
\columnbreak
\begin{lstlisting}
	LOAD	Ra, Ri, R1
loop:	LOAD	Rb, Ri, R2
	ADD	R1, R2, R3
	STORE	Rc, Ri, R3
	INC	Ri
	IF<	Ri, Rn, loop, delayed
	LOAD	Ra, Ri, R1
\end{lstlisting}
\end{multicols}
\textbf{Questa tecnica richiede una modifica firmware, in particolare sulla IU}.
\subsection{Dipendenze Logiche con Data-Flow}
Partiamo da un semplice esempio:
\begin{multicols}{2}
\begin{lstlisting}
1.loop:	LOAD Ra, Ri, R1
2.	LOAD Rb, Ri, R2
3.	ADD R1, R2, R3
4.	STORE Rx, Ri, R3
5.	INC Ri
6.	IF< Ri, Rn, loop
\end{lstlisting}
\columnbreak
\begin{center}
\includegraphics[scale=1]{dataflow_1.png}\\
\textbf{Grafo Data-Flow}\\
Fornisce un ordinamento sulle istruzioni
\end{center}
\end{multicols}
Il \textbf{grafo data-flow mette in evidenzia il flusso di dati}, quindi \textbf{fornisce il minimo ordinamento dei dati per ottenere un risultato corretto nel programma}. Se si viola una qualunque delle frecce del grafo otterremo un risultato errato.\\
Questo grafo \textbf{risulta molto utile per cercare di allontanare le dipendenze logiche tra istruzioni mantenendo un corretto ordinamento}.
\pagebreak
\subsection{Utilizzo del registro modificato}
Nell'esempio precedente possiamo notare, \textbf{anche grazie al grafo}, che \textbf{non possiamo spostare le istruzioni per aumentare l'efficienza}. Possiamo anche notare che c'è una \textbf{dipendenza logica con k $=$ 1 tra la ADD e la STORE} che comporta una bolla nell'esecuzione del programma.\\
Possiamo pensare di invertire l'istruzione INC e STORE in modo da allontanare la dipendenza logica pensando R$_x$ come un registro modificato che punta sempre a i - 1.
\begin{lstlisting}
loop:	LOAD Ra, Ri, R1
	LOAD Rb, Ri, R2
	ADD R1, R2, R3
	STORE Rx, Ri, R3
	INC Ri
	IF< Ri, Rn, loop
\end{lstlisting}
6 istruzioni $\Rightarrow$ 10$\tau$ --- $\varepsilon$ = $\frac{6}{10}$
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|}
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
\hline
\textbf{IM} & L & L & A & S & & & I & IF$<$ & & x & L & \\
\hline
\textbf{IU} & & L & L & A & S$*$ & S$*$ & S & I & IF$<*$ & IF$<\nearrow$ & x & L \\
\hline
\textbf{DM} & & & L & L & & & & S & & & & \\
\hline
\textbf{EU} & & & & L & L & A$\nearrow$ & & & I$\nearrow$ & & & \\
\hline
\end{tabular}
\end{center}
\begin{lstlisting}
loop:	LOAD Ra, Ri, R1
	LOAD Rb, Ri, R2
	ADD R1, R2, R3
	INC Ri
	IF< Ri, Rn, loop, delayed
	STORE Rx, Ri, R3
\end{lstlisting}
6 istruzioni $\Rightarrow$ 8$\tau$ --- $\varepsilon$ = $\frac{3}{4}$\\
Miglioro l'efficienza dal 60\% al 75\%
\begin{center}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c|c|}
 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 \\
\hline
\textbf{IM} & L & L & A & I & IF$<$ & & & S & L & & & \\
\hline
\textbf{IU} & & L & L & A & I & IF$<*$ & IF$<*$ & IF$<*\nearrow$ & S & L & & \\
\hline
\textbf{DM} & & & L & L & $\searrow$ & $\searrow$ & & & & S & L & \\
\hline
\textbf{EU} & & & & L & L & A & I$\nearrow$ & & & & & L \\
\hline
\end{tabular}
\end{center}
\pagebreak
\subsection{Rimozione Invarianti}
Se in un ciclo ho una istruzione che calcola qualcosa che viene consumato nelle istruzioni successive \textbf{senza che nessuno vada a modificare quel registro} e \textbf{se tale istruzione non dipende dalla variabile di iterazione}, allora tale istruzione \textbf{la posso anticipare prima di entrare nel ciclo}.\\
Vediamo un esempio: moltiplicazione fra matrici
\begin{multicols}{2}
\begin{C}
for (i = 0; i < n; i++) {
  for (j = 0; j < n; j++) {
    c[i][j] = 0;
    for (k = 0; k < n; k++) {
      c[i][j] = a[i][k] * b[k][j];
    }
  }
}
\end{C}
\columnbreak
\begin{center}
In memoria:\\
\includegraphics[scale=1]{matinmem.png}
\end{center}
\end{multicols}
\begin{multicols}{2}
\begin{lstlisting}
1.loop:	MUL R1, Rn, Rai ;INVARIANTE
2.	ADD Rai, Rbasea, Rai ;INVARIANTE
3.	LOAD Rai, Rk, R1
4.	MUL Rk, Rn, Rbk
5.	ADD Rbk, Rbaseb, Rbk
6.	LOAD Rbk, Rj, R2
7.	MUL R1, R2, R3
8.	MUL Ri, Rn, Rc ;INVARIANTE
9.	ADD Rci, Rbasec, Rci ;INVARIANTE
10.	LOAD Rci, Rj, R4
11.	ADD R4, R3, R4
12.	STORE Rci, R5, R4
13.	INC Rk
14.	IF< Rk, Rn, loop
\end{lstlisting}
\columnbreak
\begin{center}
\includegraphics[scale=1]{dataflow_2.png}
\end{center}
\end{multicols}
Oltre a portare fuori le invarianti, possiamo anche spostare istruzioni per evitare grandi bolle causate dalle MUL e LOAD seguendo sempre lo schema data-flow.
\begin{lstlisting}
4.	MUL Rk, Rn, Rbk
3.	LOAD Rai, Rk, R1
10.	LOAD Rci, Rj, R4
5.	ADD Rbk, Rbaseb, Rbk
6.	LOAD Rbk, Rj, R2
7.	MUL R1, R2, R3
11.	ADD R4, R3, R4
12.	STORE Rci, R5, R4
13.	INC Rk
14.	IF< Rk, Rn, loop
\end{lstlisting}
Se simulo questo codice noterò che riduce di molto le bolle.
\end{document}