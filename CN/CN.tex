\documentclass[10pt]{book}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{tikz}
\usepackage{color}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\usepackage{listings}
\usepackage{mathrsfs}
\lstset{
	language=Matlab,
	commentstyle=\color{mygray}
}
\usepackage{graphicx}
\usepackage{makecell}
\graphicspath{ {./img/} }
\usepackage{color}

\begin{document}
\renewcommand*\contentsname{Indice}
\title{Calcolo Numerico}
\author{Federico Matteoni}
\date{A.A. 2019/20}
\maketitle
\tableofcontents
\pagebreak
\section*{Introduzione}
Prof.: Luca Gemignani
\paragraph{Calcolo Numerico} Metodi numerici per risolvere problemi matematici con il calcolatore. In questo corso ce ne occuperemo dal punto di vista numerico, perché metodi di risoluzione diversi performano in maniera diversa sulla macchina. Cerchiamo di capire quali sono i metodi di interesse e cosa aspettarci dalla loro implementazione.\\
Il \textbf{metodi numerici approssimano la soluzione di problemi matematici}.\\
Inoltre, il computer \textbf{impatta} sul calcolo perché lavora con approssimazioni dei numeri, che su moli elevate di dati e di elaborazioni finiscono per perturbare il risultato ottenuto.
\paragraph{Tipici problemi} $$Ax = b$$ $$Ax = \lambda x$$ $$f(x) = 0$$ $$\int_a^bf(x) dx$$
\paragraph{Matlab} Matrix Laboratory, strumento di implementazione per verificare e constatare i risultati teorici.
\paragraph{Informazioni d'esame} Compitini, che se complessivamente passati rendono l'orale facoltativo. In alternativa, appelli scritti + orale.
\pagebreak
\chapter{Aritmetica di macchina}
Modello per capire cosa aspettarci dal punto di vista degli errori dell'esecuzione.
\paragraph{Esempio} Per calcolare il limite $$\lim_{x\to\infty} \sqrt{x + 1} - \sqrt{x}$$ ottengo un caso indeterminato ($\infty - \infty$). Posso semplificare l'espressione ad esempio razionalizzando, e con pochi passaggi ottengo la seguente uguaglianza $$\sqrt{x + 1} - \sqrt{x} = \frac{1}{\sqrt{x + 1} + \sqrt{x}}$$ Quindi dal punto di vista matematico, le due espressioni sono equivalenti. Ciò \textbf{non è sempre vero per la rappresentazione ed il calcolo in macchina}: le due espressioni forniranno risultati completamente diversi. Si rende \textbf{necessario}, quindi, \textbf{capire quale metodo si comporta meglio} rispetto agli altri.
\section{Rappresentazione in macchina}
\paragraph{Rappresentare i numeri} Siamo comunemente abituati a rappresentare un numero in diverse forme.\\
Ad esempio, $0.1 = \frac{1}{10} = 10 \cdot 0.01$. In generale, per ogni numero ho diversi metodi di rappresentazione $\Rightarrow$ In macchina dobbiamo poter \textbf{rappresentare i numeri in maniera univoca}.
\paragraph{Base di numerazione} $B \in N, B > 1$, poiché in base 1 non si può contare.\\Una base $B$ ha cifre nell'insieme $\{0, 1, \ldots, B - 1\}$
\paragraph{Teorema} Dato $x \in R$, con $x \neq 0$\\$\exists!\:\:(\{d_i\}_{i \geq 1}$ con $d_1 \neq 0$ e $d_i$ non definitivamente uguali a $B - 1) \wedge (p \in Z) \:\:\vline\:\: x = segno(x)\:\cdot\:B^p\:\cdot\:\sum_{i=1}^\infty d_i \cdot B^{-i}$
\begin{list}{}{Considerazioni}
	\item Se $x \in C$ allora viene rappresentato come coppia di numeri reali, quindi il problema si riconduce sempre alla loro rappresentazione
	\item Lo $0$ viene rappresentato in modo speciale, poiché non esiste una sua rappresentazione normalizzata
	\item ${d_i}_{i \geq 1}$ è una \textbf{successione} di cifre
	\item La rappresentazione è \textbf{normalizzata} se $d_1 \neq 0$, cioè se la prima cifra è diversa da 0
	\item Non può avere tutte cifre uguali a $B-1$ da un certo punto in poi, la rappresentazione "collassa" al numero successivo
	\item $p$ è detto \textbf{esponente}
	\item $\sum_{i=1}^\infty d_i \cdot B^{-i}$ è detta \textbf{mantissa}
	\item Questa rappresentazione si chiama \textbf{in virgola mobile} o \textbf{floating point}
\end{list}
\begin{center}
\begin{tikzpicture}
  \draw [draw=black, align=center] (0, 0) rectangle ++(1, 1) node[midway] {Segno};
  \draw [draw=black, align=center] (1, 0) rectangle ++(2, 1) node[midway] {Esponente};
  \draw [draw=black, align=center] (3, 0) rectangle ++(4, 1) node[midway] {Mantissa};
\end{tikzpicture}
\end{center} %credits: loures
\pagebreak
\paragraph{Esempi} Ponendo $B = 10$
\begin{list}{}{}
	\item $x = 0.01 \Rightarrow x = +10^{-1}\cdot(0.1)$
	\item $x = 1.35 \Rightarrow x = +10^1\cdot(0.135)$
	\item $x = 0.0023 \Rightarrow x = +10^{-2}\cdot(0.23)$
\end{list}
\paragraph{Numeri di Macchina} Nei computer ho \textbf{registri di lunghezza finita}, quindi essi vengono partizionati: una parte viene riservata alla rappresentazione dell'\textbf{esponente} e il resto alla rappresentazione della \textbf{mantissa}. L'\textbf{insieme dei numeri di macchina} F è quindi così definito
$$F(B, t, m, M) = \{\pm B^p \cdot \sum_{i=1}^t d_i \cdot b^{-i}\:\:\vline\:\: d_1 \neq 0 \wedge -m \leq p \leq M\} \cup \{0\}$$
\begin{list}{}{dove}
	\item $t$ sono le \textbf{cifre della mantissa}
	\item L'esponente p è compreso tra i valori $-m$ e $M$
	\item Lo 0 è incluso ma rappresentato a parte
\end{list}
\paragraph{Esempio} Ipotizzando di usare registri da 32 bit, posso stanziare 8 bit per l'esponente $p$ (quindi 1 bit per il segno e 7 bit per il valore) e i restanti 24 bit per la mantissa (1 per il segno, 23 per il valore). Quanti numeri posso rappresentare?
\begin{list}{}{}
	\item $p$ di 7 bit $\Rightarrow 2^7 - 1 = 127 \Rightarrow -127 \leq p \leq 127$ ma lo 0 è rappresentato due volte
	\item $x = \pm2^p \sum_{i=1}^23 d_i \cdot 2^{-1}$, con $d_i \in \{0, 1\}$, $d_1 \neq 0 \Rightarrow d_1 = 1$ sempre
\end{list}
Vedremo che con una serie di accorgimenti è possibile aumentare i numeri esattamente rappresentabili.\\\\
Dato quindi $F(B, t, m, M)$, osservo che:
\begin{list}{}{}
	\item $F(B, t, m, M)$ ha cardinalità finita $N = 2B^{t-1}(B - 1)(M + m + 1) + 1$
	\item Se $x \in F(B, t, m, M) \wedge x \neq 0 \Rightarrow \omega = B^{-m-1} \leq |x| \leq B^M(1 - B^{-t}) = \Omega$\\
	Quindi non è possibile rappresentare esattamente numeri non nulli minori di $\omega$. Si può introdurre una rappresentazione \textbf{denormalizzata} quando $p = -m$: la condizione $d_1 \neq 0$ può essere abbandonata e posso così rappresentare numeri positivi e negativi compresi in modulo tra $B^{-m-t}$ e $B^{-m}(B^{-1} - B^{-t})$\\
	Analogamente se $p = M$ si introducono rappresentazioni speciali per i simboli $\pm\infty$ e NaN (\textbf{not a number}).
\end{list}
\subsection{Aritmetica di Macchina}
La rappresentazione di un numero $x \in R, x \neq 0$ in macchina significa \textbf{approssimare} $x$ con un numero\\$\overline{x} \in F(B, t, m, M)$ commettendo un \textbf{errore relativo} di rappresentazione $$\epsilon_x = \frac{\overline{x} - x}{x} = \frac{\eta_x}{x}, x \neq 0$$ quanto più piccolo possibile in valore assoluto. La quantità $\eta_x = \overline{x} - x$ è detta \textbf{errore assoluto} della rappresentazione.\\
L'errore relativo è importante per la \textbf{valutazione qualitativa} dell'errore: nelle misurazioni astronomiche un errore assoluto di 1 cm è \textit{qualitativamente diverso} da un errore assoluto di 1 cm nella misurazione di un tavolo.\\\\
Dato $x \in R, x \neq 0$, distinguiamo due casi:
\begin{enumerate}
	\item $|x| < \omega$ (\textbf{underflow}) oppure $|x| > \Omega$ (\textbf{overflow})
	\item $\omega \leq |x| \leq \Omega$
\end{enumerate}
\pagebreak
Nel secondo caso abbiamo quattro tecniche di approssimazione:
\begin{enumerate}
	\item \textbf{Arrotondamento}: $x$ approssimato con il numero rappresentabile $\overline{x}$ più vicino
	\item \textbf{Troncamento}: $x$ approssimato con il più grande numero rappresentabile $\overline{x}$ tale che $|\overline{x}| \leq |x|$
	\item \textit{Round toward $+\infty$}: $x$ approssimato al più piccolo numero rappresentabile maggiore del dato
	\item \textit{Round toward $-\infty$}: $x$ approssimato al più grande numero rappresentabile minore del dato
\end{enumerate}
Per semplicità considereremo una macchina che opera per troncamento sull'insieme $F(B, t, m, M)$.\\
Indicheremo con $trn(x) = \overline{x}$ il risultato dell'approssimazione di $x$ con troncamento e più in generale $fl(x)$ l'\textbf{approssimazione in macchina del dato $x$} nel sistema floating point considerato.
\paragraph{Teorema} Sia $x \in R$ con $\omega \leq |x| \leq \Omega$. Si ha $$|\epsilon_x| = |\frac{trn(x) - x}{x}| \leq u = B^{1-t}$$
Si osserva che:
\begin{list}{}{}
	\item $u = B^{1-t}$ è detta \textbf{precisione di macchina} ed è \textbf{indipendente dalla grandezza del numero}, ma è caratteristica dell'aritmetica floating point, dell'insieme dei numeri rappresentabili e dalla tecnica di approssimazione. Se ad esempio si opera con arrotondamento, $u$ si dimezza.
	\item Per valutare la precisione di macchina possiamo determinare il più piccolo numero di macchina maggiore di 1. Dato $x$ tale numero, abbiamo $x - 1 = |x - 1| = B^{1-t}$ essendo $1 = B^1 \cdot B^{-1}$ rappresentato con esponente $p = 1$. Il seguente script MatLab fornisce il valore richiesto:
	\begin{lstlisting}
		eps = 0.5;
		eps1 = eps + 1;
		while(eps > 1)
			eps = 0.5 * eps;
			eps1 = eps + 1;
		end
		eps = 2 * eps;
	\end{lstlisting}
	\item Dal teorema si ricava che dato $x \in R$, in assenza di overflow/underflow, vale $fl(x) = x(1 + \epsilon_x)$ con $|\epsilon_x| \leq u$.\\
	Questa relazione esprime il modo in cui viene descritto generalmente il legame tra numero reale e sua rappresentazione in macchina.
\end{list}
\subsection{Operazioni di Macchina}
Per le \textbf{operazioni aritmetiche} in un sistema floating point si pone un analogo problema di approssimazione, in quanto \textbf{il risultato di un'operazione eseguita tra due numeri di macchina in generale non sarà un numero di macchina}.
\paragraph{Operazioni} Indichiamo con $\oplus$, $\ominus$, $\otimes$, $\oslash$ le \textbf{operazioni aritmetiche di macchina} corrispondenti relativamente all'addizione, sottrazione, prodotto e divisione. Si richiede che le operazioni siano interne all'insieme dei numeri di macchina. Una ragionevole definizione, derivata dal teorema precedente e in assenza di overflow/underflow, risulta: $$\forall a, b \in F(B, t, m, M), a \oplus b = fl(a + b) = (a + b)(1 + \epsilon_1), |\epsilon_1| \leq u$$ con $\epsilon_1$ detto  \textbf{errore locale dell'operazione}. Sempre in assenza di overflow/underflow, se $a, b \in R$ si ha $$fl(a + b) = fl(a)\oplus fl(b) = (a(1 + \epsilon_a) + b(1 + \epsilon_b))(1 + \epsilon_1) \doteq (a + b) + a\epsilon_a + b\epsilon_b + (a + b)\epsilon_1$$ dove con $\doteq$ si indica che l'eguaglianza vale \textbf{considerando le sole componenti lineari negli errori} e trascurando le componenti di ordine superiore (\textbf{analisi al primo ordine dell'errore}), in virtù del fatto che gli $\epsilon$ sono quantità piccole $0 < \epsilon < 1$, quindi trascurabili negli ordini superiori al primo.\\
Si ottiene che, in assenza di overflow/underflow, se $a, b \in R, a + b \neq 0$, allora $$\frac{fl(a + b) - (a + b)}{a + b} \doteq \frac{a}{a + b}\epsilon_a + \frac{b}{a + b}\epsilon_b + \epsilon_1$$ che esprime la dipendenza dell'errore totale commesso nel calcolo della somma tra due numeri reali rispetto agli errori generati dall'approssimazione dei dati iniziali (\textbf{errore inerente}) e agli errori generati dall'algoritmo di calcolo (\textbf{errore algoritmico}) visto come sequenza di operazioni artimetiche.\\
\pagebreak
\paragraph{Esempio}
Analizziamo cosa succede in macchina quando proviamo a calcolare $f(x) = x^2 - 1 = (x - 1)(x + 1)$. La \textbf{prima situazione di errore} sia ha sulla rappresentazione di $x$ che, \textbf{in generale}, \textbf{non è un numero di macchina}.$$x \rightarrow \tilde{x} = x(1 + \epsilon_x)$$ con $|\epsilon_x| \leq u$. Inoltre, sempre in generale, \textbf{le operazioni aritmetiche non sono operazioni di macchina} $$f(x) = (\tilde{x} \otimes \tilde{x}) \ominus 1 = (\tilde{x} \ominus 1)\otimes(\tilde{x} \oplus 1)$$Poniamo $g_1(x) = (\tilde{x} \otimes \tilde{x}) \ominus 1$ e $g_2(x) = (\tilde{x} \ominus 1)\otimes(\tilde{x} \oplus 1)$. La formula per l'\textbf{errore totale} dell'operazione è $$\epsilon_{tot1} = \frac{g_1(x) - f(x)}{f(x)}$$ $$\epsilon_{tot2} = \frac{g_2(x) - f(x)}{f(x)}$$ Sviluppiamo $g_1(x)$

$$g_1(x) = ((x(1+\epsilon_x)\cdot x(1 + \epsilon_x))(1 + \epsilon_1) - 1)(1 + \epsilon_2) \doteq$$
$$\doteq (x^2(1 + 2\epsilon_x)(1 + \epsilon_1) - 1)(1 + \epsilon_2) \doteq$$
$$\doteq (x^2 ((1 + 2\epsilon_x + \epsilon_1) - 1)(1 + \epsilon_2) \doteq$$
$$\doteq x^2(1 + 2\epsilon_x + \epsilon_1 + \epsilon_2) - (1 + \epsilon_2) =$$
$$= (x^2 - 1) + 2x^2\epsilon_x + x^2\epsilon_1 + (x^2 - 1)\epsilon_2 = g_1(x)$$
$$|\epsilon_i| \leq u$$ Quindi, portando al primo membro dell'uguaglianza $\epsilon_{tot1} = \frac{g_1(x) - f(x)}{f(x)}$ tutti i fattori $(x^2 - 1) = f(x)$ si ottiene $$\epsilon_{tot1} = \frac{2x^2}{x^2 - 1}\epsilon_x + \frac{x^2}{x^2 - 1}\epsilon_1 + \epsilon_2$$ Si evince che \textbf{l'errore totale è la somma di due componenti}:
\begin{list}{}{}
	\item \textbf{Errore inerente} o inevitabile: l'\textbf{errore di rappresentazione di $x$}, che vale 0 se $x$ è numero di macchina ed è \textbf{proprietà della funzione}. Nell'esempio, $\epsilon_{in} = \frac{2x^2}{x^2 - 1}\epsilon_x$.\\
	Se l'errore inerente è piccolo si dice che \textbf{la funzione è numericamente stabile}, viceversa è \textbf{numericamente instabile}.
	\item \textbf{Errore algoritmico}, locale all'operazione e \textbf{proprietà dell'algoritmo}. Nell'esempio, $\epsilon_{alg} = \frac{x^2}{x^2 - 1}\epsilon_1 + \epsilon_2$.\\
	Se è piccolo, si dice che l'\textbf{espressione è ben condizionata} e \textbf{poco sensibile rispetto alla perturbazione}. Viceversa, è \textbf{mal condizionata}.
\end{list}
Sviluppiamo ora $g_2(x)$ (gli errori $\delta_i$ sono analoghi agli $\epsilon_i$, si usa una notazione differente per evidenziare che hanno valori diversi, esseno propri del calcolo e dei valori usati in esso)
$$g_2(x) = ((x(1 + \epsilon_x) - 1)(1 + \delta_1))((x(1 + \epsilon_x) + 1)(1 + \delta_2))(1 + \delta_3) \doteq$$
$$\doteq (x^2(1 + \epsilon_x)^2 - 1)(1 + \delta_1 + \delta_2 + \delta_3) \doteq$$
$$\doteq (x^2(1 + 2\epsilon_x) - 1)(1 + \delta_1 + \delta_2 + \delta_3) =$$
$$= x^2(1 + 2\epsilon_x + \delta_1 + \delta_2 + \delta_3) - (1 + \delta_1 + \delta_2 + \delta_3) =$$
$$= (x^2 - 1) + 2x^2\epsilon_x + (x^2 - 1)(\delta_1 + \delta_2 + \delta_3) = g_2(x)$$
$$|\epsilon_x| \leq u, |\delta_i| \leq u$$
Come prima, porto gli $f(x)$ al primo membro dell'uguaglianza $\epsilon_{tot2} = \frac{g_2(x) - f(x)}{f(x)}$ e ottengo
$$\epsilon_{tot2} = \frac{2x^2}{x^2 - 1}\epsilon_x + \delta_1 + \delta_2 + \delta_3$$
\pagebreak

\begin{list}{}{Notiamo come}
	\item $\epsilon_{in} = \frac{2x^2}{x^2 - 1}\epsilon_x$, come il calcolo precedente. Infatti, ripetiamo, l'errore inerente è una proprietà della funzione e non di come essa viene calcolata
	\item $\epsilon_{alg} = \delta_1 + \delta_2 + \delta_3$, diverso poiché abbiamo seguito un calcolo differente
\end{list}
Per poter comparare i due algoritmi e scegliere il migliore, analizziamo gli errori algoritmici. L'analisi \textbf{va eseguita in valore assoluto}, poiché non si ha alcuna informazione sul segno degli errori, seguendo quindi le regole di comparazione dei valori assoluti:
\begin{list}{}{}
	\item $|a + b| \leq |a|\:+\:|b|$
	\item $|a\cdot b| = |a|\cdot|b|$
\end{list}
$$\epsilon_{alg1} = |\frac{x^2}{x^2 - 1}\epsilon_1 + \epsilon_2| \leq |\frac{x^2}{x^2 - 1}\epsilon_1| + |\epsilon_2| = |\frac{x^2}{x^2 - 1}|\cdot|\epsilon_1| + |\epsilon_2| \leq \frac{x^2}{|x^2 - 1|}u + u = (\frac{x^2}{|x^2 - 1|} + 1)u$$
$$\epsilon_{alg2} = |\delta_1 + \delta_2 + \delta_3| \leq |\delta_1| + |\delta_2| + |\delta_3| \leq 3u$$
In $\epsilon_{alg1}$, quindi, l'errore algoritmo è minore o uguale a $(\frac{x^2}{|x^2 - 1|} + 1)u$, che però diventa arbitrariamente grande quando $x$ si avvicina ad 1. Il secondo algoritmo è dunque \textbf{preferibile}, poiché l'\textbf{errore algoritmico è limitato}.
\paragraph{Esempio} Calcoliamo ora l'espressione $f(x) = x^2 + 1$. Poniamo $g(x) = (\tilde{x} \otimes \tilde{x}) \oplus 1$ e sviluppiamo, ottenendo $$g(x) \doteq x^2(1 + 2\epsilon_x + \epsilon_1 + \epsilon_2) + (1 + \epsilon_2)$$
L'errore totale è quindi
$$\epsilon_{tot} = \frac{g(x) - f(x)}{f(x)} = \frac{2x^2}{x^2 + 1}\epsilon_x + \frac{x^2}{x^2 + 1}\epsilon_1 + \epsilon_2$$ Studiamo le componenti
$$|\epsilon_{in} = |\frac{2x^2}{x^2 + 1}\epsilon_x| = |\frac{2x^2}{x^2 + 1}|\cdot|\epsilon_x| \leq \frac{2x^2}{x^2 + 1}u \leq 2u$$ perché $\frac{x^2}{x^2 + 1} \leq 1$. La funzione quindi \textbf{non è suscettibile rispetto alle perturbazioni dei dati in ingresso} ed è \textbf{ben condizionata}.
$$\epsilon_{alg} = |\frac{x^2}{x^2 + 1}\epsilon_1 + \epsilon_2| \leq |\frac{x^2}{x^2 + 1}\epsilon_1| + |\epsilon_2| = \frac{x^2}{x^2 + 1}|\epsilon_1| + |\epsilon_2| \leq |\epsilon_1| + |\epsilon_2| \leq 2u$$ Quindi l'algoritmo è \textbf{numericamente stabile} perché la componente dell'errore algoritmo è piccola ($\leq 2u$). Questo è il caso migliore che può capitare: \textbf{funzione ben condizionata} e \textbf{algoritmo numericamente stabile}.\\\\
Succede perché non vi è la sottrazione al numeratore. La sottrazione, in macchina, lavora usando molte delle cifre "sporche" della mantissa, cioè quelle che fanno parte del rumore e dell'errore di rappresentazione. Si chiama \textbf{errore di cancellazione}.
\subsection{Errore Inerente}
\paragraph{Definizione} Si dice \textbf{errore inerente} o \textbf{inevitabile} generato nel calcolo di $f(x) \neq 0$ la quantità $$\epsilon_{in} = \frac{f(\tilde{x}) - f(x)}{f(x)}$$
Osserviamo che l'errore inerente è una \textbf{proprietà della funzione}, ovvero del problema matematico, e misura la \textbf{sensibilità rispetto alla perturbazione del dato iniziale}. Quindi, è indipendente dall'algoritmo di calcolo.\\
Se l'errore inerente è qualitativamente elevato in valore assoluto, diciamo che \textbf{il problema matematico è mal condizionato}, altrimenti si dice che è \textbf{ben condizionato}.
\subsection{Errore Algoritmico}
\paragraph{Definizione} Si dice \textbf{errore algoritmico} generato nel calcolo di $f(x) \neq 0$ la quantità $$\epsilon_{alg} = \frac{g(\tilde{x}) - f(\tilde{x})}{f(\tilde{x})}$$
Osserviamo che l'errore algoritmico è \textbf{dipendente dall'algoritmo utilizzato}: la funzione $g$ dipende dall'algoritmo usato per calcolare $f(x)$. In generale, \textbf{differenti algoritmi conducono a differenti errori algoritmici}.\\
Se l'errore algoritmico è qualitativamente elevato in valore assoluto si dice che \textbf{l'algoritmo è numericamente instabile}, altrimenti si dice che è \textbf{numericamente stabile}.
\pagebreak
\subsection{Errore Totale}
\paragraph{Definizione} Si dice \textbf{errore totale} generato nel calcolo di $f(x) \neq 0$ mediante l'algoritmo specificato da $g$ la quantità $$\epsilon_{tot} = \frac{g(\tilde{x}) - f(x)}{f(x)}$$ L'errore totale misura la \textbf{differenza relativa tra l'output atteso e l'output effettivamente calcolato}.
\paragraph{Teorema} In un analisi al primo ordine, vale $$\epsilon_{tot} = \epsilon_{in} + \epsilon_{alg}$$
\subparagraph{Dim} $$\epsilon_{tot} = \frac{g(\tilde{x}) - f(x)}{f(x)} = \frac{f(\tilde{x}) - f(\tilde{x})}{f(\tilde{x})}\cdot\frac{f(\tilde{x})}{f(x)} + \frac{f(\tilde{x}) - f(x)}{f(x)} = \epsilon_{alg}(1 + \epsilon_{in}) + \epsilon_{in} \doteq \epsilon_{alg} + \epsilon_{in}$$
Viene espresso il fatto che nel calcolo di una funzione razione, in un'analisi al primo ordine, le due fonti di generazione d'errore forniscono contributi separati che possono essere analizzati indipendentemente.\\
L'obiettivo dell'analisi numerica è \textbf{trovare algoritmi numericamente stabili per problemi ben condizionati}.
\section{Tecniche per l'Analisi degli Errori}
\subsection{Coefficiente di Amplificazione}
Dalla seguente relazione $$\epsilon_{in} = \frac{f(\tilde{x}) - f(x)}{f(x)} = \frac{f(\tilde{x}) - f(x)}{\tilde{x} - x}\cdot\frac{x}{f(x)}\cdot\frac{\tilde{x} - x}{x}$$ si ricava che la \textbf{differenziabilità di $f(x)$ è essenziale} per il controllo dell'errore inerente. Se assumiamo che $f(x)$ è derivabile due volte e continua in $(a, b)$, allora vale lo sviluppo di Taylor $$f(\tilde{x}) = f(x) + f'(x)(\tilde{x} - x) + \frac{f''(\xi)}{2}(\tilde{x} - x)^2,\:\:\:\:\:\:|\xi - x| \leq |\tilde{x} - x| $$ da cui si ottiene $$ \epsilon_{in} = \frac{f(\tilde{x}) - f(x)}{f(x)} \doteq \frac{f'(x)}{f(x)}x\epsilon_x = c_x\epsilon_x,\:\:\:\:\:\:c_x = \frac{f'(x)}{f(x)}x $$
La quantità $c_x = c_x(f) = \frac{f'(x)}{f(x)}x$ è detta \textbf{coefficiente di amplificazione} e fornisce una \textbf{misura del condizionamento del problema}.\\
In generale, se $f$ : $\Omega \rightarrow R$ è definita su un insieme aperto di $R^n$, differenziabile due volte su $\Omega$ ed il segmento di estremi $\tilde{\textbf{x}}$ e $\textbf{x}$ è contenuto in $\Omega$ allora vale
$$ \epsilon_{in} = \frac{f(\tilde{\textbf{x}}) - f(\textbf{x})}{f(\textbf{x})} \doteq \frac{1}{f(\textbf{x})}\sum_{i=1}^n \frac{\partial f}{\partial x_i}(\textbf{x})x_i\epsilon_{x_i} = \sum_{i=1}^n c_{x_i}(f)\epsilon_{x_i}$$
con $$c_{x_i}(f) = \frac{1}{f(\textbf{x})}\frac{\partial f}{\partial x_i}(\textbf{x})x_i,\:\:\:\:\:\:1 \leq i \leq n$$
detti \textbf{coefficienti di amplificazione della funzione $f$ rispetto alla variabile $x_i$}.
\subparagraph{Esempio} per $f(x) = (x^2 + 1)/x$ si ha $$c_x = \frac{2 - (x^2 + 1)}{x^2}\cdot \frac{x}{x^2 + 1}\cdot x = \frac{x^2 - 1}{x^2 + 1}$$ e poiché $|c_x| \leq 1$ il problema risulta ben condizionato.
\subsubsection{Coefficienti}
Per le operazioni aritmetiche si ottiene
$$f(x, y) = x + y\:\:\:\:\:\:\epsilon_{in} \doteq c_x\epsilon_x + c_y\epsilon_y\:\:\:\:\:\:c_x=\frac{x}{x + y}\:\:\:\:c_y = \frac{y}{x + y}$$

$$f(x, y) = x - y\:\:\:\:\:\:\epsilon_{in} \doteq c_x\epsilon_x + c_y\epsilon_y\:\:\:\:\:\:c_x=\frac{x}{x - y}\:\:\:\:c_y = -\frac{y}{x - y}$$

$$f(x, y) = x \cdot y\:\:\:\:\:\:\epsilon_{in} \doteq c_x\epsilon_x + c_y\epsilon_y\:\:\:\:\:\:c_x= 1\:\:\:\:c_y = 1$$

$$f(x, y) = \frac{x}{y}\:\:\:\:\:\:\epsilon_{in} \doteq c_x\epsilon_x + c_y\epsilon_y\:\:\:\:\:\:c_x= 1\:\:\:\:c_y = -1$$
Ne segue, come visto in precedenza, che la sottrazione di due numeri vicini tra loro è potenziale causa di elevata amplificazione degli errori relativi (\textbf{cancellazione numerica}). Si nota anche come le operazioni moltiplicative siano ben condizionate.
\paragraph{Esempio} Siano $x = 0.2178\cdot10^2$ e $y = 0.218\cdot10^2$, supponendo di operare con troncamento in base B = 10 e t = 3 cifre di rappresentazione ($u = 10^{1 - t} = 10^{-2}$). Si ha $\tilde{x} = 0.217\cdot10^2$ e $\tilde{y} = y$.\\
Pertanto $\tilde{x}\:\oplus\:\tilde{y} = -0.001\cdot10^2 = -0.1$ mentre $x - y = -0.0002\cdot10^2 = -0.2\cdot10^{-1}$ e quindi $|\epsilon_{in}| = \frac{0.8}{0.2} = 0.4$.
\chapter{Problemi dell'Algebra Lineare Numerica}
\section{Norme Matriciali e Vettoriali}
I principali problemi dell'algebra lineare concernono la \textbf{risoluzione di sistemi lineari} ed il \textbf{calcolo di autovalori/autovettori di matrici}. Per studiarne il condizionamento è fondamentale avere degli strumenti per valutare la distanza tra vettori e tra matrici.\\
La risoluzione di sistemi lineari può essere eseguita in aritmetica reale, ma gli autovalori/autovettori possono essere complessi. Servono quindi \textbf{strumenti che operino su spazi vettoriali} $F^n$ e $F^{n \times n}$, con $n \geq 1$ e $F \in \{R, C\}$
\paragraph{Condizionamento su problemi reali} $f:\:R^n \rightarrow R$, $G_m = \frac{f(\tilde{x}) - f(x)}{f(x)}$\\
Problema: calcolo della soluzione di un sistema lineare
\begin{list}{}{}
	\item \textbf{Input} $A \in R^{2 \times 2}$, $b \in R^2$
	\item \textbf{Output} $x \in R^2\:\:\vline\:\:Ax = b$
\end{list}
Condizionamento: $A$, $B$ $\rightarrow$ devono essere approssimati a valori di macchina
\begin{list}{}{}
	\item $A = $
	\begin{math}
		\left[
		\begin{array}{c c}
			a & b\\
			c & d
		\end{array}
		\right]
	\end{math}
	$\rightarrow$ $\widehat{A} = $
	\begin{math}
		\left[
		\begin{array}{c c}
			\widehat{a} & \widehat{b}\\
			\widehat{c} & \widehat{d}
		\end{array}
		\right]
	\end{math}
	con $\widehat{a} = a(1 + \epsilon_a)$ \ldots
	\item $b = $
	\begin{math}
		\left[
		\begin{array}{c}
			b_1\\
			b_2
		\end{array}
		\right]
	\end{math}
	$\rightarrow$ $\widehat{b} = $
	\begin{math}
		\left[
		\begin{array}{c}
			\widehat{b_1}\\
			\widehat{b_2}
		\end{array}
		\right]
	\end{math}
	con $\widehat{b_i} = b_i(1 + \epsilon_{b_i})$ \ldots
	\item $Ax = b \rightarrow \widehat{A}\widehat{x} = \widehat{b}$
\end{list}
Quando $\widehat{x}$ è vicino a $x$? Per valutare la distanza fra vettori, ho bisogno delle \textbf{norme vettoriali}
\subsection{Norma Vettoriale}
\paragraph{Definizione} $f: R^n \rightarrow R$ si dice \textbf{Norma Vettoriale} se soddisfa le seguenti proprietà
\begin{enumerate}
	\item $f(x) \geq 0 \wedge (f(x) = 0 \Leftrightarrow x = 0)$
	\item $f(\alpha x) = |\alpha|f(x)\:\:\:\:\forall x \in R^n\:\:\forall\alpha\in R$
	\item $f(x + y) \leq f(x) + f(y)\:\:\:\:\forall x$, $y \in R^n$
\end{enumerate}
\pagebreak
\paragraph{Norma Euclidea} $$f(x) = ||x||_2 = \sqrt{\sum_{i = 1}^n x_i^2}$$ con $x = \left[x_1, x_2, \ldots, x_n\right]$.
\paragraph{Norma Infinito} $$f(x) = ||x||_\infty = max_{i = 1}^n |x_i|$$
\textbf{Dimostrazione} che è una norma vettoriale
\begin{enumerate}
	\item $f(x) = max |x_i|$ è sempre $\geq 0$\\
	$f(x) = max |x_i| = 0 \Leftrightarrow |x_i| = 0\:\:\:\forall i \Leftrightarrow x = 0$
	\item $f(\alpha x) = max |\alpha x_i| = max |\alpha||x_i| = |\alpha|\cdot max |x_i| = |\alpha|f(x)$
	\item $f(x + y) = max |x_i + y_i| \leq max(|x_i| + |y_i|) \leq max|x_1| + max|y_i| = f(x) + f(y)$
\end{enumerate}
\paragraph{Norma Uno} $$f(x) = ||x||_1 = \sum_{i = 1}^n |x_i|$$
\paragraph{In generale le norme di uno stesso vettore hanno valori differenti}
\paragraph{Esempio} $x = $
\begin{math}
	\left[
	\begin{array}{c}
		2\\3
	\end{array}
	\right]
\end{math}
\begin{list}{}{}
	\item $||x||_1 = |2| + |3| = 5$
	\item $||x||_2 = \sqrt{2^2 + 3^2} = \sqrt{4 + 9} = \sqrt{13}$
	\item $||x||_\infty = 3$
\end{list}
\paragraph{Distanza} Data $f: R^n \rightarrow R$ \textbf{norma}, allora possiamo definire la \textbf{distanza su $R^n$} come $$dist(x, y) = ||x - y|| = f(x - y)$$
\begin{enumerate}
	\item $f(x) \geq 0$ e $f(y)\geq 0 \Rightarrow dist \geq 0$ e $(dist = 0 \Leftrightarrow x  = y)$
	\item $f(\alpha x) = |\alpha|f(x)$
	\item $f(x + y) \leq f(x) + f(y)$
\end{enumerate}
\paragraph{Anche la distanza assume valori differenti su norme differenti}
\paragraph{Esempio} $x = $
\begin{math}
	\left[
	\begin{array}{c}
		1\\0
	\end{array}
	\right]
\end{math}
, $y = $
\begin{math}
	\left[
	\begin{array}{c}
		2\\3
	\end{array}
	\right]
\end{math}
\begin{list}{}{}
	\item $dist(x, y) = ||x - y||_1 = ||$\begin{math}
	\left[
	\begin{array}{c}
		-1\\-3
	\end{array}
	\right]
	||_1 = 4
\end{math}
	\item $dist(x, y) = ||x - y||_2 = ||$\begin{math}
	\left[
	\begin{array}{c}
		-1\\-3
	\end{array}
	\right]
	||_2 = \sqrt{10}
\end{math}
	\item $dist(x, y) = ||x - y||_\infty = ||$\begin{math}
	\left[
	\begin{array}{c}
		-1\\-3
	\end{array}
	\right]
	||_\infty = 3
\end{math}
\end{list}
\paragraph{Equivalenza topologica} Può succedere che $x$ e $y$ siano vicini in norma uno e lontani in norma 2? \textbf{No} per l'\textbf{equivalenza topologica delle norme}: quantitativamente differenti, ma qualitativamente danno gli stessi risultati.
\begin{list}{}{}
	\item $||x - y||_1$ è piccolo $\Rightarrow ||x - y||_2$ è piccolo
	\item $||x - y||_\infty$ è grande $\Rightarrow ||x - y||_1$ è grande
\end{list}
\pagebreak
\subsection{Norma Matriciale}
\paragraph{Definizione} $f: R^{n \times n} \rightarrow R$ si dice \textbf{norma matriciale} se soddisfa le seguenti proprietà
\begin{enumerate}
	\item $f(A) \geq 0 \wedge (f(A) = 0 \Leftrightarrow A = 0 \Leftrightarrow a_{ij} = 0\:\:\forall i$, $j$
	\item $f(\alpha A) = |\alpha| f(A)$
	\item $f(A + B) \leq f(A) + f(B)$
	\item $f(A\cdot B) \leq f(A)\cdot f(B)$
\end{enumerate}
Difficile definire le norme metriciali, quind \textbf{costruiamo le norme matriciali a partire dalle norme vettoriali}.
\paragraph{Definizione} Data $f_V: R^n \rightarrow R$ norma vettoriale, si dice \textbf{norma matriciale indotta} (\textbf{compatibile}) la norma $f_M: R^{n \times n} \rightarrow R$ definita da
$$f_M(A) = max_{\{x \in R^n\:\vline\:f_V(x) = 1\}}\:\:f_V(Ax)$$
\begin{enumerate}
	\item Prendo i vettori $x \in R^n\:\:\vline\:\:f_V(x) = 1$, in generale sono infiniti
	\item Calcolare $Ax$, in generale infiniti vettori
	\item Prendo $f_V(Ax)$, infiniti \textbf{valori}
	\item Calcolo il \textbf{massimo} di questi valori
\end{enumerate}
\begin{list}{}{}
	\item $||A||_1 = max_{||x||_1 = 1} ||Ax||_1$
	\item $||A||_2 = max_{||x||_2 = 1} ||Ax||_2$
	\item $||A||_\infty = max_{||x||_\infty = 1} ||Ax||_\infty$
\end{list}
\paragraph{Esempio} $A = $
	\begin{math}
		\left[
		\begin{array}{c c}
			1 & 2\\
			3 & 4
		\end{array}
		\right]
	\end{math}
$\rightarrow ||A||_2 = max_{||x||_2 = 1} ||Ax||_2$.\\
$||x||_2 = 1 \Leftrightarrow \sqrt{x_1^2 + x_2^2} = 1 \Leftrightarrow x_1^2 + x_2^2 = 1$ che è l'equazione per la circonferenza di centro 0 e raggio 1. Quindi tutti i vettori per cui $||x||_2 = 1$ sono tutti quelli la cui punta giace sulla circonferenza, quindi tanti quanti i punti della circonferenza \textbf{quindi infiniti}.
\subsection{Teoremi di Caratterizzazione}
La definizione quindi \textbf{non è pratica} (calcolabile) in generale, perché dovrei fare un massimo su infiniti termini. Quindi si introducono dei \textbf{teoremi di caratterizzazione}.
\paragraph{Teorema 1} $||A||_2 = \sqrt{\rho (A^TA)}$\\Dove $\rho(B) = max |\lambda_i|$ e $\lambda_i \in $ spettro$(B)$\\
$\rho(B)$ è il \textbf{raggio spettrale} di $B$, cioè il \textbf{modulo dell'autovalore di modulo massimo} (spettro$(B)$ è l'insieme degli autovalori).
\paragraph{Teorema 2} $||A||_\infty = max_{i = 1}^n \sum_{j = 1}^n |a_{ij}|$ \textbf{somme per riga} poi \textbf{prendo il massimo}
\paragraph{Teorema 2} $||A||_\infty = max_{j = 1}^n \sum_{i = 1}^n |a_{ij}|$ \textbf{somme per colonna} poi \textbf{prendo il massimo}\\\\
Da questi teoremi, se ne deriva che \textbf{calcolare la norma uno e la norma infinito è più semplice che calcolare la norma euclidea}.\\\\
\textbf{Vantaggi} delle norme matriciali $\rightarrow$ \textbf{ulteriorie proprietà} delle norme matriciali indotte
\pagebreak
\paragraph{Teorema} Sia $||$•$||$ una norma vettoriale e $||-||$ la norma matriciale indotta.\\
Allora $\forall A \in R^{n \times n}$, $\forall v \in R^n$ si ha $||Av|| \leq ||A||\cdot||v||$
\subparagraph{Dimostrazione} Due casi
\begin{list}{}{}
	\item[$v = 0$] $||Av|| = ||0|| = 0$\\
	$||A||\cdot||v|| = ||A||\cdot 0 = 0$
	\item[$v \neq 0$] $||Av|| = ||\:A\frac{v}{||v||}\cdot||v||\:||$ e chiamo il vettore $A\frac{v}{||v||} = z$. Quindi, per la seconda proprietà avrò\\
	$= ||z||\cdot||v|| = ||A\frac{v}{||v||}||\cdot||v|| \leq ||A||\cdot||v||$
\end{list}
\subsection{Condizionamento della risoluzione di un sistema lineare}
$$Ax = b \longrightarrow \widehat{A}\widehat{x} = \widehat{b}$$
Per semplicità di analisi, $\widehat{A} = A$ e considero la perturbazione solo su $b$.
$$Ax = b \longrightarrow A\widehat{x} = \widehat{b}$$
Supponiamo anche A invertibile.\\
Misurare il condizionamento significa misurare quanto $x$ e $\widehat{x}$ sono vicini, attraverso $$\epsilon_{in} = \frac{||x - \widehat{x}||}{||x||}$$
Studiamo la quantità $$||x - \widehat{x}|| = ||A^{-1}b - A^{-1}\widehat{b}|| =$$
$$\widehat{b} = b + f\:\:\:\:\:\:\:\:b = \left[\begin{array}{c} b_1\\b_2 \end{array}\right] \rightarrow \widehat{b} = \left[\begin{array}{c} \widehat{b_1}\\\widehat{b_2} \end{array}\right] = \left[\begin{array}{c} b_1(1 + \epsilon_1)\\b_2(1 + \epsilon_2) \end{array}\right] = \left[\begin{array}{c} b_1\\b_2 \end{array}\right] + \left[\begin{array}{c} b_1\epsilon_1\\b_2\epsilon_2 \end{array}\right] = b + f$$
$$= ||A^{-1}b - A^{-1}(b + f)|| = ||-A^{-1}f|| = ||A^{-1}f|| \leq ||A^{-1}||\cdot||f||$$
Quindi $||x - \widehat{x}|| \leq ||A^{-1}||\cdot||f||$\\
Siccome $||x||$ sta al denominatore, maggiorazione a denominatore usa una quantità più piccola.
$$Ax = b \Rightarrow ||Ax|| = ||b|| \Leftrightarrow ||b|| = ||Ax|| \leq ||A||\cdot||x|| $$
Quindi $||x|| \geq \frac{||b||}{||A||}$\\
Da questo ottengo $$\frac{||x - \widehat{x}||}{||x||} \leq \frac{||A^{-1}||\cdot||f||}{\frac{||b||}{||A||}} = ||A||\cdot||A^{-1}||\cdot\frac{||f||}{||b||}$$
Quindi $$\epsilon_{in} = \frac{||x - \widehat{x}||}{||x||} \leq ||A||\cdot||A^{-1}||\cdot\frac{||\widehat{b} - b||}{||b||}$$
$\frac{||\widehat{b} - b||}{||b||}$ \textbf{perturbazione relativa sul termine noto}\\
$||A||\cdot||A^{-1}||$ \textbf{coefficiente di amplificazione}\\\\
Quindi se $||A||\cdot||A^{-1}||$ è grande, allora il sistem è mal condizionato.\\
Se $||A||\cdot||A^{-1}||$ è piccolo, allora è ben condizionato. Piccolo quanto?
\paragraph{Osservazione} $||A\cdot A^{-1}|| = ||I|| \leq ||A||\cdot||A^{-1}||$, ma $||I|| = max_{||x|| = 1} ||Ix|| = max_{||x|| = 1} ||x|| = 1$\\
Quindi $||A||\cdot||A^{-1}|| \geq 1$ che quindi è un coefficiente di amplificazione effettivo, con valore minimo pari a 1.
%TODO esempio
\pagebreak
\section{Autovalori e Autovettori}
\paragraph{Autovalore}  $\lambda \in C$ è \textbf{autovalore} di $A \in C^{n \times n}$ se $\exists\, x \neq 0 \:\:\vline\:\:A\cdot x = \lambda\cdot x$\\
Attenzione a $x \neq 0$.
\paragraph{Autovettore} $x$ è detto \textbf{autovettore destro} relativo all'autovalore $\lambda$\\\\
Si osserva che 
$$Ax = \lambda x \Leftrightarrow Ax - \lambda x = 0 \Leftrightarrow (A - \lambda I)x = 0$$
Quindi possiamo alternativamente dire
\paragraph{Definizione} $\lambda \in C$ è \textbf{autovalore} di $A \in C^{n \times n} \Leftrightarrow \textsl{det}(A - \lambda I) = 0$
\subsection{Polinomio caratteristico}
$p(\lambda) = \textsl{det}(A - \lambda I)$ è \textbf{polinomio caratteristico} di A
\paragraph{Definizione} $\lambda \in C$ è \textbf{autovalore} di $A \in C^{n \times n} \Leftrightarrow p(\lambda) = 0$ con $p(z) = \textsl{det}(A - zI)$ polinomio caratteristico di A\\
Quindi, per trovare gli autovalori un metodo è trovare il polinomio caratteristico e trovarne gli zeri.\\Gli \textbf{autovalori sono gli zeri del polinomio caratteristico}.
\subparagraph{Osservazione} L'\textbf{autovettore non è univocamente determinato}, ma è \textbf{determinato a meno di una costante moltiplicativa}, quindi in generale sono infiniti autovettori per ogni autovalore.
\paragraph{} $p(z) = \textsl{det}(A - zI)$ con $A \in C^{n \times n}$ è un \textbf{polinomio di grado $n$}. Possiamo esser anche più precisi: siccome è un polinomio il cui termine di grado più alto si ottiene moltiplicando gli elementi della diagonale principale, può essere scritto come $$p(z) = (-1)^n z^n + p_{n-1} z^{n-1} + \ldots + p_1 z + p_0$$
\paragraph{Teorema Fondamentale dell'Algebra} Un polinomio di grado $n$, su C, ha $n$ zeri eventualmente contati con la loro molteplicità algebrica. Quindi $p(z)$ su $C$ ha $n$ zeri $\lambda_1 \ldots \lambda_n$ eventualmente ripetuti.\\
Quindi possiamo scriverlo come $$p(z) = (-1)^n \cdot \prod_{i = 1}^n (z - \lambda_i)$$
che si può ulteriormente riscrivere mettendo insieme tutti gli autovalori uguali
$$p(z) = (-1)^n \cdot \prod_{i = 1}^k (z - \lambda_i)^{\sigma_i}$$ con $k$ numero di zeri distinti, cioè di autovalori distinti ($\lambda_1 \neq \lambda_2 \neq \ldots \neq \lambda_k$) e\\$\sigma_i$ è detta \textbf{molteplicità algebrica dell'autovalore $\lambda_i$}.\\
\paragraph{Molteplicità Algebrica} La \textbf{molteplicità algebrica} non è altro che il numero di \textbf{volte che l'autovalore si ripete} come zero del polinomio caratteristico.
\paragraph{Molteplicità Geometrica} Se $\lambda$ è autovalore di $A$, allora $(A - \lambda I)$ è una \textbf{matrice singolare} $\Rightarrow$\\$\tau = \textsl{dim}(Ker(A - \lambda I)) \geq 1$ e $\tau_i$ è la \textbf{molteplicità geometrica} dell'autovalore $\lambda_i$
\paragraph{Teorema} La \textbf{molteplicità algebrica è maggiore o uguale della molteplicità geometrica}, cioè $\sigma_i \geq \tau_i$
\subsection{Diagonalizzazione}
\paragraph{Definizione} $A \in C^{n \times n}$ si dice \textbf{diagonalizzabile} se $\exists \:S \in C^{n \times n}$ invertibile $\:\:\vline\:\:S^{-1}\cdot A\cdot S = D$ \textbf{diagonale}\\
Gli \textbf{autovalori di $A$ sono gli autovalori di $D$}, questo perché $$\textsl{det}(D - \lambda I) = \textsl{det}(S^{-1}AS - \lambda I) = \textsl{det}(S^{-1}AS - \lambda S^{-1}S) = \textsl{det}(S^{-1}(A - \lambda I)S) = \textsl{det}(S^{-1})\cdot\textsl{det}(A - \lambda I)\cdot\textsl{det}(S) = \textsl{det}(A - \lambda I)$$ poiché $\textsl{det}(S)\cdot\textsl{det}(S^{-1}) = \textsl{det}(SS^{-1}) = \textsl{det}(I) = 1$. Quindi \textbf{$D$ e $A$ hanno lo stesso polinomio caratteristico, quindi stessi autovalori}.
\pagebreak
\paragraph{Teorema} $A \in C^{n \times n}$ \textbf{diagonalizzabile} $\Leftrightarrow \sigma_i = \tau_i$ per $i = 1\ldots k$, cioè \textbf{le molteplicità algebriche e geometriche sono coincidenti}.
\paragraph{Esempi di matrici diagonalizzabili}
\begin{list}{}{}
	\item $k = n$, cioè la matrice ha tutti autovalori distinti. Quindi $\sigma_i = \tau_i = 1$ per $i = 1 \ldots n$
	\item (\textbf{Teorema Spettrale}) $A$ diagonalizzabile se $A$ = $A^T$, cioè se $A$ è \textbf{simmetrica} $\Rightarrow$ \textbf{autovalori $\in R$}
\end{list}
\paragraph{Diagonalizzare}
$S^{-1}AS = D \Leftrightarrow AS = SD \Leftrightarrow AS\cdot e_j = SD\cdot e_j$ per $j = 1 \ldots n$\\
Sappiamo che $S\cdot e_j$ corrisponde alla $j$-esima colonna di $S$, che chiamiamo $S_j$. Inoltre $D\cdot e_j$ è semplicemente $d_j\cdot e_j$ perché $D$ è diagonale e ha solo l'elemento $d_j$ sulla $j$-esima colonna.\\
Quindi $\Leftrightarrow AS_j = Sd_j e_j = d_j S e_j = d_j S_j$\\
Questo ci dice che \textbf{le colonne di $S$ sono gli autovettori di $A$}.\\\\
\textbf{Calcolare gli autovalori di una matrice è, in generale, un problema difficile}, perché \textbf{gli autovalori non sono funzioni razionali dei coefficienti della matrice}.\\
Quindi \textbf{non esiste algoritmo che esegue un numero finito di operazioni razionali per calcolare gli autovalori}.\\
In generale si utilizzano \textbf{metodi iterativi} che \textbf{costruiscono successioni convergenti agli autovalori}. Per la convergenza di queste successioni, è \textbf{importante avere teoremi di localizzazione che mi dicono dove stanno gli autovalori nel piano complesso}.
\subsection{Teorema di Gerschgorin}
Sia $A \in C^{n \times n}$ e siano $K_i$ gli \textbf{insiemi} definiti da $$K_i = \left\{z \in C \:\:\vline\:\: |z-a_{ii}| \leq \sum_{j = 1, j \neq i}^n |a_{ij}|\right\}\:\:\:\:\textit{per}\:\:i = 1 \ldots n$$ Allora se $\lambda \in C$ è autovalore di A $$\Rightarrow \lambda \in \bigcup_{i = 1}^n K_i$$\\
Gli autovalori di $A$ quindi appartengono ad almeno uno dei $K_i$, i cosiddetti \textbf{cerchi di Gerschgorin}, dove $a_{ii}$ è l'elemento della diagonale principale, $|z - a_{ii}|$ è il \textbf{centro} e $\sum_{j = 1, j \neq i}^n |a_{ij}|$ è il \textbf{raggio}, formato dalla somma degli elementi della stessa riga escluso l'elemento della diagonale principale.
\paragraph{Dimostrazione} $\lambda$ autovalore di $A$ se $\exists\:x \neq 0 \:\:\vline\:\: Ax = \lambda x$. Riscrivendo per componenti, otteniamo
$$Ax = \lambda x \Leftrightarrow \sum_{j = 1}^n a_{ij}x_j = \lambda x_i\:\:\:\:\textsl{per}\:i = 1\ldots n$$
Porto al primo membro
$$\Leftrightarrow  \sum_{j = 1, j \neq i}^n a_{ij}x_j = (\lambda - a_{ii})x_i\:\:\:\:\textsl{per}\:i = 1\ldots n$$
$x \neq 0$, quindi esiste almeno una componente $\neq 0$. Prendiamo quella di modulo massimo $x_p$, quindi $|x_p| = ||x||_\infty$
$$Ax = \lambda x \Rightarrow \sum_{j = 1, j \neq p}^n a_{pj}x_j = (\lambda - a_{pp})x_p \Rightarrow\:\vline\sum_{j = 1, j \neq p}^n a_{pj}x_j\vline = |(\lambda - a_{pp})x_p|$$
Per le proprietà del valore assoluto
$$\Rightarrow |\lambda - a_{pp}|\cdot|x_p| \leq \sum_{j = 1, j \neq p}^n |a_{pj}|\cdot |x_j| $$
$x_p \in C$ in generale, quindi posso dividere per $|x_p| \in R$, sicuramente $> 0$ e sicuramente positivo quindi la disequazione non cambia verso
$$\Rightarrow |\lambda - a_{pp}| \leq \sum_{j = 1, j \neq p}^n |a_{pj}|\cdot \frac{|x_j|}{|x_p|} $$
Siccome $x_p$ è la componente di modulo massimo, allora tutte le $\frac{|x_j|}{|x_p|} \leq 1$ posso concludere che
$$\Rightarrow |\lambda - a_{pp}| \leq \sum_{j = 1, j \neq p}^n |a_{pj}|$$
Per la definizione di $K_p$, questa relazione $\Rightarrow \lambda \in K_p$. Dato che non conosco $p$ in generale, $$\Rightarrow \lambda \in \bigcup_{i = 1}^n K_i$$\\\\\\
Abbiamo considerato i cerchi \textbf{per riga}. Ma posso anche considerare i cerchi per colonna, perché \textbf{gli autovalori di $A$ sono gli autovalori di $A^T$} in generale.\\
Questo perché $\textsl{det}(A - \lambda I) = \textsl{det}((A - \lambda I)^T) = \textsl{det}(A^T - \lambda I)$
\paragraph{Esempio} Data \begin{math}
A = \left[ 
	\begin{array}{c c c}
		3 & 1 & 0\\
		1 & 3 & 1\\
		0 & 1 & 3
	\end{array}
	\right]
\end{math}\\
$K_1 = \left\{ z \in C \:\:\vline\:\: |z - 3| \leq 1\right\}$\\
$K_2 = \left\{ z \in C \:\:\vline\:\: |z - 3| \leq 2\right\}$\\
$K_3 = \left\{ z \in C \:\:\vline\:\: |z - 3| \leq 2\right\} = K_1$
\chapter{Esercitazioni}
\section{Esercitazione 2}
Studiare il \textbf{condizionamento} del problema, la \textbf{stabilità} degli algoritmi e il \textbf{costo computazionale} (stimare il numero di operazioni) di $$f(x) = x^n = e^{n\log(x)}$$ con $x > 0$
\paragraph{Condizionamento} Dato che non è dipendente dal modo in cui lo si calcola, calcoliamo il condizionamento per $f(x) = x^n$ con la formula $\epsilon_{in} = \frac{f'(x)}{f(x)}\cdot x\cdot\epsilon_x$ $$\epsilon_{in} = \frac{n\cdot \not x^{n-1} \cdot \not x}{\not x^n}\epsilon_x = n\epsilon_x$$ quindi $$|\epsilon_{in}| = |n\cdot \epsilon_x| \leq n\cdot u$$
Dato che $C_x = n$ possiamo dire che il problema è \textbf{ben condizionato}
\paragraph{Stabilità} Dobbiamo considerare i due algoritmi, perché algoritmi differenti producono errore algoritmico diverso. $$f(x) = x^n = (((x\cdot x)\cdot x)\cdot x) \ldots)\cdot x$$ Non possiamo calcolare $x \cdot x$ ma calcoleremo $x \otimes x = x^2(1 + \epsilon_1)$, assumendo $x \in$ F(B, t, m, M) per il calcolo dell'errore algoritmico.\\
Successivamente calcoliamo $x\cdot x\cdot x$, cioè $(x \otimes x) \otimes x = x^3 \cdot (1 + \epsilon_1) \cdot (1 + \epsilon_2)$.\\
Quindi $x\cdot \ldots \cdot x$ $n$ volte diventerà $x^n \prod_{i = 1}^{n-1}(1 + \epsilon_i)$. Quindi $$g(x) =  x^n \prod_{i = 1}^{n-1}(1 + \epsilon_i) = x^n(1 + \sum_{i = 1}^{n - 1}\epsilon_i)$$
$$\epsilon_{alg} = \frac{g(x) - f(x)}{f(x)} \doteq \frac{x^n(1 + \sum_{i = 1}^{n - 1}\epsilon_i) - x^n}{x^n} = \frac{x^n\sum_{i = 1}^{n - 1}\epsilon_i}{x^n} = \sum_{i = 1}^{n - 1}\epsilon_i$$
Quindi $$|\epsilon_{alg} = |\sum_{i = 1}^{n - 1}\epsilon_i| \leq \sum_{i = 1}^{n - 1}|\epsilon_i| \leq (n - 1)u$$ Quindi l'algoritmo è \textbf{numericamente stabile}. In questo caso, $\epsilon_{in}$ è dello stesso ordine di $\epsilon_{alg}$
\begin{center}
	$<$grafo dell'errore$>$
\end{center}
L'errore accumulato è $\delta_k = \epsilon_k + \delta_{k - 1}$, si somma all'errore locale della moltiplicazione ($\epsilon_k$). $$|\delta_k| = |\epsilon_k + \delta_{k - 1}| \leq |\epsilon_k| + |\delta_{k - 1}| \leq |\delta_{k - 1}| + u \leq |\delta_{k - 2}| + 2u \leq |\delta_1| + (k - 1)u \leq |\delta_0| + ku = ku$$ Quindi possiamo dire $|\delta_n| \leq n\cdot u$. Notiamo che gli errori non si amplificano, perché è un operazione di moltiplicazione, ma vengono semplicemente sommati. La relazione trovata lo dimostra.\\\\\\\\
Per quanto riguarda il secondo algoritmo $$f(x) = e^{n\cdot\log(x)}$$ Si osserva che \textbf{la funzione è razionale} ($f(x) = x^n$) ma \textbf{viene calcolata in macchina usando le funzioni non razionali} $h(x) = e^x$ e $g(x) = \log(x)$. In generale, quindi, non posso eseguire l'analisi dell'errore algoritmico con gli strumenti precedentemente usati. Questo \textbf{a meno che non si sappia com'è calcolata la funzione non razionale}.\\
Aggiriamo il problema dicendo che $$Exp(x) = e^x\cdot(1 + \epsilon_1)$$ con $|\epsilon_1| \leq u$ e $$Log(x) = \log(x)\cdot(1 + \epsilon_2)$$ con $|\epsilon_2| \leq u$. Queste sono approssimazioni, in generale $|\epsilon_1|,\:\:|\epsilon_2| \leq k\cdot u$ con $k$ piccola che adesso trascuriamo perché l'analisi qualitativa è analoga.\\
In questo caso, trattiamo le funzioni non razionali come razionali.
\subparagraph{Analisi del grafo}
\begin{center}
	\includegraphics[scale=0.4]{es2_grafo2.png}
\end{center}
$|\epsilon_1| \leq u$, $|\epsilon_2| \leq u$, $|\epsilon_3| \leq u$\\
Per $x \rightarrow \log(x)$ allora $C_x = \frac{\frac{1}{x}}{\log(x)} = \frac{1}{\log(x)}$ ma non amplifica errore su $x$, perché per il calcolo dell'errore algoritmi assumiamo numeri di macchina.\\
Per $x \rightarrow e^x$ allora $C_x = \frac{e^x}{e^x}x = x$. Quindi $\epsilon_2$ ha come coefficiente di amplificazione pari a $n\cdot\log(x)$.\\
Di conseguenza calcolo l'errore algoritmico partendo dalla cima dell'albero e verso le radici (destra verso sinistra) $$\epsilon_{alg} \doteq \epsilon_1 + (n\cdot\log(x))(\epsilon_2 + \epsilon_3)$$
Concludiamo che $|\epsilon_{alg}| = |\epsilon_1 + n\log(x)(\epsilon_2 + \epsilon_3)| \leq |\epsilon_1| + |n\log(x)(\epsilon_2 + \epsilon_3)| \leq u + |n\log(x)|\cdot |\epsilon_2 + \epsilon_3| \leq u + n|\log(x)|(|\epsilon_2| + |\epsilon_3|) \leq u + 2un|\log(x)| = u\cdot(1 + 2\cdot n\cdot|\log(x)|)$ cioè $$|\epsilon_{alg}| \leq u\cdot(1 + 2\cdot n\cdot|\log(x)|)$$ Concludiamo che l'algoritmo è \textbf{numericamente instabile per $x$ piccolo o $x$ è grande}, ma è stabile in un intorno di $1$.\\\\\\\\
L'analisi della stabilità ci dice che il primo algoritmo è preferibile.
\paragraph{Costo computazionale} Il \textbf{primo algoritmo} $\Rightarrow$ $(n - 1)$ operazioni moltiplicative, quindi \textbf{O(n)}.\\
Il \textbf{secondo algoritmo}, più complicato perché ci sono due operazioni non razionali oltre alla moltiplicazione.
\paragraph{Esercizio} Supposto $n = 2^k$ e $f(x) = x^n = x^{2^k}$, determinare un algoritmo che calcola $f(x)$ con $k = \log_2(n)$ operazioni moltiplicative. Eseguire anche l'analisi dell'errore algoritmico per l'algoritmo trovato.
\pagebreak
\chapter{Matlab}
11 bit esponente rappresentato in traslazione, 52 bit mantissa (53 cifre rappresentabili per il bit nascosto) $\Rightarrow$ $u = B^{1-t} = 2^{-52}$
\section{Esponenziale}
$f(x) = e^x$ approssimato con Taylor $$e^x \simeq 1 + x + \frac{x^2}{2} + \frac{x^3}{3!} + \ldots + \frac{x^n}{n!}$$ Implementiamo un algoritmo che dati $x$, $n$ in input restituisce $y = 1 + x + \frac{x^2}{2} + \frac{x^3}{3!} + \ldots + \frac{x^n}{n!}$ in output.\\
Potrebbe essere utile un programma del tipo
\begin{lstlisting}
y = 1;
for k = 1 .. n
	y = y + x^(k)/factorial(k)
end
\end{lstlisting}
Dal punto di vista del costo computazionale, però, ci sono dei problemi. Al passo $k$ faccio $k$ prodotti per $x^k$ e $k$ prodotti per $k!$, quindi \textbf{circa $2k$ prodotti}.\\
Il costo totale è del tipo $C = 2 + 2\cdot 2 + 2\cdot 3 + \ldots + 2\cdot n = 2(1 + 2 + 3 + \ldots + n)$.\\
Ricordiamo $1 + 2 + \ldots + n = \frac{n(n + 1)}{2}$, quindi $C = \not 2\cdot\frac{n(n+1)}{\not 2} =$ O($n^2$) quindi costo quadratico.\\
Una variazione più efficiente
\begin{lstlisting}
y = 1; p = 1; m = 1;
for k = 1 .. n
	p = p * x;	% accumula le potente
	m = m * k;	% accumula il fattoriale
	y = y + p/m	% accumula la somma
end
\end{lstlisting}
Al passo $k$ ho 4 operazioni aritmetiche (o 3 operazioni moltiplicative, perché alcune volte si afferma che $t_* >> t_+$ in termini di operazioni tra bit (\textbf{costo booleano}))\\
Il costo totale $C = 4n$ operazioni (o $3n$).\\
I due algoritmi possono produrre numeri molto grandi. Una versione che argina questo problema è la seguente.
\begin{lstlisting}
y = 1; z = 1;
for k = 1 .. n
	z = z * x/k;	% controllare l'esplosione dei numeri
	y = y + z;
end
\end{lstlisting}
\begin{center}
\pagebreak
\textbf{Codice MatLab} (file: \texttt{myexp.m})
\begin{lstlisting}
function [y] = myexp(x,n)
    % y = 1 + x + (x^2)/2 + ... + (x^n)/(n!)
    y = 1; z = 1;
    for k = 1:n
        z = z * x/k;
        y = y + z;
    end
end
\end{lstlisting}
\end{center}
\end{document}
