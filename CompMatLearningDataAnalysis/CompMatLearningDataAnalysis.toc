\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Introduction}{2}{section.0.1}% 
\contentsline {chapter}{\numberline {1}Numerical Analysis}{3}{chapter.1}% 
\contentsline {section}{\numberline {1.1}Quick recap of linear algebra}{3}{section.1.1}% 
\contentsline {paragraph}{}{4}{section*.2}% 
\contentsline {paragraph}{Norms}{4}{section*.3}% 
\contentsline {paragraph}{Orthogonal}{4}{section*.4}% 
\contentsline {paragraph}{Eigenvalues and eigenvectors}{4}{section*.5}% 
\contentsline {paragraph}{Symmetry}{4}{section*.6}% 
\contentsline {paragraph}{Spectral Theorem}{4}{section*.7}% 
\contentsline {paragraph}{Quadratic form}{4}{section*.8}% 
\contentsline {paragraph}{Positive Semidefinite}{4}{section*.9}% 
\contentsline {paragraph}{Recall theorem}{5}{section*.10}% 
\contentsline {paragraph}{Generalization for complex matrices}{5}{section*.11}% 
\contentsline {paragraph}{Singular Value Decomposition}{5}{section*.12}% 
\contentsline {paragraph}{Eckart-Young Theorem}{5}{section*.13}% 
\contentsline {paragraph}{Ranks}{5}{section*.14}% 
\contentsline {section}{\numberline {1.2}SVD Approximation}{5}{section.1.2}% 
\contentsline {subparagraph}{Best approximations}{6}{section*.15}% 
\contentsline {paragraph}{Linear Least Squares problems}{6}{section*.16}% 
\contentsline {paragraph}{Polynomial Fitting}{6}{section*.17}% 
\contentsline {paragraph}{Theory of least-squares problems}{6}{section*.18}% 
\contentsline {subparagraph}{Theorem}{6}{section*.19}% 
\contentsline {subparagraph}{Algorithm}{7}{section*.20}% 
\contentsline {paragraph}{Pseudoinverse}{7}{section*.21}% 
\contentsline {section}{\numberline {1.3}QR factorization}{7}{section.1.3}% 
\contentsline {subparagraph}{Numerical problems}{8}{section*.22}% 
\contentsline {paragraph}{Theorem}{8}{section*.23}% 
\contentsline {paragraph}{Effect of noise in data}{10}{section*.24}% 
\contentsline {paragraph}{Tikhonov Regularization/Ridge Regression}{10}{section*.25}% 
\contentsline {paragraph}{Sensitivity or conditioning of a problem}{10}{section*.26}% 
\contentsline {subparagraph}{Example}{11}{section*.27}% 
\contentsline {paragraph}{Linear Systems}{11}{section*.28}% 
\contentsline {paragraph}{Least Squares Problem}{11}{section*.29}% 
\contentsline {paragraph}{Theorem}{11}{section*.30}% 
\contentsline {paragraph}{Condition Number}{11}{section*.31}% 
\contentsline {section}{\numberline {1.4}Floating Point Numbers}{12}{section.1.4}% 
\contentsline {paragraph}{Quick recap}{12}{section*.32}% 
\contentsline {paragraph}{Error analysis}{12}{section*.33}% 
\contentsline {subparagraph}{Example}{12}{section*.34}% 
\contentsline {paragraph}{Backward Stability}{13}{section*.35}% 
\contentsline {subparagraph}{Theorem}{13}{section*.36}% 
\contentsline {paragraph}{Residuals and a-posteriori stability checks}{13}{section*.37}% 
\contentsline {subparagraph}{Theorem}{13}{section*.38}% 
\contentsline {section}{\numberline {1.5}Algorithms for square linear systems $Ax=b$}{14}{section.1.5}% 
\contentsline {subsection}{\numberline {1.5.1}Gaussian Elimination}{14}{subsection.1.5.1}% 
\contentsline {paragraph}{Complexity}{14}{section*.39}% 
\contentsline {paragraph}{Storage problem}{14}{section*.40}% 
\contentsline {paragraph}{Idea}{14}{section*.41}% 
\contentsline {subparagraph}{Is LU plus pivoting stable?}{14}{section*.42}% 
\contentsline {subparagraph}{Another problem}{14}{section*.43}% 
\contentsline {subsection}{\numberline {1.5.2}Symmetric Gaussian Elimination}{14}{subsection.1.5.2}% 
\contentsline {subparagraph}{Theorem}{15}{section*.44}% 
\contentsline {subparagraph}{Lemma}{15}{section*.45}% 
\contentsline {subsection}{\numberline {1.5.3}Cholesky factorization}{15}{subsection.1.5.3}% 
\contentsline {subsection}{\numberline {1.5.4}Algorithms for solving linear systems}{15}{subsection.1.5.4}% 
\contentsline {subsection}{\numberline {1.5.5}Iterative Methods}{15}{subsection.1.5.5}% 
\contentsline {paragraph}{Idea}{15}{section*.46}% 
\contentsline {paragraph}{Krylov Subspace}{16}{section*.47}% 
\contentsline {paragraph}{Arnoldi Algorithm}{16}{section*.48}% 
\contentsline {subparagraph}{Factorization}{17}{section*.49}% 
\contentsline {paragraph}{GMRES}{17}{section*.50}% 
\contentsline {subparagraph}{Complexity of Arnoldi}{17}{section*.51}% 
\contentsline {chapter}{\numberline {2}Optimization}{18}{chapter.2}% 
\contentsline {subparagraph}{Example: Linear estimation}{18}{section*.52}% 
\contentsline {subparagraph}{Example: Low-rank approximation}{18}{section*.53}% 
\contentsline {subparagraph}{Example: Support Vector Machines}{18}{section*.54}% 
\contentsline {section}{\numberline {2.1}Optimization problems}{19}{section.2.1}% 
\contentsline {paragraph}{Multi-objective optimization}{19}{section*.55}% 
\contentsline {subsection}{\numberline {2.1.1}Optimization is hard}{19}{subsection.2.1.1}% 
\contentsline {paragraph}{Optimization need to be approximate}{19}{section*.56}% 
\contentsline {paragraph}{Optimization is really hard}{19}{section*.57}% 
\contentsline {paragraph}{Optimization at least possible}{20}{section*.58}% 
\contentsline {subsection}{\numberline {2.1.2}Local Optimization}{20}{subsection.2.1.2}% 
\contentsline {paragraph}{Optimally choosing the iterates}{20}{section*.59}% 
\contentsline {paragraph}{To make it go faster, give it more information}{21}{section*.60}% 
\contentsline {paragraph}{Dichotomic Search}{22}{section*.61}% 
\contentsline {paragraph}{Extreme value theorem}{22}{section*.62}% 
\contentsline {paragraph}{Fastest local optimization}{22}{section*.63}% 
\contentsline {subsection}{\numberline {2.1.3}Measuring algorithms speed}{23}{subsection.2.1.3}% 
\contentsline {paragraph}{Improving dichotomic search}{23}{section*.64}% 
\contentsline {paragraph}{Newton's method}{23}{section*.65}% 
\contentsline {subsection}{\numberline {2.1.4}Global optimization}{24}{subsection.2.1.4}% 
\contentsline {paragraph}{Convexity}{24}{section*.66}% 
\contentsline {section}{\numberline {2.2}Unconstrained optimization}{24}{section.2.2}% 
\contentsline {subparagraph}{Unconstraint global optimization}{24}{section*.67}% 
\contentsline {paragraph}{Notation}{24}{section*.68}% 
\contentsline {paragraph}{Tomography}{25}{section*.69}% 
\contentsline {paragraph}{Simple Functions}{25}{section*.70}% 
\contentsline {paragraph}{Directional/partial derivatives}{25}{section*.71}% 
\contentsline {paragraph}{Jacobian}{26}{section*.72}% 
\contentsline {paragraph}{Hessian}{26}{section*.73}% 
\contentsline {subparagraph}{Theorem}{26}{section*.74}% 
\contentsline {subsection}{\numberline {2.2.1}Optimality conditions}{26}{subsection.2.2.1}% 
\contentsline {subsection}{\numberline {2.2.2}Gradient Methods}{27}{subsection.2.2.2}% 
\contentsline {paragraph}{Multivariate optimization algorithms}{27}{section*.75}% 
\contentsline {paragraph}{First order model $\equiv $ gradient method}{27}{section*.76}% 
\contentsline {paragraph}{Step selection}{27}{section*.77}% 
\contentsline {paragraph}{Gradient for quadratic functions}{27}{section*.78}% 
\contentsline {subparagraph}{Analysis}{27}{section*.79}% 
\contentsline {paragraph}{When linear convergence may not be enough}{28}{section*.80}% 
\contentsline {subsubsection}{Gradient methods for general functions}{28}{section*.81}% 
\contentsline {paragraph}{Notes on the stopping criterion}{28}{section*.82}% 
\contentsline {paragraph}{Efficiency}{28}{section*.83}% 
\contentsline {subsubsection}{Fixed Stepsize}{28}{section*.84}% 
\contentsline {paragraph}{L-smoothness}{29}{section*.85}% 
\contentsline {paragraph}{Stronger forms of convexity}{29}{section*.86}% 
\contentsline {paragraph}{Convergence rate with strong convexity}{29}{section*.87}% 
\contentsline {subsubsection}{Inexact Line Search}{29}{section*.88}% 
\contentsline {paragraph}{Armijo}{29}{section*.89}% 
\contentsline {paragraph}{Wolfe}{30}{section*.90}% 
\contentsline {paragraph}{Amijo-Wolfe in practice}{30}{section*.91}% 
\contentsline {subsection}{\numberline {2.2.3}More-Than-Gradient Methods}{30}{subsection.2.2.3}% 
\contentsline {paragraph}{General descent methods}{30}{section*.92}% 
\contentsline {subparagraph}{Convergence of general descent methods}{30}{section*.93}% 
\contentsline {paragraph}{Newton's Method}{31}{section*.94}% 
\contentsline {paragraph}{Globalised Netwtod}{31}{section*.95}% 
\contentsline {subparagraph}{Theorem 1}{31}{section*.96}% 
\contentsline {subparagraph}{Theorem 2}{31}{section*.97}% 
\contentsline {subparagraph}{Theorem 3}{31}{section*.98}% 
\contentsline {subparagraph}{Nonconvex case}{31}{section*.99}% 
\contentsline {paragraph}{Quasi-Newton}{31}{section*.100}% 
\contentsline {paragraph}{DFP}{32}{section*.101}% 
\contentsline {paragraph}{BFGS}{32}{section*.102}% 
\contentsline {paragraph}{Conjugate gradient method for quadratic functions}{32}{section*.103}% 
\contentsline {subparagraph}{Convergence and efficiency}{32}{section*.104}% 
\contentsline {paragraph}{Deflected Gradients method}{33}{section*.105}% 
\contentsline {subsection}{\numberline {2.2.4}Less-Than-Gradient Methods}{33}{subsection.2.2.4}% 
\contentsline {paragraph}{Stochastic Gradient}{33}{section*.106}% 
\contentsline {paragraph}{Nondifferentiable functions}{33}{section*.107}% 
\contentsline {paragraph}{Smooth methods fail on nonsmooth functions}{33}{section*.108}% 
\contentsline {subsubsection}{Convex Nondifferentiable Functions}{33}{section*.109}% 
\contentsline {paragraph}{Subgradients and subdifferentials}{33}{section*.110}% 
\contentsline {paragraph}{Subgradients in $R^n$}{34}{section*.111}% 
\contentsline {paragraph}{Convex nondifferentiable optimization is hard}{34}{section*.112}% 
\contentsline {subsubsection}{Subgradient methods}{35}{section*.113}% 
\contentsline {paragraph}{Fundamental relationship}{35}{section*.114}% 
\contentsline {paragraph}{Deflected Subgradient}{35}{section*.115}% 
\contentsline {subsubsection}{Smoothed Gradient Methods}{36}{section*.116}% 
\contentsline {subsubsection}{Bundle Methods}{36}{section*.117}% 
\contentsline {paragraph}{Basic Idea}{36}{section*.118}% 
\contentsline {paragraph}{Cutting Plane Algorithm}{36}{section*.119}% 
\contentsline {subparagraph}{Why}{36}{section*.120}% 
\contentsline {subparagraph}{Stabilizing}{36}{section*.121}% 
