\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Introduction}{3}{section.0.1}% 
\contentsline {paragraph}{What will we learn}{3}{section*.2}% 
\contentsline {paragraph}{Books}{3}{section*.3}% 
\contentsline {paragraph}{Exam}{3}{section*.4}% 
\contentsline {paragraph}{Experimental Approach}{3}{section*.5}% 
\contentsline {paragraph}{Motivations}{3}{section*.6}% 
\contentsline {paragraph}{Structured vs unstructured data}{3}{section*.7}% 
\contentsline {section}{\numberline {0.2}State of the Art}{3}{section.0.2}% 
\contentsline {paragraph}{Early History}{3}{section*.8}% 
\contentsline {paragraph}{Resurgence in the 1990s}{3}{section*.9}% 
\contentsline {paragraph}{Statistical Machine Learning}{3}{section*.10}% 
\contentsline {paragraph}{Traditional Supervised Learning Approach}{4}{section*.11}% 
\contentsline {paragraph}{Technological Breakthroughs}{4}{section*.12}% 
\contentsline {paragraph}{Deep Learning Approach}{4}{section*.13}% 
\contentsline {subparagraph}{Feature representation}{4}{section*.14}% 
\contentsline {subparagraph}{Language Model}{4}{section*.15}% 
\contentsline {subparagraph}{Dealing with Sentences}{4}{section*.16}% 
\contentsline {section}{\numberline {0.3}Language Modeling}{4}{section.0.3}% 
\contentsline {paragraph}{Probabilistic Language Model}{4}{section*.17}% 
\contentsline {paragraph}{Markov Model and N-Grams}{5}{section*.18}% 
\contentsline {paragraph}{Maximum likelihood estimate}{5}{section*.19}% 
\contentsline {paragraph}{Shannon Visualization Method}{5}{section*.20}% 
\contentsline {paragraph}{Shannon Game}{5}{section*.21}% 
\contentsline {paragraph}{Perils of Overfitting}{5}{section*.22}% 
\contentsline {subparagraph}{Smoothing}{5}{section*.23}% 
\contentsline {paragraph}{Zipf's Law}{6}{section*.24}% 
\contentsline {subsection}{\numberline {0.3.1}Evaluation and Perplexity}{6}{subsection.0.3.1}% 
\contentsline {paragraph}{Evaluation}{6}{section*.25}% 
\contentsline {paragraph}{Extrinsic Evaluation}{6}{section*.26}% 
\contentsline {paragraph}{Language Identification Task}{7}{section*.27}% 
\contentsline {paragraph}{Difficulty of Extrinsic Evaluation}{7}{section*.28}% 
\contentsline {paragraph}{Perplexity}{7}{section*.29}% 
\contentsline {section}{\numberline {0.4}Representation of Words}{7}{section.0.4}% 
\contentsline {paragraph}{Word Meaning}{7}{section*.30}% 
\contentsline {paragraph}{Linguistic Solution}{7}{section*.31}% 
\contentsline {subparagraph}{Problems with lexical resources}{7}{section*.32}% 
\contentsline {paragraph}{Vector Space Model}{7}{section*.33}% 
\contentsline {subparagraph}{One Hot Representation}{8}{section*.34}% 
\contentsline {subparagraph}{tf*idf Measure}{8}{section*.35}% 
\contentsline {subparagraph}{Classical VSM}{8}{section*.36}% 
\contentsline {paragraph}{Problems with Discrete Symbols}{8}{section*.37}% 
\contentsline {subparagraph}{Intuition}{8}{section*.38}% 
\contentsline {paragraph}{Word Vectors/Embeddings}{8}{section*.39}% 
\contentsline {paragraph}{Distributional Hypothesis}{8}{section*.40}% 
\contentsline {subparagraph}{Word Context Matrix}{8}{section*.41}% 
\contentsline {subparagraph}{Co-Occurrence Matrix}{8}{section*.42}% 
\contentsline {subsection}{\numberline {0.4.1}Word Embeddings}{8}{subsection.0.4.1}% 
\contentsline {paragraph}{Dense Representations}{8}{section*.43}% 
\contentsline {paragraph}{Collobert}{9}{section*.44}% 
\contentsline {paragraph}{Word2Vec}{9}{section*.45}% 
\contentsline {subparagraph}{Skip-Gram}{9}{section*.46}% 
\contentsline {subparagraph}{Objective Function}{10}{section*.47}% 
\contentsline {paragraph}{Softmax}{10}{section*.48}% 
\contentsline {paragraph}{Can we really capture the concept represented by a word?}{10}{section*.49}% 
\contentsline {paragraph}{Negative Sampling}{11}{section*.50}% 
\contentsline {paragraph}{CBoW}{11}{section*.51}% 
\contentsline {paragraph}{Which Embeddings}{11}{section*.52}% 
\contentsline {paragraph}{GloVe}{11}{section*.53}% 
\contentsline {paragraph}{fastText}{11}{section*.54}% 
\contentsline {paragraph}{Co-Occurrence Counts}{11}{section*.55}% 
\contentsline {paragraph}{Weighting}{11}{section*.56}% 
\contentsline {subparagraph}{Which One?}{12}{section*.57}% 
\contentsline {paragraph}{Parallel word2vec}{12}{section*.58}% 
\contentsline {paragraph}{Computing embeddings}{12}{section*.59}% 
\contentsline {paragraph}{Gensim}{12}{section*.60}% 
\contentsline {paragraph}{Fang}{12}{section*.61}% 
\contentsline {subsection}{\numberline {0.4.2}Evaluation}{12}{subsection.0.4.2}% 
\contentsline {paragraph}{Polysemy}{12}{section*.62}% 
\contentsline {paragraph}{Extrinsic Vector Evaluation}{12}{section*.63}% 
\contentsline {subsubsection}{Embeddings in Neural Networks}{12}{section*.64}% 
\contentsline {subsubsection}{Limits of Word Embeddings}{12}{section*.65}% 
\contentsline {paragraph}{Word Senses and Ambiguity}{13}{section*.66}% 
\contentsline {paragraph}{Sentiment Specific}{13}{section*.67}% 
\contentsline {paragraph}{Context Aware Word Embeddings}{13}{section*.68}% 
\contentsline {paragraph}{ELMo}{13}{section*.69}% 
\contentsline {paragraph}{BERT}{13}{section*.70}% 
\contentsline {section}{\numberline {0.5}Text Classification}{13}{section.0.5}% 
\contentsline {paragraph}{Definition}{13}{section*.71}% 
\contentsline {paragraph}{Hand-Coded Rules}{14}{section*.72}% 
\contentsline {paragraph}{Supervised Machine Learning}{14}{section*.73}% 
\contentsline {subsection}{\numberline {0.5.1}Naive Bayes}{14}{subsection.0.5.1}% 
\contentsline {paragraph}{Bag of words representation}{14}{section*.74}% 
\contentsline {paragraph}{Bayes Rule}{14}{section*.75}% 
\contentsline {paragraph}{Text classification problem}{14}{section*.76}% 
\contentsline {subsubsection}{Naive Bayes Classifiers}{14}{section*.77}% 
\contentsline {paragraph}{Naive Bayes Assumption}{15}{section*.78}% 
\contentsline {subsubsection}{Multinomial Naive Bayes Text Classification}{15}{section*.79}% 
\contentsline {paragraph}{Learning the Model}{15}{section*.80}% 
\contentsline {paragraph}{Smoothing to Avoid Overfitting}{15}{section*.81}% 
\contentsline {paragraph}{Classifying}{15}{section*.82}% 
\contentsline {paragraph}{Preventing Underflow}{15}{section*.83}% 
\contentsline {paragraph}{Generate}{15}{section*.84}% 
\contentsline {paragraph}{Naive Bayes and Language Modeling}{16}{section*.85}% 
\contentsline {paragraph}{Evaluating Categorization}{16}{section*.86}% 
\contentsline {paragraph}{Micro vs Macro Averaging}{16}{section*.87}% 
\contentsline {paragraph}{Multiclass Classification}{16}{section*.88}% 
\contentsline {paragraph}{Training Size}{16}{section*.89}% 
\contentsline {paragraph}{Violation of Naive Bayes Assumptions}{16}{section*.90}% 
\contentsline {paragraph}{Example: SpamAssassin}{16}{section*.91}% 
\contentsline {section}{\numberline {0.6}\IeC {\textbullet }}{16}{section.0.6}% 
\contentsline {paragraph}{Regular Expressions}{16}{section*.92}% 
\contentsline {paragraph}{Tokenization}{16}{section*.93}% 
\contentsline {subparagraph}{What's a Word?}{17}{section*.94}% 
\contentsline {paragraph}{Stanza Tokenizer}{17}{section*.95}% 
\contentsline {paragraph}{Clitics}{17}{section*.96}% 
\contentsline {section}{\numberline {0.7}Classification}{17}{section.0.7}% 
\contentsline {paragraph}{Naive Bayes}{17}{section*.97}% 
\contentsline {paragraph}{Decision Trees}{17}{section*.98}% 
\contentsline {paragraph}{Linear vs non-linear algorithms}{17}{section*.99}% 
\contentsline {subsection}{\numberline {0.7.1}Linear Binary Classification}{18}{subsection.0.7.1}% 
\contentsline {paragraph}{Perceptron}{18}{section*.100}% 
\contentsline {subsection}{\numberline {0.7.2}Hidden Markov Models}{18}{subsection.0.7.2}% 
\contentsline {paragraph}{Markov Chain}{18}{section*.101}% 
\contentsline {paragraph}{Hidden Markov Model}{18}{section*.102}% 
\contentsline {subparagraph}{Example: speech}{18}{section*.103}% 
\contentsline {paragraph}{Markov Assumption}{19}{section*.104}% 
\contentsline {paragraph}{Output-independence assumption}{19}{section*.105}% 
\contentsline {paragraph}{Three basic problems}{19}{section*.106}% 
\contentsline {paragraph}{Computing the likelihood}{19}{section*.107}% 
\contentsline {subparagraph}{Forward Algorithm}{19}{section*.108}% 
\contentsline {paragraph}{Decoding}{20}{section*.109}% 
\contentsline {paragraph}{Viterbi Algorithm}{20}{section*.110}% 
\contentsline {paragraph}{Training a HMM}{21}{section*.111}% 
\contentsline {subsubsection}{Part of Speech Tagging}{21}{section*.112}% 
\contentsline {paragraph}{Two kinds of probabilities}{21}{section*.113}% 
\contentsline {subsubsection}{Sequence Tagging}{21}{section*.114}% 
\contentsline {paragraph}{Discriminative Model}{21}{section*.115}% 
\contentsline {paragraph}{Generative Model}{21}{section*.116}% 
\contentsline {paragraph}{Naive Bayes}{22}{section*.117}% 
\contentsline {paragraph}{Logistic Regression}{22}{section*.118}% 
\contentsline {paragraph}{Problems}{22}{section*.119}% 
\contentsline {paragraph}{MEMM}{22}{section*.120}% 
\contentsline {subsubsection}{Named Entity Tagging}{23}{section*.121}% 
\contentsline {paragraph}{Approaches}{24}{section*.122}% 
\contentsline {section}{\numberline {0.8}Convolutional Neural Networks for NLP}{24}{section.0.8}% 
\contentsline {paragraph}{Distant Supervision}{25}{section*.123}% 
\contentsline {paragraph}{Sentiment Specific Word Embeddings}{25}{section*.124}% 
\contentsline {paragraph}{Sentiment Classification from a Single Neuron}{25}{section*.125}% 
\contentsline {subsection}{\numberline {0.8.1}Regularization}{25}{subsection.0.8.1}% 
\contentsline {section}{\numberline {0.9}Recurrent Neural Networks}{26}{section.0.9}% 
\contentsline {paragraph}{Recap}{26}{section*.126}% 
\contentsline {paragraph}{Recurrent}{26}{section*.127}% 
\contentsline {paragraph}{Hidden Units}{26}{section*.128}% 
\contentsline {paragraph}{Advantages}{26}{section*.129}% 
\contentsline {paragraph}{Disadvantages}{26}{section*.130}% 
\contentsline {paragraph}{Simple RNN Language Model}{26}{section*.131}% 
\contentsline {paragraph}{Vanilla RNN}{27}{section*.132}% 
\contentsline {paragraph}{Training}{27}{section*.133}% 
\contentsline {paragraph}{Backpropagation through time}{27}{section*.134}% 
\contentsline {paragraph}{Training RNN Language Model}{28}{section*.135}% 
\contentsline {paragraph}{Vanishing Gradients}{28}{section*.136}% 
\contentsline {paragraph}{Exploding Gradients}{28}{section*.137}% 
\contentsline {subsection}{\numberline {0.9.1}Specializations}{28}{subsection.0.9.1}% 
\contentsline {paragraph}{Notation}{28}{section*.138}% 
\contentsline {subsubsection}{LSTM}{29}{section*.139}% 
\contentsline {paragraph}{Long Short-Term Memory}{29}{section*.140}% 
\contentsline {subsubsection}{GRU}{30}{section*.141}% 
\contentsline {paragraph}{Gated Recurrent Units}{30}{section*.142}% 
\contentsline {section}{\numberline {0.10}Parsing}{31}{section.0.10}% 
\contentsline {paragraph}{Dealing with Text}{31}{section*.143}% 
\contentsline {paragraph}{Sentence Structure}{31}{section*.144}% 
\contentsline {paragraph}{Practical uses of parsing}{31}{section*.145}% 
\contentsline {subsection}{\numberline {0.10.1}Parsing Approaches}{32}{subsection.0.10.1}% 
\contentsline {subsubsection}{Constituency Grammar}{32}{section*.146}% 
\contentsline {paragraph}{Context Free Grammars}{32}{section*.147}% 
\contentsline {paragraph}{Constituency Parsing}{32}{section*.148}% 
\contentsline {subparagraph}{Statistical Parsing}{32}{section*.149}% 
\contentsline {subsubsection}{Dependency Grammar}{32}{section*.150}% 
\contentsline {paragraph}{Dependency Structure}{32}{section*.151}% 
\contentsline {paragraph}{Difference Between Constituency Tree and Dependency Trees}{33}{section*.152}% 
\contentsline {paragraph}{Annotation Constraints}{33}{section*.153}% 
\contentsline {paragraph}{Data-Driven Dependency Parsing}{33}{section*.154}% 
\contentsline {subparagraph}{Transition-Based Shift-Reduce Parsing}{33}{section*.155}% 
\contentsline {paragraph}{Parsing as Classification}{33}{section*.156}% 
\contentsline {paragraph}{Dependency Graph}{34}{section*.157}% 
\contentsline {paragraph}{Arc Standard Transitions}{34}{section*.158}% 
\contentsline {paragraph}{Parser Algorithm}{34}{section*.159}% 
\contentsline {paragraph}{Oracle}{34}{section*.160}% 
\contentsline {paragraph}{Projectivity}{34}{section*.161}% 
\contentsline {paragraph}{Arc-Standard Algorithm}{34}{section*.162}% 
\contentsline {paragraph}{Arc Eager Transitions}{35}{section*.163}% 
\contentsline {paragraph}{Non-Projective Transitions}{35}{section*.164}% 
\contentsline {paragraph}{Learning Procedure}{35}{section*.165}% 
\contentsline {paragraph}{Dependency Shift-Reduce Parsers}{35}{section*.166}% 
\contentsline {paragraph}{CoNLL-X Shared Task}{35}{section*.167}% 
\contentsline {paragraph}{Problems with Oracles}{35}{section*.168}% 
\contentsline {subsubsection}{Graph-Based Parsing}{35}{section*.169}% 
\contentsline {paragraph}{NN Graph-Based Parser}{36}{section*.170}% 
\contentsline {subparagraph}{Parser}{36}{section*.171}% 
\contentsline {subparagraph}{Dependency Relations}{36}{section*.172}% 
\contentsline {subparagraph}{Self-Attention}{36}{section*.173}% 
\contentsline {subparagraph}{Classifier for Labels}{36}{section*.174}% 
\contentsline {section}{\numberline {0.11}Universal Dependencies}{37}{section.0.11}% 
\contentsline {paragraph}{Goal}{37}{section*.175}% 
\contentsline {paragraph}{Guiding Principles}{37}{section*.176}% 
\contentsline {paragraph}{Design Principles}{37}{section*.177}% 
\contentsline {paragraph}{Morphological Annotation}{37}{section*.178}% 
\contentsline {paragraph}{Syntactic Annotation}{37}{section*.179}% 
\contentsline {paragraph}{Coordination}{38}{section*.180}% 
\contentsline {paragraph}{Enhanced Dependencies}{38}{section*.181}% 
\contentsline {paragraph}{Basic}{38}{section*.182}% 
\contentsline {paragraph}{Enhanced}{38}{section*.183}% 
\contentsline {paragraph}{Dependency Structure}{39}{section*.184}% 
\contentsline {paragraph}{Parsing}{39}{section*.185}% 
\contentsline {section}{\numberline {0.12}Machine Translation}{39}{section.0.12}% 
\contentsline {paragraph}{Issues}{39}{section*.186}% 
\contentsline {paragraph}{Alignment}{39}{section*.187}% 
\contentsline {paragraph}{MT Already Good for...}{39}{section*.188}% 
\contentsline {paragraph}{MT Not Yet Good Enough for...}{39}{section*.189}% 
\contentsline {subsection}{\numberline {0.12.1}Language Similarities and Divergences}{40}{subsection.0.12.1}% 
\contentsline {paragraph}{Typology}{40}{section*.190}% 
\contentsline {paragraph}{Morphology}{40}{section*.191}% 
\contentsline {paragraph}{Morphological Variation}{40}{section*.192}% 
\contentsline {paragraph}{Segmentation Variation}{40}{section*.193}% 
\contentsline {paragraph}{Lexical Gaps}{40}{section*.194}% 
\contentsline {paragraph}{Event-To-Argument Divergences}{40}{section*.195}% 
\contentsline {subsection}{\numberline {0.12.2}Classical Techniques}{40}{subsection.0.12.2}% 
\contentsline {paragraph}{Direct Translation}{41}{section*.196}% 
\contentsline {subparagraph}{Pros}{41}{section*.197}% 
\contentsline {subparagraph}{Cons}{41}{section*.198}% 
\contentsline {paragraph}{Transfer Model}{41}{section*.199}% 
\contentsline {subparagraph}{Lexical Transfer}{41}{section*.200}% 
\contentsline {subparagraph}{Systram}{41}{section*.201}% 
\contentsline {paragraph}{Interlingua}{42}{section*.202}% 
\contentsline {subparagraph}{Pros}{42}{section*.203}% 
\contentsline {subparagraph}{Cons}{42}{section*.204}% 
\contentsline {subsection}{\numberline {0.12.3}Statistical Machine Translation}{42}{subsection.0.12.3}% 
\contentsline {paragraph}{Example}{42}{section*.205}% 
\contentsline {paragraph}{What Makes a Good Translation}{42}{section*.206}% 
\contentsline {paragraph}{Fluency $P(T)$}{43}{section*.207}% 
\contentsline {paragraph}{Faithfulness $P(S\mskip \medmuskip |\mskip \medmuskip T)$}{43}{section*.208}% 
\contentsline {subparagraph}{Sentence Alignment}{43}{section*.209}% 
\contentsline {subparagraph}{Word Alignment}{43}{section*.210}% 
\contentsline {paragraph}{Three Problems for Statistical Machine Translation}{43}{section*.211}% 
\contentsline {subsection}{\numberline {0.12.4}Phrase Based Machine Translation}{43}{subsection.0.12.4}% 
\contentsline {paragraph}{Translation Probabilities}{44}{section*.212}% 
\contentsline {paragraph}{Distortion probability}{44}{section*.213}% 
\contentsline {paragraph}{Training $P(F\mskip \medmuskip |\mskip \medmuskip E)$}{44}{section*.214}% 
\contentsline {paragraph}{Computing Word Alignments}{45}{section*.215}% 
\contentsline {paragraph}{Training Alignment Probabilities}{45}{section*.216}% 
\contentsline {subsubsection}{Phrase-Based Translation Model}{45}{section*.218}% 
\contentsline {paragraph}{Phrase Alignment}{46}{section*.219}% 
\contentsline {paragraph}{Decoding}{46}{section*.220}% 
\contentsline {paragraph}{Evaluation}{46}{section*.221}% 
\contentsline {subparagraph}{Scores}{46}{section*.222}% 
\contentsline {subsection}{\numberline {0.12.5}Syntax Based Statistical Machine Translation}{46}{subsection.0.12.5}% 
\contentsline {paragraph}{Synchronous Grammar}{46}{section*.223}% 
\contentsline {paragraph}{Synchronous Derivations and Translation Models}{47}{section*.224}% 
\contentsline {paragraph}{Use of Dependency Parsing}{47}{section*.225}% 
\contentsline {subsection}{\numberline {0.12.6}Minimum Error Rate Training}{47}{subsection.0.12.6}% 
\contentsline {paragraph}{Conclusions}{47}{section*.226}% 
\contentsline {section}{\numberline {0.13}Neural Machine Translation}{47}{section.0.13}% 
\contentsline {paragraph}{NMT}{47}{section*.227}% 
\contentsline {paragraph}{Beam Search}{48}{section*.228}% 
\contentsline {subparagraph}{Example}{48}{section*.229}% 
\contentsline {subparagraph}{Stopping Criterion}{49}{section*.230}% 
\contentsline {subparagraph}{Finishing Up}{49}{section*.231}% 
\contentsline {subparagraph}{Benefits}{49}{section*.232}% 
\contentsline {subparagraph}{Disadvantages}{49}{section*.233}% 
\contentsline {paragraph}{Attention}{49}{section*.234}% 
\contentsline {subparagraph}{In equations}{50}{section*.235}% 
\contentsline {subparagraph}{Attention Variants}{50}{section*.236}% 
\contentsline {subsection}{\numberline {0.13.1}Self-Attention}{50}{subsection.0.13.1}% 
\contentsline {paragraph}{Attention}{51}{section*.237}% 
\contentsline {paragraph}{Issues with Recurrent Models}{51}{section*.238}% 
\contentsline {paragraph}{Word Windows}{51}{section*.239}% 
\contentsline {paragraph}{Attention}{51}{section*.240}% 
\contentsline {paragraph}{Self-Attention}{51}{section*.241}% 
\contentsline {paragraph}{Vector Notation}{52}{section*.242}% 
\contentsline {paragraph}{Self-Attention as a NLP Building Block}{52}{section*.243}% 
\contentsline {subparagraph}{No Notion of Order}{52}{section*.244}% 
\contentsline {subparagraph}{Adding Non-Linearities}{53}{section*.245}% 
\contentsline {subparagraph}{Future}{53}{section*.246}% 
\contentsline {subsection}{\numberline {0.13.2}Transformers}{53}{subsection.0.13.2}% 
\contentsline {paragraph}{Self-Attention}{53}{section*.247}% 
\contentsline {paragraph}{Multi-Headed Attention}{54}{section*.248}% 
\contentsline {paragraph}{Training Tricks}{54}{section*.249}% 
\contentsline {subsubsection}{Transformers Library}{54}{section*.250}% 
\contentsline {paragraph}{Hugging Face Transformers}{54}{section*.251}% 
\contentsline {subsubsection}{Transformers Architectures}{55}{section*.252}% 
\contentsline {paragraph}{Pretraining Transformers}{55}{section*.253}% 
\contentsline {subparagraph}{Pretraining through language modeling}{55}{section*.254}% 
\contentsline {paragraph}{Pretraining-Finetuning Paradigm}{55}{section*.255}% 
\contentsline {subparagraph}{SGD}{55}{section*.256}% 
\contentsline {paragraph}{Pretraining for Three Types of Architectures}{55}{section*.257}% 
\contentsline {subparagraph}{Pretraining Decoders}{56}{section*.258}% 
\contentsline {paragraph}{Pretraining Encoders}{56}{section*.259}% 
\contentsline {paragraph}{Problems with previous methods}{56}{section*.260}% 
\contentsline {subparagraph}{BERT}{56}{section*.261}% 
\contentsline {paragraph}{Masked LM}{57}{section*.262}% 
\contentsline {paragraph}{Wordpiece}{57}{section*.263}% 
\contentsline {paragraph}{Pretraining Encoder-Decoders}{57}{section*.264}% 
\contentsline {paragraph}{What pretraining objective to use?}{57}{section*.265}% 
\contentsline {paragraph}{Pre-training Tasks}{57}{section*.266}% 
\contentsline {paragraph}{Masked LM}{57}{section*.267}% 
\contentsline {paragraph}{Next Sentence Prediction}{57}{section*.268}% 
\contentsline {section}{\numberline {0.14}Analysis of Language Models}{58}{section.0.14}% 
\contentsline {paragraph}{Questions About Language Models}{58}{section*.269}% 
\contentsline {paragraph}{What Linguistic Knowledge is Present in LM?}{58}{section*.270}% 
\contentsline {subparagraph}{Unsupervised NER}{58}{section*.271}% 
\contentsline {subparagraph}{LM Effectivness}{58}{section*.272}% 
\contentsline {subparagraph}{LM as Linguistic Test Subjects}{58}{section*.273}% 
\contentsline {subparagraph}{Prediction Explanations}{59}{section*.274}% 
\contentsline {subsection}{\numberline {0.14.1}Probes}{59}{subsection.0.14.1}% 
\contentsline {paragraph}{Probing}{59}{section*.275}% 
\contentsline {paragraph}{Contextual Representation of Language}{60}{section*.276}% 
\contentsline {paragraph}{Structural Probe}{60}{section*.277}% 
\contentsline {subparagraph}{Syntax Distance Hypothesis}{60}{section*.278}% 
\contentsline {subparagraph}{Finding a Parse Tree Encoding Distance Metric}{60}{section*.279}% 
\contentsline {subparagraph}{Finding $B$}{60}{section*.280}% 
\contentsline {paragraph}{Conclusion}{60}{section*.281}% 
\contentsline {section}{\numberline {0.15}Prompt-Based Learning}{61}{section.0.15}% 
\contentsline {paragraph}{Finetuning}{61}{section*.282}% 
\contentsline {paragraph}{Zero-shot}{61}{section*.283}% 
\contentsline {paragraph}{One-Shot}{61}{section*.284}% 
\contentsline {paragraph}{Few-Shot}{61}{section*.285}% 
\contentsline {subsection}{\numberline {0.15.1}Prompts}{61}{subsection.0.15.1}% 
\contentsline {paragraph}{Prefix Tuning}{62}{section*.286}% 
\contentsline {paragraph}{Prompt Tuning}{62}{section*.287}% 
\contentsline {paragraph}{Prompt-Tuning Ensembles}{64}{section*.288}% 
\contentsline {paragraph}{Interpretability}{64}{section*.289}% 
\contentsline {subsection}{\numberline {0.15.2}Soft Prompt Transfer}{64}{subsection.0.15.2}% 
\contentsline {section}{\numberline {0.16}Reading Comprehension}{64}{section.0.16}% 
\contentsline {paragraph}{Taxonomy of Question Answering}{64}{section*.290}% 
\contentsline {paragraph}{Difference Between QA Tasks}{65}{section*.291}% 
\contentsline {paragraph}{Machine Comprehension}{65}{section*.292}% 
\contentsline {paragraph}{SQuAD}{65}{section*.293}% 
\contentsline {subparagraph}{Evaluation}{65}{section*.294}% 
\contentsline {paragraph}{SQuAD 2.0}{65}{section*.295}% 
\contentsline {subsection}{\numberline {0.16.1}Neural Models for Reading Comprehension}{66}{subsection.0.16.1}% 
\contentsline {paragraph}{LSTM-based vs BERT-based}{67}{section*.296}% 
\contentsline {paragraph}{Seq2Seq w/ Attention (recap)}{67}{section*.297}% 
\contentsline {subsubsection}{BiDAF}{67}{section*.298}% 
\contentsline {paragraph}{Encoding}{67}{section*.299}% 
\contentsline {paragraph}{Attention}{68}{section*.300}% 
\contentsline {paragraph}{Modeling and Output Layers}{68}{section*.301}% 
\contentsline {subsubsection}{BERT}{69}{section*.302}% 
\contentsline {paragraph}{BERT for Reading Comprehension}{69}{section*.303}% 
\contentsline {paragraph}{Comparing BiDAF and BERT}{69}{section*.304}% 
\contentsline {subsubsection}{Stanford Attentive Reader}{69}{section*.305}% 
\contentsline {subsubsection}{SpanBERT}{69}{section*.306}% 
\contentsline {subsection}{\numberline {0.16.2}State-of-the-Art}{69}{subsection.0.16.2}% 
\contentsline {paragraph}{Is Reading Comprehension Solved?}{69}{section*.307}% 
\contentsline {section}{\numberline {0.17}Open Domain Question Answering}{70}{section.0.17}% 
\contentsline {paragraph}{PiQASso}{70}{section*.308}% 
\contentsline {paragraph}{Question Analysis}{70}{section*.309}% 
\contentsline {paragraph}{Memory}{70}{section*.310}% 
\contentsline {subparagraph}{Retriever-Reader Framework}{71}{section*.311}% 
\contentsline {subparagraph}{Document Reader}{71}{section*.312}% 
\contentsline {paragraph}{Inference Question Answering}{71}{section*.313}% 
\contentsline {paragraph}{Reading Comprehension}{71}{section*.314}% 
\contentsline {paragraph}{Facebook's bAbI Dataset}{71}{section*.315}% 
\contentsline {paragraph}{Question Dependent Recurrent Entity Network for Question Answering}{72}{section*.316}% 
\contentsline {subparagraph}{Input encoder}{72}{section*.317}% 
\contentsline {subparagraph}{Dynamic Memory}{73}{section*.318}% 
\contentsline {subparagraph}{Output Module}{73}{section*.319}% 
\contentsline {subparagraph}{Training}{73}{section*.320}% 
\contentsline {subparagraph}{Architecture}{73}{section*.321}% 
\contentsline {section}{\numberline {0.18}Coreference Resolution}{74}{section.0.18}% 
\contentsline {paragraph}{Coreference Resoultion}{74}{section*.322}% 
\contentsline {paragraph}{Applications}{74}{section*.323}% 
\contentsline {paragraph}{Coreference Resolution in two steps}{74}{section*.324}% 
\contentsline {paragraph}{Mention Detection}{74}{section*.325}% 
\contentsline {paragraph}{Can We Avoid a Pipelined System?}{75}{section*.326}% 
\contentsline {paragraph}{Anaphora}{75}{section*.327}% 
\contentsline {paragraph}{Context}{75}{section*.328}% 
\contentsline {subsection}{\numberline {0.18.1}Coreference Models}{75}{subsection.0.18.1}% 
\contentsline {subsubsection}{Rule-Based Approach}{75}{section*.329}% 
\contentsline {paragraph}{Hobb's Naive Algorithm}{75}{section*.330}% 
\contentsline {paragraph}{Knowledge-based Pronominal Coreference}{76}{section*.331}% 
\contentsline {subsubsection}{Mention Pair/Mention Ranking}{76}{section*.332}% 
\contentsline {paragraph}{Mention Pair Training}{76}{section*.333}% 
\contentsline {paragraph}{Mention Pair Prediction}{76}{section*.334}% 
\contentsline {paragraph}{Coreference Models}{77}{section*.335}% 
\contentsline {subparagraph}{Mention Ranking}{77}{section*.336}% 
\contentsline {subparagraph}{Training}{77}{section*.337}% 
\contentsline {paragraph}{End-To-End Neural Coref Model}{78}{section*.338}% 
\contentsline {paragraph}{BERT-based coref: now has the best results}{79}{section*.339}% 
\contentsline {paragraph}{CorefQA}{79}{section*.340}% 
\contentsline {subparagraph}{Mention Proposal}{79}{section*.341}% 
\contentsline {subparagraph}{Mention Linking}{79}{section*.342}% 
\contentsline {subparagraph}{Mention Pruning}{80}{section*.343}% 
\contentsline {paragraph}{Coreference Evaluation}{80}{section*.344}% 
\contentsline {subparagraph}{B-CUBED}{80}{section*.345}% 
\contentsline {section}{\numberline {0.19}Integrating Knowledge in Language Models}{80}{section.0.19}% 
\contentsline {paragraph}{Recap}{80}{section*.346}% 
\contentsline {paragraph}{Eliciting Knowledge}{80}{section*.347}% 
\contentsline {subparagraph}{Querying Traditional Knowledgebases}{80}{section*.348}% 
\contentsline {paragraph}{Querying LM as Knowledgebases}{81}{section*.349}% 
\contentsline {paragraph}{Advantages of LMs over traditional KBs}{81}{section*.350}% 
\contentsline {subsection}{\numberline {0.19.1}Adding Knowledge to LMs}{81}{subsection.0.19.1}% 
\contentsline {subsubsection}{Add pretrained entity embeddings}{81}{section*.351}% 
\contentsline {subparagraph}{Entity Linking}{82}{section*.352}% 
\contentsline {paragraph}{Incorporate Entity Embeddings from a Different Embeddings Space}{82}{section*.353}% 
\contentsline {subparagraph}{ERNIE}{82}{section*.354}% 
\contentsline {subparagraph}{QAGNN/GreaseLM}{83}{section*.355}% 
\contentsline {subsubsection}{Use an external memory}{84}{section*.356}% 
\contentsline {subparagraph}{KGLM}{84}{section*.357}% 
\contentsline {subsubsection}{Modify Pretrained Data}{84}{section*.358}% 
\contentsline {subparagraph}{WKLM}{85}{section*.359}% 
\contentsline {subparagraph}{Inductive Biases Through Masking}{85}{section*.360}% 
\contentsline {section}{\numberline {0.20}Dialogue Systems}{85}{section.0.20}% 
\contentsline {paragraph}{Early Approaches}{85}{section*.361}% 
\contentsline {subparagraph}{Templates and Rules}{85}{section*.362}% 
\contentsline {paragraph}{Open Domain (harder)}{86}{section*.363}% 
\contentsline {paragraph}{Task Oriented (easier)}{86}{section*.364}% 
\contentsline {paragraph}{Retrieval Based (easier)}{86}{section*.365}% 
\contentsline {paragraph}{Generative (harder)}{86}{section*.366}% 
\contentsline {paragraph}{Short Conversation (easier)}{86}{section*.367}% 
\contentsline {paragraph}{Long Conversation (harder)}{86}{section*.368}% 
\contentsline {subsection}{\numberline {0.20.1}Task-Oriented}{86}{subsection.0.20.1}% 
\contentsline {paragraph}{Natural Language Understanding}{87}{section*.369}% 
\contentsline {paragraph}{Dialogue Manager}{87}{section*.370}% 
\contentsline {paragraph}{Dialogue State}{88}{section*.371}% 
\contentsline {paragraph}{Dialogue Act Labeling and Slot Filling}{88}{section*.372}% 
\contentsline {subparagraph}{Slot Filling with ConVEx}{88}{section*.373}% 
\contentsline {subparagraph}{DialoGPT}{88}{section*.374}% 
\contentsline {subsection}{\numberline {0.20.2}Generation-Oriented}{89}{subsection.0.20.2}% 
\contentsline {paragraph}{Transducer Model}{89}{section*.375}% 
\contentsline {paragraph}{Neural Models for Dialogue Response Generation}{89}{section*.376}% 
\contentsline {paragraph}{Seq2Seq}{89}{section*.377}% 
\contentsline {subsection}{\numberline {0.20.3}Retrieval-Based}{89}{subsection.0.20.3}% 
\contentsline {paragraph}{Templates}{89}{section*.378}% 
\contentsline {paragraph}{Retrieval-Based Chat}{89}{section*.379}% 
\contentsline {paragraph}{Neural Response Retrieval}{89}{section*.380}% 
\contentsline {subsection}{\numberline {0.20.4}Challenges}{89}{subsection.0.20.4}% 
\contentsline {paragraph}{Incorporating Context}{89}{section*.381}% 
\contentsline {paragraph}{Coherence}{89}{section*.382}% 
\contentsline {paragraph}{Diversity}{89}{section*.383}% 
\contentsline {paragraph}{Personality}{90}{section*.384}% 
\contentsline {subsection}{\numberline {0.20.5}Google DialogFlow}{90}{subsection.0.20.5}% 
