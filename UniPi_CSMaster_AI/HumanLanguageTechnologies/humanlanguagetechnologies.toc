\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Introduction}{3}{section.0.1}% 
\contentsline {paragraph}{What will we learn}{3}{section*.2}% 
\contentsline {paragraph}{Books}{3}{section*.3}% 
\contentsline {paragraph}{Exam}{3}{section*.4}% 
\contentsline {paragraph}{Experimental Approach}{3}{section*.5}% 
\contentsline {paragraph}{Motivations}{3}{section*.6}% 
\contentsline {paragraph}{Structured vs unstructured data}{3}{section*.7}% 
\contentsline {section}{\numberline {0.2}State of the Art}{3}{section.0.2}% 
\contentsline {paragraph}{Early History}{3}{section*.8}% 
\contentsline {paragraph}{Resurgence in the 1990s}{3}{section*.9}% 
\contentsline {paragraph}{Statistical Machine Learning}{4}{section*.10}% 
\contentsline {paragraph}{Traditional Supervised Learning Approach}{4}{section*.11}% 
\contentsline {paragraph}{Technological Breakthroughs}{4}{section*.12}% 
\contentsline {paragraph}{Deep Learning Approach}{4}{section*.13}% 
\contentsline {subparagraph}{Feature representation}{4}{section*.14}% 
\contentsline {subparagraph}{Language Model}{4}{section*.15}% 
\contentsline {subparagraph}{Dealing with Sentences}{4}{section*.16}% 
\contentsline {section}{\numberline {0.3}Language Modeling}{5}{section.0.3}% 
\contentsline {paragraph}{Probabilistic Language Model}{5}{section*.17}% 
\contentsline {paragraph}{Markov Model and N-Grams}{5}{section*.18}% 
\contentsline {paragraph}{Maximum likelihood estimate}{5}{section*.19}% 
\contentsline {paragraph}{Shannon Visualization Method}{5}{section*.20}% 
\contentsline {paragraph}{Shannon Game}{5}{section*.21}% 
\contentsline {paragraph}{Perils of Overfitting}{6}{section*.22}% 
\contentsline {subparagraph}{Smoothing}{6}{section*.23}% 
\contentsline {paragraph}{Zipf's Law}{6}{section*.24}% 
\contentsline {subsection}{\numberline {0.3.1}Evaluation and Perplexity}{6}{subsection.0.3.1}% 
\contentsline {paragraph}{Evaluation}{6}{section*.25}% 
\contentsline {paragraph}{Extrinsic Evaluation}{7}{section*.26}% 
\contentsline {paragraph}{Language Identification Task}{7}{section*.27}% 
\contentsline {paragraph}{Difficulty of Extrinsic Evaluation}{7}{section*.28}% 
\contentsline {paragraph}{Perplexity}{7}{section*.29}% 
\contentsline {section}{\numberline {0.4}Representation of Words}{7}{section.0.4}% 
\contentsline {paragraph}{Word Meaning}{7}{section*.30}% 
\contentsline {paragraph}{Linguistic Solution}{7}{section*.31}% 
\contentsline {subparagraph}{Problems with lexical resources}{7}{section*.32}% 
\contentsline {paragraph}{Vector Space Model}{8}{section*.33}% 
\contentsline {subparagraph}{One Hot Representation}{8}{section*.34}% 
\contentsline {subparagraph}{tf*idf Measure}{8}{section*.35}% 
\contentsline {subparagraph}{Classical VSM}{8}{section*.36}% 
\contentsline {paragraph}{Problems with Discrete Symbols}{8}{section*.37}% 
\contentsline {subparagraph}{Intuition}{8}{section*.38}% 
\contentsline {paragraph}{Word Vectors/Embeddings}{8}{section*.39}% 
\contentsline {paragraph}{Distributional Hypothesis}{8}{section*.40}% 
\contentsline {subparagraph}{Word Context Matrix}{8}{section*.41}% 
\contentsline {subparagraph}{Co-Occurrence Matrix}{8}{section*.42}% 
\contentsline {subsection}{\numberline {0.4.1}Word Embeddings}{9}{subsection.0.4.1}% 
\contentsline {paragraph}{Dense Representations}{9}{section*.43}% 
\contentsline {paragraph}{Collobert}{9}{section*.44}% 
\contentsline {paragraph}{Word2Vec}{9}{section*.45}% 
\contentsline {subparagraph}{Skip-Gram}{10}{section*.46}% 
\contentsline {subparagraph}{Objective Function}{11}{section*.47}% 
\contentsline {paragraph}{Softmax}{11}{section*.48}% 
\contentsline {paragraph}{Can we really capture the concept represented by a word?}{11}{section*.49}% 
\contentsline {paragraph}{Negative Sampling}{12}{section*.50}% 
\contentsline {paragraph}{CBoW}{12}{section*.51}% 
\contentsline {paragraph}{Which Embeddings}{12}{section*.52}% 
\contentsline {paragraph}{GloVe}{12}{section*.53}% 
\contentsline {paragraph}{fastText}{12}{section*.54}% 
\contentsline {paragraph}{Co-Occurrence Counts}{12}{section*.55}% 
\contentsline {paragraph}{Weighting}{12}{section*.56}% 
\contentsline {subparagraph}{Which One?}{13}{section*.57}% 
\contentsline {paragraph}{Parallel word2vec}{13}{section*.58}% 
\contentsline {paragraph}{Computing embeddings}{13}{section*.59}% 
\contentsline {paragraph}{Gensim}{13}{section*.60}% 
\contentsline {paragraph}{Fang}{13}{section*.61}% 
\contentsline {subsection}{\numberline {0.4.2}Evaluation}{13}{subsection.0.4.2}% 
\contentsline {paragraph}{Polysemy}{13}{section*.62}% 
\contentsline {paragraph}{Extrinsic Vector Evaluation}{13}{section*.63}% 
\contentsline {subsubsection}{Embeddings in Neural Networks}{13}{section*.64}% 
\contentsline {subsubsection}{Limits of Word Embeddings}{13}{section*.65}% 
\contentsline {paragraph}{Word Senses and Ambiguity}{14}{section*.66}% 
\contentsline {paragraph}{Sentiment Specific}{14}{section*.67}% 
\contentsline {paragraph}{Context Aware Word Embeddings}{14}{section*.68}% 
\contentsline {paragraph}{ELMo}{14}{section*.69}% 
\contentsline {paragraph}{BERT}{14}{section*.70}% 
\contentsline {section}{\numberline {0.5}Text Classification}{14}{section.0.5}% 
\contentsline {paragraph}{Definition}{14}{section*.71}% 
\contentsline {paragraph}{Hand-Coded Rules}{15}{section*.72}% 
\contentsline {paragraph}{Supervised Machine Learning}{15}{section*.73}% 
\contentsline {subsection}{\numberline {0.5.1}Naive Bayes}{15}{subsection.0.5.1}% 
\contentsline {paragraph}{Bag of words representation}{15}{section*.74}% 
\contentsline {paragraph}{Bayes Rule}{15}{section*.75}% 
\contentsline {paragraph}{Text classification problem}{15}{section*.76}% 
\contentsline {subsubsection}{Naive Bayes Classifiers}{15}{section*.77}% 
\contentsline {paragraph}{Naive Bayes Assumption}{16}{section*.78}% 
\contentsline {subsubsection}{Multinomial Naive Bayes Text Classification}{16}{section*.79}% 
\contentsline {paragraph}{Learning the Model}{16}{section*.80}% 
\contentsline {paragraph}{Smoothing to Avoid Overfitting}{16}{section*.81}% 
\contentsline {paragraph}{Classifying}{16}{section*.82}% 
\contentsline {paragraph}{Preventing Underflow}{16}{section*.83}% 
\contentsline {paragraph}{Generate}{16}{section*.84}% 
\contentsline {paragraph}{Naive Bayes and Language Modeling}{17}{section*.85}% 
\contentsline {paragraph}{Evaluating Categorization}{17}{section*.86}% 
\contentsline {paragraph}{Micro vs Macro Averaging}{17}{section*.87}% 
\contentsline {paragraph}{Multiclass Classification}{17}{section*.88}% 
\contentsline {paragraph}{Training Size}{17}{section*.89}% 
\contentsline {paragraph}{Violation of Naive Bayes Assumptions}{17}{section*.90}% 
\contentsline {paragraph}{Example: SpamAssassin}{17}{section*.91}% 
\contentsline {section}{\numberline {0.6}\IeC {\textbullet }}{17}{section.0.6}% 
\contentsline {paragraph}{Regular Expressions}{17}{section*.92}% 
\contentsline {paragraph}{Tokenization}{17}{section*.93}% 
\contentsline {subparagraph}{What's a Word?}{18}{section*.94}% 
\contentsline {paragraph}{Stanza Tokenizer}{18}{section*.95}% 
\contentsline {paragraph}{Clitics}{18}{section*.96}% 
\contentsline {section}{\numberline {0.7}Classification}{18}{section.0.7}% 
\contentsline {paragraph}{Naive Bayes}{18}{section*.97}% 
\contentsline {paragraph}{Decision Trees}{18}{section*.98}% 
\contentsline {paragraph}{Linear vs non-linear algorithms}{18}{section*.99}% 
\contentsline {subsection}{\numberline {0.7.1}Linear Binary Classification}{19}{subsection.0.7.1}% 
\contentsline {paragraph}{Perceptron}{19}{section*.100}% 
\contentsline {subsection}{\numberline {0.7.2}Hidden Markov Models}{19}{subsection.0.7.2}% 
\contentsline {paragraph}{Markov Chain}{19}{section*.101}% 
\contentsline {paragraph}{Hidden Markov Model}{19}{section*.102}% 
\contentsline {subparagraph}{Example: speech}{19}{section*.103}% 
\contentsline {paragraph}{Markov Assumption}{20}{section*.104}% 
\contentsline {paragraph}{Output-independence assumption}{20}{section*.105}% 
\contentsline {paragraph}{Three basic problems}{20}{section*.106}% 
\contentsline {paragraph}{Computing the likelihood}{20}{section*.107}% 
\contentsline {subparagraph}{Forward Algorithm}{20}{section*.108}% 
\contentsline {paragraph}{Decoding}{21}{section*.109}% 
\contentsline {paragraph}{Viterbi Algorithm}{21}{section*.110}% 
\contentsline {paragraph}{Training a HMM}{22}{section*.111}% 
\contentsline {subsubsection}{Part of Speech Tagging}{22}{section*.112}% 
\contentsline {paragraph}{Two kinds of probabilities}{22}{section*.113}% 
\contentsline {subsubsection}{Sequence Tagging}{22}{section*.114}% 
\contentsline {paragraph}{Discriminative Model}{22}{section*.115}% 
\contentsline {paragraph}{Generative Model}{22}{section*.116}% 
\contentsline {paragraph}{Naive Bayes}{23}{section*.117}% 
\contentsline {paragraph}{Logistic Regression}{23}{section*.118}% 
\contentsline {paragraph}{Problems}{23}{section*.119}% 
\contentsline {paragraph}{MEMM}{23}{section*.120}% 
\contentsline {subsubsection}{Named Entity Tagging}{24}{section*.121}% 
\contentsline {paragraph}{Approaches}{25}{section*.122}% 
\contentsline {section}{\numberline {0.8}Convolutional Neural Networks for NLP}{25}{section.0.8}% 
\contentsline {paragraph}{Distant Supervision}{26}{section*.123}% 
\contentsline {paragraph}{Sentiment Specific Word Embeddings}{26}{section*.124}% 
\contentsline {paragraph}{Sentiment Classification from a Single Neuron}{26}{section*.125}% 
\contentsline {subsection}{\numberline {0.8.1}Regularization}{26}{subsection.0.8.1}% 
\contentsline {section}{\numberline {0.9}Recurrent Neural Networks}{27}{section.0.9}% 
\contentsline {paragraph}{Recap}{27}{section*.126}% 
\contentsline {paragraph}{Recurrent}{27}{section*.127}% 
\contentsline {paragraph}{Hidden Units}{27}{section*.128}% 
\contentsline {paragraph}{Advantages}{27}{section*.129}% 
\contentsline {paragraph}{Disadvantages}{27}{section*.130}% 
\contentsline {paragraph}{Simple RNN Language Model}{27}{section*.131}% 
\contentsline {paragraph}{Vanilla RNN}{28}{section*.132}% 
\contentsline {paragraph}{Training}{28}{section*.133}% 
\contentsline {paragraph}{Backpropagation through time}{28}{section*.134}% 
\contentsline {paragraph}{Training RNN Language Model}{29}{section*.135}% 
\contentsline {paragraph}{Vanishing Gradients}{29}{section*.136}% 
\contentsline {paragraph}{Exploding Gradients}{29}{section*.137}% 
\contentsline {subsection}{\numberline {0.9.1}Specializations}{29}{subsection.0.9.1}% 
\contentsline {paragraph}{Notation}{29}{section*.138}% 
\contentsline {subsubsection}{LSTM}{30}{section*.139}% 
\contentsline {paragraph}{Long Short-Term Memory}{30}{section*.140}% 
\contentsline {subsubsection}{GRU}{31}{section*.141}% 
\contentsline {paragraph}{Gated Recurrent Units}{31}{section*.142}% 
\contentsline {section}{\numberline {0.10}Parsing}{32}{section.0.10}% 
\contentsline {paragraph}{Dealing with Text}{32}{section*.143}% 
\contentsline {paragraph}{Sentence Structure}{32}{section*.144}% 
\contentsline {paragraph}{Practical uses of parsing}{32}{section*.145}% 
\contentsline {subsection}{\numberline {0.10.1}Parsing Approaches}{33}{subsection.0.10.1}% 
\contentsline {subsubsection}{Constituency Grammar}{33}{section*.146}% 
\contentsline {paragraph}{Context Free Grammars}{33}{section*.147}% 
\contentsline {paragraph}{Constituency Parsing}{33}{section*.148}% 
\contentsline {subparagraph}{Statistical Parsing}{33}{section*.149}% 
\contentsline {subsubsection}{Dependency Grammar}{33}{section*.150}% 
\contentsline {paragraph}{Dependency Structure}{33}{section*.151}% 
\contentsline {paragraph}{Difference Between Constituency Tree and Dependency Trees}{34}{section*.152}% 
\contentsline {paragraph}{Annotation Constraints}{34}{section*.153}% 
\contentsline {paragraph}{Data-Driven Dependency Parsing}{34}{section*.154}% 
\contentsline {subparagraph}{Transition-Based Shift-Reduce Parsing}{34}{section*.155}% 
\contentsline {paragraph}{Parsing as Classification}{34}{section*.156}% 
\contentsline {paragraph}{Dependency Graph}{35}{section*.157}% 
\contentsline {paragraph}{Arc Standard Transitions}{35}{section*.158}% 
\contentsline {paragraph}{Parser Algorithm}{35}{section*.159}% 
\contentsline {paragraph}{Oracle}{35}{section*.160}% 
\contentsline {paragraph}{Projectivity}{35}{section*.161}% 
\contentsline {paragraph}{Arc-Standard Algorithm}{35}{section*.162}% 
\contentsline {paragraph}{Arc Eager Transitions}{36}{section*.163}% 
\contentsline {paragraph}{Non-Projective Transitions}{36}{section*.164}% 
\contentsline {paragraph}{Learning Procedure}{36}{section*.165}% 
\contentsline {paragraph}{Dependency Shift-Reduce Parsers}{36}{section*.166}% 
\contentsline {paragraph}{CoNLL-X Shared Task}{36}{section*.167}% 
\contentsline {paragraph}{Problems with Oracles}{36}{section*.168}% 
\contentsline {subsubsection}{Graph-Based Parsing}{36}{section*.169}% 
\contentsline {paragraph}{NN Graph-Based Parser}{37}{section*.170}% 
\contentsline {subparagraph}{Parser}{37}{section*.171}% 
\contentsline {subparagraph}{Dependency Relations}{37}{section*.172}% 
\contentsline {subparagraph}{Self-Attention}{37}{section*.173}% 
\contentsline {subparagraph}{Classifier for Labels}{37}{section*.174}% 
\contentsline {section}{\numberline {0.11}Universal Dependencies}{38}{section.0.11}% 
\contentsline {paragraph}{Goal}{38}{section*.175}% 
\contentsline {paragraph}{Guiding Principles}{38}{section*.176}% 
\contentsline {paragraph}{Design Principles}{38}{section*.177}% 
\contentsline {paragraph}{Morphological Annotation}{38}{section*.178}% 
\contentsline {paragraph}{Syntactic Annotation}{38}{section*.179}% 
\contentsline {paragraph}{Coordination}{39}{section*.180}% 
\contentsline {paragraph}{Enhanced Dependencies}{39}{section*.181}% 
\contentsline {paragraph}{Basic}{39}{section*.182}% 
\contentsline {paragraph}{Enhanced}{39}{section*.183}% 
\contentsline {paragraph}{Dependency Structure}{40}{section*.184}% 
\contentsline {paragraph}{Parsing}{40}{section*.185}% 
\contentsline {section}{\numberline {0.12}Machine Translation}{40}{section.0.12}% 
\contentsline {paragraph}{Issues}{40}{section*.186}% 
\contentsline {paragraph}{Alignment}{40}{section*.187}% 
\contentsline {paragraph}{MT Already Good for...}{40}{section*.188}% 
\contentsline {paragraph}{MT Not Yet Good Enough for...}{40}{section*.189}% 
\contentsline {subsection}{\numberline {0.12.1}Language Similarities and Divergences}{41}{subsection.0.12.1}% 
\contentsline {paragraph}{Typology}{41}{section*.190}% 
\contentsline {paragraph}{Morphology}{41}{section*.191}% 
\contentsline {paragraph}{Morphological Variation}{41}{section*.192}% 
\contentsline {paragraph}{Segmentation Variation}{41}{section*.193}% 
\contentsline {paragraph}{Lexical Gaps}{41}{section*.194}% 
\contentsline {paragraph}{Event-To-Argument Divergences}{41}{section*.195}% 
\contentsline {subsection}{\numberline {0.12.2}Classical Techniques}{41}{subsection.0.12.2}% 
\contentsline {paragraph}{Direct Translation}{42}{section*.196}% 
\contentsline {subparagraph}{Pros}{42}{section*.197}% 
\contentsline {subparagraph}{Cons}{42}{section*.198}% 
\contentsline {paragraph}{Transfer Model}{42}{section*.199}% 
\contentsline {subparagraph}{Lexical Transfer}{42}{section*.200}% 
\contentsline {subparagraph}{Systram}{42}{section*.201}% 
\contentsline {paragraph}{Interlingua}{43}{section*.202}% 
\contentsline {subparagraph}{Pros}{43}{section*.203}% 
\contentsline {subparagraph}{Cons}{43}{section*.204}% 
\contentsline {subsection}{\numberline {0.12.3}Statistical Machine Translation}{43}{subsection.0.12.3}% 
\contentsline {paragraph}{Example}{43}{section*.205}% 
\contentsline {paragraph}{What Makes a Good Translation}{43}{section*.206}% 
\contentsline {paragraph}{Fluency $P(T)$}{44}{section*.207}% 
\contentsline {paragraph}{Faithfulness $P(S\mskip \medmuskip |\mskip \medmuskip T)$}{44}{section*.208}% 
\contentsline {subparagraph}{Sentence Alignment}{44}{section*.209}% 
\contentsline {subparagraph}{Word Alignment}{44}{section*.210}% 
\contentsline {paragraph}{Three Problems for Statistical Machine Translation}{44}{section*.211}% 
\contentsline {subsection}{\numberline {0.12.4}Phrase Based Machine Translation}{44}{subsection.0.12.4}% 
\contentsline {paragraph}{Translation Probabilities}{45}{section*.212}% 
\contentsline {paragraph}{Distortion probability}{45}{section*.213}% 
\contentsline {paragraph}{Training $P(F\mskip \medmuskip |\mskip \medmuskip E)$}{45}{section*.214}% 
\contentsline {paragraph}{Computing Word Alignments}{46}{section*.215}% 
\contentsline {paragraph}{Training Alignment Probabilities}{46}{section*.216}% 
\contentsline {subsubsection}{Phrase-Based Translation Model}{46}{section*.218}% 
\contentsline {paragraph}{Phrase Alignment}{47}{section*.219}% 
\contentsline {paragraph}{Decoding}{47}{section*.220}% 
\contentsline {paragraph}{Evaluation}{47}{section*.221}% 
\contentsline {subparagraph}{Scores}{47}{section*.222}% 
\contentsline {subsection}{\numberline {0.12.5}Syntax Based Statistical Machine Translation}{47}{subsection.0.12.5}% 
\contentsline {paragraph}{Synchronous Grammar}{47}{section*.223}% 
\contentsline {paragraph}{Synchronous Derivations and Translation Models}{48}{section*.224}% 
\contentsline {paragraph}{Use of Dependency Parsing}{48}{section*.225}% 
\contentsline {subsection}{\numberline {0.12.6}Minimum Error Rate Training}{48}{subsection.0.12.6}% 
\contentsline {paragraph}{Conclusions}{48}{section*.226}% 
\contentsline {section}{\numberline {0.13}Neural Machine Translation}{48}{section.0.13}% 
\contentsline {paragraph}{NMT}{48}{section*.227}% 
\contentsline {paragraph}{Beam Search}{49}{section*.228}% 
\contentsline {subparagraph}{Example}{49}{section*.229}% 
\contentsline {subparagraph}{Stopping Criterion}{50}{section*.230}% 
\contentsline {subparagraph}{Finishing Up}{50}{section*.231}% 
\contentsline {subparagraph}{Benefits}{50}{section*.232}% 
\contentsline {subparagraph}{Disadvantages}{50}{section*.233}% 
\contentsline {paragraph}{Attention}{50}{section*.234}% 
\contentsline {subparagraph}{In equations}{51}{section*.235}% 
\contentsline {subparagraph}{Attention Variants}{51}{section*.236}% 
\contentsline {subsection}{\numberline {0.13.1}Self-Attention}{51}{subsection.0.13.1}% 
\contentsline {paragraph}{Attention}{52}{section*.237}% 
\contentsline {paragraph}{Issues with Recurrent Models}{52}{section*.238}% 
\contentsline {paragraph}{Word Windows}{52}{section*.239}% 
\contentsline {paragraph}{Attention}{52}{section*.240}% 
\contentsline {paragraph}{Self-Attention}{52}{section*.241}% 
\contentsline {paragraph}{Vector Notation}{53}{section*.242}% 
\contentsline {paragraph}{Self-Attention as a NLP Building Block}{53}{section*.243}% 
\contentsline {subparagraph}{No Notion of Order}{53}{section*.244}% 
\contentsline {subparagraph}{Adding Non-Linearities}{54}{section*.245}% 
\contentsline {subparagraph}{Future}{54}{section*.246}% 
\contentsline {subsection}{\numberline {0.13.2}Transformers}{54}{subsection.0.13.2}% 
\contentsline {paragraph}{Self-Attention}{54}{section*.247}% 
\contentsline {paragraph}{Multi-Headed Attention}{55}{section*.248}% 
\contentsline {paragraph}{Training Tricks}{55}{section*.249}% 
\contentsline {subsubsection}{Transformers Library}{55}{section*.250}% 
\contentsline {paragraph}{Hugging Face Transformers}{55}{section*.251}% 
\contentsline {subsubsection}{Transformers Architectures}{56}{section*.252}% 
\contentsline {paragraph}{Pretraining Transformers}{56}{section*.253}% 
\contentsline {subparagraph}{Pretraining through language modeling}{56}{section*.254}% 
\contentsline {paragraph}{Pretraining-Finetuning Paradigm}{56}{section*.255}% 
\contentsline {subparagraph}{SGD}{56}{section*.256}% 
\contentsline {paragraph}{Pretraining for Three Types of Architectures}{56}{section*.257}% 
\contentsline {subparagraph}{Pretraining Decoders}{57}{section*.258}% 
\contentsline {paragraph}{Pretraining Encoders}{57}{section*.259}% 
\contentsline {paragraph}{Problems with previous methods}{57}{section*.260}% 
\contentsline {subparagraph}{BERT}{57}{section*.261}% 
\contentsline {paragraph}{Masked LM}{58}{section*.262}% 
\contentsline {paragraph}{Wordpiece}{58}{section*.263}% 
\contentsline {paragraph}{Pretraining Encoder-Decoders}{58}{section*.264}% 
\contentsline {paragraph}{What pretraining objective to use?}{58}{section*.265}% 
\contentsline {paragraph}{Pre-training Tasks}{58}{section*.266}% 
\contentsline {paragraph}{Masked LM}{58}{section*.267}% 
\contentsline {paragraph}{Next Sentence Prediction}{58}{section*.268}% 
\contentsline {section}{\numberline {0.14}Analysis of Language Models}{59}{section.0.14}% 
\contentsline {paragraph}{Questions About Language Models}{59}{section*.269}% 
\contentsline {paragraph}{What Linguistic Knowledge is Present in LM?}{59}{section*.270}% 
\contentsline {subparagraph}{Unsupervised NER}{59}{section*.271}% 
\contentsline {subparagraph}{LM Effectivness}{59}{section*.272}% 
\contentsline {subparagraph}{LM as Linguistic Test Subjects}{59}{section*.273}% 
\contentsline {subparagraph}{Prediction Explanations}{60}{section*.274}% 
\contentsline {subsection}{\numberline {0.14.1}Probes}{60}{subsection.0.14.1}% 
\contentsline {paragraph}{Probing}{60}{section*.275}% 
\contentsline {paragraph}{Contextual Representation of Language}{61}{section*.276}% 
\contentsline {paragraph}{Structural Probe}{61}{section*.277}% 
\contentsline {subparagraph}{Syntax Distance Hypothesis}{61}{section*.278}% 
\contentsline {subparagraph}{Finding a Parse Tree Encoding Distance Metric}{61}{section*.279}% 
\contentsline {subparagraph}{Finding $B$}{61}{section*.280}% 
\contentsline {paragraph}{Conclusion}{61}{section*.281}% 
\contentsline {section}{\numberline {0.15}Prompt-Based Learning}{62}{section.0.15}% 
\contentsline {paragraph}{Finetuning}{62}{section*.282}% 
\contentsline {paragraph}{Zero-shot}{62}{section*.283}% 
\contentsline {paragraph}{One-Shot}{62}{section*.284}% 
\contentsline {paragraph}{Few-Shot}{62}{section*.285}% 
\contentsline {subsection}{\numberline {0.15.1}Prompts}{62}{subsection.0.15.1}% 
\contentsline {paragraph}{Prefix Tuning}{63}{section*.286}% 
\contentsline {paragraph}{Prompt Tuning}{63}{section*.287}% 
\contentsline {paragraph}{Prompt-Tuning Ensembles}{65}{section*.288}% 
\contentsline {paragraph}{Interpretability}{65}{section*.289}% 
\contentsline {subsection}{\numberline {0.15.2}Soft Prompt Transfer}{65}{subsection.0.15.2}% 
\contentsline {section}{\numberline {0.16}Reading Comprehension}{65}{section.0.16}% 
\contentsline {paragraph}{Taxonomy of Question Answering}{65}{section*.290}% 
\contentsline {paragraph}{Difference Between QA Tasks}{66}{section*.291}% 
\contentsline {paragraph}{Machine Comprehension}{66}{section*.292}% 
\contentsline {paragraph}{SQuAD}{66}{section*.293}% 
\contentsline {subparagraph}{Evaluation}{66}{section*.294}% 
\contentsline {paragraph}{SQuAD 2.0}{66}{section*.295}% 
\contentsline {subsection}{\numberline {0.16.1}Neural Models for Reading Comprehension}{67}{subsection.0.16.1}% 
\contentsline {paragraph}{LSTM-based vs BERT-based}{68}{section*.296}% 
\contentsline {paragraph}{Seq2Seq w/ Attention (recap)}{68}{section*.297}% 
\contentsline {subsubsection}{BiDAF}{68}{section*.298}% 
\contentsline {paragraph}{Encoding}{68}{section*.299}% 
\contentsline {paragraph}{Attention}{69}{section*.300}% 
\contentsline {paragraph}{Modeling and Output Layers}{69}{section*.301}% 
\contentsline {subsubsection}{BERT}{70}{section*.302}% 
\contentsline {paragraph}{BERT for Reading Comprehension}{70}{section*.303}% 
\contentsline {paragraph}{Comparing BiDAF and BERT}{70}{section*.304}% 
\contentsline {subsubsection}{Stanford Attentive Reader}{70}{section*.305}% 
\contentsline {subsubsection}{SpanBERT}{70}{section*.306}% 
\contentsline {subsection}{\numberline {0.16.2}State-of-the-Art}{70}{subsection.0.16.2}% 
\contentsline {paragraph}{Is Reading Comprehension Solved?}{70}{section*.307}% 
\contentsline {section}{\numberline {0.17}Open Domain Question Answering}{71}{section.0.17}% 
\contentsline {paragraph}{PiQASso}{71}{section*.308}% 
\contentsline {paragraph}{Question Analysis}{71}{section*.309}% 
\contentsline {paragraph}{Memory}{71}{section*.310}% 
\contentsline {subparagraph}{Retriever-Reader Framework}{72}{section*.311}% 
\contentsline {subparagraph}{Document Reader}{72}{section*.312}% 
\contentsline {paragraph}{Inference Question Answering}{72}{section*.313}% 
\contentsline {paragraph}{Reading Comprehension}{72}{section*.314}% 
\contentsline {paragraph}{Facebook's bAbI Dataset}{72}{section*.315}% 
\contentsline {paragraph}{Question Dependent Recurrent Entity Network for Question Answering}{73}{section*.316}% 
\contentsline {subparagraph}{Input encoder}{73}{section*.317}% 
\contentsline {subparagraph}{Dynamic Memory}{74}{section*.318}% 
\contentsline {subparagraph}{Output Module}{74}{section*.319}% 
\contentsline {subparagraph}{Training}{74}{section*.320}% 
\contentsline {subparagraph}{Architecture}{74}{section*.321}% 
\contentsline {section}{\numberline {0.18}Coreference Resolution}{75}{section.0.18}% 
\contentsline {paragraph}{Coreference Resoultion}{75}{section*.322}% 
\contentsline {paragraph}{Applications}{75}{section*.323}% 
\contentsline {paragraph}{Coreference Resolution in two steps}{75}{section*.324}% 
\contentsline {paragraph}{Mention Detection}{75}{section*.325}% 
\contentsline {paragraph}{Can We Avoid a Pipelined System?}{76}{section*.326}% 
\contentsline {paragraph}{Anaphora}{76}{section*.327}% 
\contentsline {paragraph}{Context}{76}{section*.328}% 
\contentsline {subsection}{\numberline {0.18.1}Coreference Models}{76}{subsection.0.18.1}% 
\contentsline {subsubsection}{Rule-Based Approach}{76}{section*.329}% 
\contentsline {paragraph}{Hobb's Naive Algorithm}{76}{section*.330}% 
\contentsline {paragraph}{Knowledge-based Pronominal Coreference}{77}{section*.331}% 
\contentsline {subsubsection}{Mention Pair/Mention Ranking}{77}{section*.332}% 
\contentsline {paragraph}{Mention Pair Training}{77}{section*.333}% 
\contentsline {paragraph}{Mention Pair Prediction}{77}{section*.334}% 
\contentsline {paragraph}{Coreference Models}{78}{section*.335}% 
\contentsline {subparagraph}{Mention Ranking}{78}{section*.336}% 
\contentsline {subparagraph}{Training}{78}{section*.337}% 
\contentsline {paragraph}{End-To-End Neural Coref Model}{79}{section*.338}% 
\contentsline {paragraph}{BERT-based coref: now has the best results}{80}{section*.339}% 
\contentsline {paragraph}{CorefQA}{80}{section*.340}% 
\contentsline {subparagraph}{Mention Proposal}{80}{section*.341}% 
\contentsline {subparagraph}{Mention Linking}{80}{section*.342}% 
\contentsline {subparagraph}{Mention Pruning}{81}{section*.343}% 
\contentsline {paragraph}{Coreference Evaluation}{81}{section*.344}% 
\contentsline {subparagraph}{B-CUBED}{81}{section*.345}% 
\contentsline {section}{\numberline {0.19}Integrating Knowledge in Language Models}{81}{section.0.19}% 
\contentsline {paragraph}{Recap}{81}{section*.346}% 
\contentsline {paragraph}{Eliciting Knowledge}{81}{section*.347}% 
\contentsline {subparagraph}{Querying Traditional Knowledgebases}{81}{section*.348}% 
\contentsline {paragraph}{Querying LM as Knowledgebases}{82}{section*.349}% 
\contentsline {paragraph}{Advantages of LMs over traditional KBs}{82}{section*.350}% 
\contentsline {subsection}{\numberline {0.19.1}Adding Knowledge to LMs}{82}{subsection.0.19.1}% 
\contentsline {subsubsection}{Add pretrained entity embeddings}{82}{section*.351}% 
\contentsline {subparagraph}{Entity Linking}{83}{section*.352}% 
\contentsline {paragraph}{Incorporate Entity Embeddings from a Different Embeddings Space}{83}{section*.353}% 
\contentsline {subparagraph}{ERNIE}{83}{section*.354}% 
\contentsline {subparagraph}{QAGNN/GreaseLM}{84}{section*.355}% 
\contentsline {subsubsection}{Use an external memory}{85}{section*.356}% 
\contentsline {subparagraph}{KGLM}{85}{section*.357}% 
\contentsline {subsubsection}{Modify Pretrained Data}{85}{section*.358}% 
\contentsline {subparagraph}{WKLM}{86}{section*.359}% 
\contentsline {subsubsection}{Inductive Biases Through Masking}{86}{section*.360}% 
\contentsline {paragraph}{ERNIE}{86}{section*.361}% 
\contentsline {paragraph}{Salient span masking}{86}{section*.362}% 
\contentsline {subsection}{\numberline {0.19.2}Evaluation on Downstream Tasks}{86}{subsection.0.19.2}% 
\contentsline {paragraph}{Knowledge-Intensive Downstream Tasks}{86}{section*.363}% 
\contentsline {section}{\numberline {0.20}Dialogue Systems}{86}{section.0.20}% 
\contentsline {paragraph}{Early Approaches}{86}{section*.364}% 
\contentsline {subparagraph}{Templates and Rules}{87}{section*.365}% 
\contentsline {paragraph}{Open Domain (harder)}{88}{section*.366}% 
\contentsline {paragraph}{Task Oriented (easier)}{88}{section*.367}% 
\contentsline {paragraph}{Retrieval Based (easier)}{88}{section*.368}% 
\contentsline {paragraph}{Generative (harder)}{88}{section*.369}% 
\contentsline {paragraph}{Short Conversation (easier)}{88}{section*.370}% 
\contentsline {paragraph}{Long Conversation (harder)}{88}{section*.371}% 
\contentsline {subsection}{\numberline {0.20.1}Task-Oriented}{88}{subsection.0.20.1}% 
\contentsline {paragraph}{Natural Language Understanding}{89}{section*.372}% 
\contentsline {paragraph}{Dialogue Manager}{89}{section*.373}% 
\contentsline {paragraph}{Dialogue State}{90}{section*.374}% 
\contentsline {paragraph}{Dialogue Act Labeling and Slot Filling}{90}{section*.375}% 
\contentsline {subparagraph}{Slot Filling with ConVEx}{90}{section*.376}% 
\contentsline {subparagraph}{DialoGPT}{90}{section*.377}% 
\contentsline {subsection}{\numberline {0.20.2}Generation-Oriented}{91}{subsection.0.20.2}% 
\contentsline {paragraph}{Transducer Model}{91}{section*.378}% 
\contentsline {paragraph}{Neural Models for Dialogue Response Generation}{91}{section*.379}% 
\contentsline {paragraph}{Seq2Seq}{91}{section*.380}% 
\contentsline {subsection}{\numberline {0.20.3}Retrieval-Based}{91}{subsection.0.20.3}% 
\contentsline {paragraph}{Templates}{91}{section*.381}% 
\contentsline {paragraph}{Retrieval-Based Chat}{91}{section*.382}% 
\contentsline {paragraph}{Neural Response Retrieval}{91}{section*.383}% 
\contentsline {subsection}{\numberline {0.20.4}Challenges}{92}{subsection.0.20.4}% 
\contentsline {paragraph}{Incorporating Context}{92}{section*.384}% 
\contentsline {paragraph}{Coherence}{92}{section*.385}% 
\contentsline {paragraph}{Diversity}{92}{section*.386}% 
\contentsline {paragraph}{Personality}{93}{section*.387}% 
\contentsline {subsection}{\numberline {0.20.5}Google DialogFlow}{93}{subsection.0.20.5}% 
\contentsline {paragraph}{Flow}{94}{section*.388}% 
\contentsline {paragraph}{Intent}{94}{section*.389}% 
\contentsline {section}{\numberline {0.21}Trends and Future of NLP}{94}{section.0.21}% 
\contentsline {paragraph}{Applications}{94}{section*.390}% 
\contentsline {paragraph}{Attention Mechanism}{95}{section*.391}% 
\contentsline {paragraph}{Story of recent years in Deep Learning NLP}{95}{section*.392}% 
\contentsline {paragraph}{Large Language Models and GPT-3}{95}{section*.393}% 
\contentsline {subsection}{\numberline {0.21.1}Compositional Representations and Systematic Generalization}{96}{subsection.0.21.1}% 
\contentsline {paragraph}{Systematicity}{96}{section*.394}% 
\contentsline {paragraph}{Compositionality}{96}{section*.395}% 
\contentsline {paragraph}{Tree Reconstruction Error}{96}{section*.396}% 
\contentsline {subsection}{\numberline {0.21.2}Assessing Deep Learning for NLP}{96}{subsection.0.21.2}% 
\contentsline {paragraph}{Neural Network Effectiveness}{96}{section*.397}% 
\contentsline {paragraph}{Understanding Models by Breaking Them}{96}{section*.398}% 
\contentsline {paragraph}{Remarks}{96}{section*.399}% 
\contentsline {subsection}{\numberline {0.21.3}Inferring From Memory}{96}{subsection.0.21.3}% 
\contentsline {paragraph}{Current State}{96}{section*.400}% 
\contentsline {paragraph}{Memory and Inference}{96}{section*.401}% 
\contentsline {paragraph}{Inter-Sentence}{96}{section*.402}% 
\contentsline {paragraph}{Limits of Single Task Learning}{96}{section*.403}% 
\contentsline {paragraph}{Multiple Tasks}{97}{section*.404}% 
\contentsline {paragraph}{Deep Sequence Models}{97}{section*.405}% 
\contentsline {paragraph}{The 3 Equivalent NLP-Complete Super Tasks}{97}{section*.406}% 
\contentsline {subparagraph}{QA Completeness}{97}{section*.407}% 
\contentsline {subparagraph}{Seq2SQL}{97}{section*.408}% 
\contentsline {paragraph}{Machine Translation}{98}{section*.409}% 
\contentsline {subsection}{\numberline {0.21.4}Huge Models}{98}{subsection.0.21.4}% 
\contentsline {paragraph}{GPT-2 Reactions}{98}{section*.410}% 
\contentsline {paragraph}{High Impact Decisions}{98}{section*.411}% 
\contentsline {section}{\numberline {0.22}Thinking Fast and Slow}{99}{section.0.22}% 
