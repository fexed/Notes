\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Introduction}{2}{section.0.1}% 
\contentsline {paragraph}{What will we learn}{2}{section*.2}% 
\contentsline {paragraph}{Books}{2}{section*.3}% 
\contentsline {paragraph}{Exam}{2}{section*.4}% 
\contentsline {paragraph}{Experimental Approach}{2}{section*.5}% 
\contentsline {paragraph}{Motivations}{2}{section*.6}% 
\contentsline {paragraph}{Structured vs unstructured data}{2}{section*.7}% 
\contentsline {section}{\numberline {0.2}State of the Art}{2}{section.0.2}% 
\contentsline {paragraph}{Early History}{2}{section*.8}% 
\contentsline {paragraph}{Resurgence in the 1990s}{2}{section*.9}% 
\contentsline {paragraph}{Statistical Machine Learning}{2}{section*.10}% 
\contentsline {paragraph}{Traditional Supervised Learning Approach}{3}{section*.11}% 
\contentsline {paragraph}{Technological Breakthroughs}{3}{section*.12}% 
\contentsline {paragraph}{Deep Learning Approach}{3}{section*.13}% 
\contentsline {subparagraph}{Feature representation}{3}{section*.14}% 
\contentsline {subparagraph}{Language Model}{3}{section*.15}% 
\contentsline {subparagraph}{Dealing with Sentences}{3}{section*.16}% 
\contentsline {section}{\numberline {0.3}Language Modeling}{3}{section.0.3}% 
\contentsline {paragraph}{Probabilistic Language Model}{3}{section*.17}% 
\contentsline {paragraph}{Markov Model and N-Grams}{4}{section*.18}% 
\contentsline {paragraph}{Maximum likelihood estimate}{4}{section*.19}% 
\contentsline {paragraph}{Shannon Visualization Method}{4}{section*.20}% 
\contentsline {paragraph}{Shannon Game}{4}{section*.21}% 
\contentsline {paragraph}{Perils of Overfitting}{4}{section*.22}% 
\contentsline {paragraph}{Zipf's Law}{4}{section*.23}% 
\contentsline {subsection}{\numberline {0.3.1}Evaluation and Perplexity}{5}{subsection.0.3.1}% 
\contentsline {paragraph}{Evaluation}{5}{section*.24}% 
\contentsline {paragraph}{Extrinsic Evaluation}{5}{section*.25}% 
\contentsline {paragraph}{Language Identification Task}{5}{section*.26}% 
\contentsline {paragraph}{Difficulty of Extrinsic Evaluation}{5}{section*.27}% 
\contentsline {paragraph}{Perplexity}{5}{section*.28}% 
\contentsline {section}{\numberline {0.4}Representation of Words}{6}{section.0.4}% 
\contentsline {paragraph}{Word Meaning}{6}{section*.29}% 
\contentsline {paragraph}{Linguistic Solution}{6}{section*.30}% 
\contentsline {subparagraph}{Problems with lexical resources}{6}{section*.31}% 
\contentsline {paragraph}{Vector Space Model}{6}{section*.32}% 
\contentsline {subparagraph}{One Hot Representation}{6}{section*.33}% 
\contentsline {subparagraph}{tf*idf Measure}{6}{section*.34}% 
\contentsline {subparagraph}{Classical VSM}{7}{section*.35}% 
\contentsline {paragraph}{Problems with Discrete Symbols}{7}{section*.36}% 
\contentsline {subparagraph}{Intuition}{7}{section*.37}% 
\contentsline {paragraph}{Word Vectors/Embeddings}{7}{section*.38}% 
\contentsline {paragraph}{Distributional Hypothesis}{7}{section*.39}% 
\contentsline {subparagraph}{Word Context Matrix}{7}{section*.40}% 
\contentsline {subparagraph}{Co-Occurrence Matrix}{7}{section*.41}% 
\contentsline {subsection}{\numberline {0.4.1}Word Embeddings}{7}{subsection.0.4.1}% 
\contentsline {paragraph}{Dense Representations}{7}{section*.42}% 
\contentsline {paragraph}{Collobert}{7}{section*.43}% 
\contentsline {paragraph}{Word2Vec}{8}{section*.44}% 
