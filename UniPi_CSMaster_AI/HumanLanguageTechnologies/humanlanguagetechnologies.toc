\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {0.1}Introduction}{3}{section.0.1}%
\contentsline {paragraph}{What will we learn}{3}{section*.2}%
\contentsline {paragraph}{Books}{3}{section*.3}%
\contentsline {paragraph}{Exam}{3}{section*.4}%
\contentsline {paragraph}{Experimental Approach}{3}{section*.5}%
\contentsline {paragraph}{Motivations}{3}{section*.6}%
\contentsline {paragraph}{Structured vs unstructured data}{3}{section*.7}%
\contentsline {section}{\numberline {0.2}State of the Art}{3}{section.0.2}%
\contentsline {paragraph}{Early History}{3}{section*.8}%
\contentsline {paragraph}{Resurgence in the 1990s}{3}{section*.9}%
\contentsline {paragraph}{Statistical Machine Learning}{4}{section*.10}%
\contentsline {paragraph}{Traditional Supervised Learning Approach}{4}{section*.11}%
\contentsline {paragraph}{Technological Breakthroughs}{4}{section*.12}%
\contentsline {paragraph}{Deep Learning (DL) Approach}{4}{section*.13}%
\contentsline {subparagraph}{Feature representation}{4}{section*.14}%
\contentsline {subparagraph}{Language Model}{4}{section*.15}%
\contentsline {subparagraph}{Dealing with sentences}{4}{section*.16}%
\contentsline {section}{\numberline {0.3}Language Modeling}{5}{section.0.3}%
\contentsline {paragraph}{Probabilistic Language Model}{5}{section*.17}%
\contentsline {paragraph}{Markov Model and N-Grams}{5}{section*.18}%
\contentsline {paragraph}{Maximum likelihood estimate (MLE)}{5}{section*.19}%
\contentsline {paragraph}{Shannon Visualization Method}{5}{section*.20}%
\contentsline {paragraph}{Shannon Game}{6}{section*.21}%
\contentsline {paragraph}{Perils of Overfitting}{6}{section*.22}%
\contentsline {subparagraph}{Smoothing}{6}{section*.23}%
\contentsline {paragraph}{Zipf's Law}{6}{section*.24}%
\contentsline {subsection}{\numberline {0.3.1}Evaluation and Perplexity}{7}{subsection.0.3.1}%
\contentsline {paragraph}{Evaluation}{7}{section*.25}%
\contentsline {paragraph}{Extrinsic Evaluation}{7}{section*.26}%
\contentsline {paragraph}{Language Identification Task}{7}{section*.27}%
\contentsline {paragraph}{Difficulty of Extrinsic Evaluation}{7}{section*.28}%
\contentsline {paragraph}{Perplexity}{7}{section*.29}%
\contentsline {section}{\numberline {0.4}Representation of Words}{8}{section.0.4}%
\contentsline {paragraph}{Word Meaning}{8}{section*.30}%
\contentsline {paragraph}{Linguistic Solution}{8}{section*.31}%
\contentsline {paragraph}{Vector Space Model}{8}{section*.32}%
\contentsline {subparagraph}{One Hot Representation}{8}{section*.33}%
\contentsline {subparagraph}{tf*idf Measure}{8}{section*.34}%
\contentsline {subparagraph}{Classical VSM}{8}{section*.35}%
\contentsline {paragraph}{Problems with Discrete Symbols}{8}{section*.36}%
\contentsline {subparagraph}{Intuition}{8}{section*.37}%
\contentsline {paragraph}{Word Vectors/Embeddings}{9}{section*.38}%
\contentsline {paragraph}{Distributional Hypothesis}{9}{section*.39}%
\contentsline {subparagraph}{Word Context Matrix}{9}{section*.40}%
\contentsline {subparagraph}{Co-Occurrence Matrix}{9}{section*.41}%
\contentsline {subsection}{\numberline {0.4.1}Word Embeddings}{9}{subsection.0.4.1}%
\contentsline {paragraph}{Dense Representations}{9}{section*.42}%
\contentsline {paragraph}{Collobert}{9}{section*.43}%
\contentsline {paragraph}{Word2Vec}{9}{section*.44}%
\contentsline {subparagraph}{Skip-Gram}{11}{section*.45}%
\contentsline {subparagraph}{Objective Function}{11}{section*.46}%
\contentsline {paragraph}{Softmax}{11}{section*.47}%
\contentsline {paragraph}{Can we really capture the concept represented by a word?}{12}{section*.48}%
\contentsline {paragraph}{Negative Sampling}{12}{section*.49}%
\contentsline {paragraph}{CBoW}{12}{section*.50}%
\contentsline {paragraph}{Which Embeddings}{12}{section*.51}%
\contentsline {paragraph}{GloVe}{12}{section*.52}%
\contentsline {paragraph}{fastText}{12}{section*.53}%
\contentsline {paragraph}{Co-Occurrence Counts}{12}{section*.54}%
\contentsline {paragraph}{Weighting}{12}{section*.55}%
\contentsline {subparagraph}{Which One?}{13}{section*.56}%
\contentsline {paragraph}{Parallel word2vec}{13}{section*.57}%
\contentsline {paragraph}{Computing embeddings}{13}{section*.58}%
\contentsline {paragraph}{Gensim}{13}{section*.59}%
\contentsline {paragraph}{Fang}{13}{section*.60}%
\contentsline {subsection}{\numberline {0.4.2}Evaluation}{13}{subsection.0.4.2}%
\contentsline {paragraph}{Polysemy}{13}{section*.61}%
\contentsline {paragraph}{Extrinsic Vector Evaluation}{13}{section*.62}%
\contentsline {subsubsection}{Embeddings in Neural Networks}{13}{section*.63}%
\contentsline {subsubsection}{Limits of Word Embeddings}{13}{section*.64}%
\contentsline {paragraph}{Word Senses and Ambiguity}{14}{section*.65}%
\contentsline {paragraph}{Sentiment Specific}{14}{section*.66}%
\contentsline {paragraph}{Context Aware Word Embeddings}{14}{section*.67}%
\contentsline {paragraph}{ELMo}{14}{section*.68}%
\contentsline {paragraph}{BERT}{14}{section*.69}%
\contentsline {section}{\numberline {0.5}Text Classification}{14}{section.0.5}%
\contentsline {paragraph}{Definition}{14}{section*.70}%
\contentsline {paragraph}{Hand-Coded Rules}{15}{section*.71}%
\contentsline {paragraph}{Supervised Machine Learning}{15}{section*.72}%
\contentsline {subsection}{\numberline {0.5.1}Naive Bayes}{15}{subsection.0.5.1}%
\contentsline {paragraph}{Bag of words representation}{15}{section*.73}%
\contentsline {paragraph}{Bayes Rule}{15}{section*.74}%
\contentsline {paragraph}{Text classification problem}{15}{section*.75}%
\contentsline {subsubsection}{Naive Bayes Classifiers}{15}{section*.76}%
\contentsline {paragraph}{Naive Bayes Assumption}{16}{section*.77}%
\contentsline {subsubsection}{Multinomial Naive Bayes Text Classification}{16}{section*.78}%
\contentsline {paragraph}{Learning the Model}{16}{section*.79}%
\contentsline {paragraph}{Smoothing to Avoid Overfitting}{16}{section*.80}%
\contentsline {paragraph}{Classifying}{16}{section*.81}%
\contentsline {paragraph}{Preventing Underflow}{16}{section*.82}%
\contentsline {paragraph}{Generate}{16}{section*.83}%
\contentsline {paragraph}{Naive Bayes and Language Modeling}{17}{section*.84}%
\contentsline {paragraph}{Evaluating Categorization}{17}{section*.85}%
\contentsline {paragraph}{Micro vs Macro Averaging}{17}{section*.86}%
\contentsline {paragraph}{Multiclass Classification}{17}{section*.87}%
\contentsline {paragraph}{Training Size}{17}{section*.88}%
\contentsline {paragraph}{Violation of Naive Bayes Assumptions}{17}{section*.89}%
\contentsline {paragraph}{Example: SpamAssassin}{17}{section*.90}%
\contentsline {section}{\numberline {0.6}Tokenization}{17}{section.0.6}%
\contentsline {paragraph}{Regular Expressions}{17}{section*.91}%
\contentsline {paragraph}{Tokenization}{18}{section*.92}%
\contentsline {subparagraph}{What's a Word?}{18}{section*.93}%
\contentsline {paragraph}{Stanza Tokenizer}{18}{section*.94}%
\contentsline {paragraph}{Clitics}{18}{section*.95}%
\contentsline {paragraph}{Normalization}{18}{section*.96}%
\contentsline {subparagraph}{Morphology and Stemming}{18}{section*.97}%
\contentsline {section}{\numberline {0.7}Classification}{18}{section.0.7}%
\contentsline {paragraph}{Naive Bayes}{19}{section*.98}%
\contentsline {paragraph}{Decision Trees}{19}{section*.99}%
\contentsline {paragraph}{Linear vs non-linear algorithms}{19}{section*.100}%
\contentsline {subsection}{\numberline {0.7.1}Linear Binary Classification}{19}{subsection.0.7.1}%
\contentsline {paragraph}{Perceptron}{19}{section*.101}%
\contentsline {subparagraph}{Multi-Layer Perceptron}{19}{section*.102}%
\contentsline {paragraph}{Universal Approximation Theorem}{19}{section*.103}%
\contentsline {subsection}{\numberline {0.7.2}Hidden Markov Models}{20}{subsection.0.7.2}%
\contentsline {paragraph}{Markov Chain}{20}{section*.104}%
\contentsline {paragraph}{Hidden Markov Model}{20}{section*.105}%
\contentsline {subparagraph}{Example: speech}{20}{section*.106}%
\contentsline {paragraph}{Markov Assumption}{20}{section*.107}%
\contentsline {paragraph}{Output-independence assumption}{20}{section*.108}%
\contentsline {paragraph}{Three basic problems}{21}{section*.109}%
\contentsline {paragraph}{Computing the likelihood}{21}{section*.110}%
\contentsline {subparagraph}{Forward Algorithm}{21}{section*.111}%
\contentsline {paragraph}{Decoding}{21}{section*.112}%
\contentsline {paragraph}{Viterbi Algorithm}{22}{section*.113}%
\contentsline {paragraph}{Training a HMM}{22}{section*.114}%
\contentsline {subsubsection}{Part of Speech Tagging}{22}{section*.115}%
\contentsline {paragraph}{Two kinds of probabilities}{23}{section*.116}%
\contentsline {subsubsection}{Sequence Tagging}{23}{section*.117}%
\contentsline {paragraph}{Discriminative Model}{23}{section*.118}%
\contentsline {paragraph}{Generative Model}{23}{section*.119}%
\contentsline {paragraph}{Naive Bayes}{23}{section*.120}%
\contentsline {paragraph}{Logistic Regression}{23}{section*.121}%
\contentsline {paragraph}{Problems}{24}{section*.122}%
\contentsline {paragraph}{MEMM}{24}{section*.123}%
\contentsline {subsubsection}{Named Entity Tagging}{25}{section*.124}%
\contentsline {paragraph}{Approaches}{26}{section*.125}%
\contentsline {section}{\numberline {0.8}Convolutional Neural Networks for NLP}{26}{section.0.8}%
\contentsline {paragraph}{Distant Supervision}{27}{section*.126}%
\contentsline {paragraph}{Sentiment Specific Word Embeddings}{27}{section*.127}%
\contentsline {paragraph}{Ensemble of Classifiers}{27}{section*.128}%
\contentsline {paragraph}{Sentiment Classification from a Single Neuron}{27}{section*.129}%
\contentsline {subsection}{\numberline {0.8.1}Regularization}{27}{subsection.0.8.1}%
\contentsline {paragraph}{Pitfalls when using word vectors}{28}{section*.130}%
\contentsline {section}{\numberline {0.9}Recurrent Neural Networks}{28}{section.0.9}%
\contentsline {paragraph}{Recap}{28}{section*.131}%
\contentsline {paragraph}{Recurrent}{28}{section*.132}%
\contentsline {paragraph}{Hidden Units}{28}{section*.133}%
\contentsline {paragraph}{Advantages}{28}{section*.134}%
\contentsline {paragraph}{Disadvantages}{29}{section*.135}%
\contentsline {paragraph}{Simple RNN Language Model}{29}{section*.136}%
\contentsline {paragraph}{Vanilla RNN}{29}{section*.137}%
\contentsline {paragraph}{Training}{29}{section*.138}%
\contentsline {paragraph}{Backpropagation through time}{30}{section*.139}%
\contentsline {paragraph}{Training RNN Language Model}{30}{section*.140}%
\contentsline {paragraph}{Vanishing Gradients}{30}{section*.141}%
\contentsline {paragraph}{Exploding Gradients}{30}{section*.142}%
\contentsline {subsection}{\numberline {0.9.1}Specializations}{31}{subsection.0.9.1}%
\contentsline {paragraph}{Notation}{31}{section*.143}%
\contentsline {subsubsection}{LSTM}{31}{section*.144}%
\contentsline {paragraph}{Long Short-Term Memory}{31}{section*.145}%
\contentsline {subsubsection}{GRU}{32}{section*.146}%
\contentsline {paragraph}{Gated Recurrent Units}{32}{section*.147}%
\contentsline {section}{\numberline {0.10}Parsing}{33}{section.0.10}%
\contentsline {paragraph}{Dealing with Text}{33}{section*.148}%
\contentsline {paragraph}{Sentence Structure}{33}{section*.149}%
\contentsline {paragraph}{Practical uses of parsing}{33}{section*.150}%
\contentsline {subsection}{\numberline {0.10.1}Parsing Approaches}{34}{subsection.0.10.1}%
\contentsline {subsubsection}{Constituency Grammar}{34}{section*.151}%
\contentsline {paragraph}{Context Free Grammars}{34}{section*.152}%
\contentsline {paragraph}{Constituency Parsing}{34}{section*.153}%
\contentsline {subparagraph}{Statistical Parsing}{34}{section*.154}%
\contentsline {subsubsection}{Dependency Grammar}{34}{section*.155}%
\contentsline {paragraph}{Dependency Structure}{34}{section*.156}%
\contentsline {paragraph}{Difference Between Constituency Tree and Dependency Trees}{35}{section*.157}%
\contentsline {paragraph}{Annotation Constraints}{35}{section*.158}%
\contentsline {paragraph}{Data-Driven Dependency Parsing}{35}{section*.159}%
\contentsline {subparagraph}{Transition-Based Shift-Reduce Parsing}{35}{section*.160}%
\contentsline {paragraph}{Parsing as Classification}{35}{section*.161}%
\contentsline {paragraph}{Dependency Graph}{36}{section*.162}%
\contentsline {paragraph}{Arc Standard Transitions}{36}{section*.163}%
\contentsline {paragraph}{Parser Algorithm}{36}{section*.164}%
\contentsline {paragraph}{Oracle}{36}{section*.165}%
\contentsline {paragraph}{Projectivity}{36}{section*.166}%
\contentsline {paragraph}{Arc-Standard Algorithm}{36}{section*.167}%
\contentsline {paragraph}{Arc Eager Transitions}{37}{section*.168}%
\contentsline {paragraph}{Non-Projective Transitions}{37}{section*.169}%
\contentsline {paragraph}{Learning Procedure}{37}{section*.170}%
\contentsline {paragraph}{CoNLL-X Shared Task}{37}{section*.171}%
\contentsline {paragraph}{Problems with Oracles}{37}{section*.172}%
\contentsline {subsubsection}{Graph-Based Depencency Parsing}{37}{section*.173}%
\contentsline {paragraph}{NN Graph-Based Parser}{38}{section*.174}%
\contentsline {subparagraph}{Parser}{38}{section*.175}%
\contentsline {subparagraph}{Dependency Relations}{38}{section*.176}%
\contentsline {subparagraph}{Self-Attention}{38}{section*.177}%
\contentsline {subparagraph}{Classifier for Labels}{38}{section*.178}%
\contentsline {section}{\numberline {0.11}Universal Dependencies}{39}{section.0.11}%
\contentsline {paragraph}{Goals}{39}{section*.179}%
\contentsline {paragraph}{Guiding Principles}{39}{section*.180}%
\contentsline {paragraph}{Design Principles}{39}{section*.181}%
\contentsline {paragraph}{Morphological Annotation}{39}{section*.182}%
\contentsline {paragraph}{Syntactic Annotation}{40}{section*.183}%
\contentsline {paragraph}{Coordination}{40}{section*.184}%
\contentsline {paragraph}{Enhanced Dependencies}{40}{section*.185}%
\contentsline {paragraph}{Basic}{41}{section*.186}%
\contentsline {paragraph}{Enhanced}{41}{section*.187}%
\contentsline {paragraph}{Dependency Structure}{41}{section*.188}%
\contentsline {paragraph}{Parsing}{41}{section*.189}%
\contentsline {section}{\numberline {0.12}Machine Translation}{41}{section.0.12}%
\contentsline {paragraph}{Issues}{41}{section*.190}%
\contentsline {paragraph}{Alignment}{41}{section*.191}%
\contentsline {paragraph}{MT Already Good for...}{42}{section*.192}%
\contentsline {paragraph}{MT Not Yet Good Enough for...}{42}{section*.193}%
\contentsline {subsection}{\numberline {0.12.1}Language Similarities and Divergences}{42}{subsection.0.12.1}%
\contentsline {paragraph}{Typology}{42}{section*.194}%
\contentsline {paragraph}{Morphology}{42}{section*.195}%
\contentsline {paragraph}{Morphological Variation}{42}{section*.196}%
\contentsline {paragraph}{Segmentation Variation}{42}{section*.197}%
\contentsline {paragraph}{Lexical Gaps}{42}{section*.198}%
\contentsline {paragraph}{Event-To-Argument Divergences}{42}{section*.199}%
\contentsline {subsection}{\numberline {0.12.2}Classical Techniques}{43}{subsection.0.12.2}%
\contentsline {paragraph}{Direct Translation}{43}{section*.200}%
\contentsline {subparagraph}{Pros}{43}{section*.201}%
\contentsline {subparagraph}{Cons}{43}{section*.202}%
\contentsline {paragraph}{Transfer Model}{43}{section*.203}%
\contentsline {subparagraph}{Lexical Transfer}{43}{section*.204}%
\contentsline {subparagraph}{Systram}{43}{section*.205}%
\contentsline {paragraph}{Interlingua}{44}{section*.206}%
\contentsline {subparagraph}{Pros}{44}{section*.207}%
\contentsline {subparagraph}{Cons}{44}{section*.208}%
\contentsline {subsection}{\numberline {0.12.3}Statistical Machine Translation}{44}{subsection.0.12.3}%
\contentsline {paragraph}{Example}{44}{section*.209}%
\contentsline {paragraph}{What Makes a Good Translation}{44}{section*.210}%
\contentsline {paragraph}{Fluency $P(T)$}{45}{section*.211}%
\contentsline {paragraph}{Faithfulness $P(S\:|\:T)$}{45}{section*.212}%
\contentsline {subparagraph}{Sentence Alignment}{45}{section*.213}%
\contentsline {subparagraph}{Word Alignment}{45}{section*.214}%
\contentsline {paragraph}{Three Problems for Statistical Machine Translation}{45}{section*.215}%
\contentsline {subsection}{\numberline {0.12.4}Phrase Based Machine Translation}{45}{subsection.0.12.4}%
\contentsline {paragraph}{Translation Probabilities}{46}{section*.216}%
\contentsline {paragraph}{Distortion probability}{46}{section*.217}%
\contentsline {paragraph}{Training $P(F\:|\:E)$}{46}{section*.218}%
\contentsline {paragraph}{Computing Word Alignments}{47}{section*.219}%
\contentsline {paragraph}{Training Alignment Probabilities}{47}{section*.220}%
\contentsline {subsubsection}{Phrase-Based Translation Model}{47}{section*.222}%
\contentsline {paragraph}{Phrase Alignment}{48}{section*.223}%
\contentsline {paragraph}{Decoding}{48}{section*.224}%
\contentsline {paragraph}{Evaluation}{48}{section*.225}%
\contentsline {subparagraph}{Scores}{48}{section*.226}%
\contentsline {subsection}{\numberline {0.12.5}Syntax Based Statistical Machine Translation}{48}{subsection.0.12.5}%
\contentsline {paragraph}{Synchronous Grammar}{49}{section*.227}%
\contentsline {paragraph}{Synchronous Derivations and Translation Models}{49}{section*.228}%
\contentsline {paragraph}{Use of Dependency Parsing}{49}{section*.229}%
\contentsline {subsection}{\numberline {0.12.6}Minimum Error Rate Training}{49}{subsection.0.12.6}%
\contentsline {paragraph}{Conclusions}{49}{section*.230}%
\contentsline {section}{\numberline {0.13}Neural Machine Translation}{49}{section.0.13}%
\contentsline {paragraph}{NMT}{49}{section*.231}%
\contentsline {paragraph}{Beam Search}{50}{section*.232}%
\contentsline {subparagraph}{Example}{51}{section*.233}%
\contentsline {subparagraph}{Stopping Criterion}{51}{section*.234}%
\contentsline {subparagraph}{Finishing Up}{51}{section*.235}%
\contentsline {subparagraph}{Benefits}{51}{section*.236}%
\contentsline {subparagraph}{Disadvantages}{51}{section*.237}%
\contentsline {paragraph}{Attention}{52}{section*.238}%
\contentsline {subparagraph}{In equations}{52}{section*.239}%
\contentsline {subparagraph}{Attention Variants}{53}{section*.240}%
\contentsline {subsection}{\numberline {0.13.1}Self-Attention}{53}{subsection.0.13.1}%
\contentsline {paragraph}{Attention}{53}{section*.241}%
\contentsline {paragraph}{Issues with Recurrent Models}{54}{section*.242}%
\contentsline {paragraph}{Word Windows}{54}{section*.243}%
\contentsline {paragraph}{Attention}{54}{section*.244}%
\contentsline {paragraph}{Self-Attention}{54}{section*.245}%
\contentsline {paragraph}{Vector Notation}{54}{section*.246}%
\contentsline {paragraph}{Self-Attention as a NLP Building Block}{55}{section*.247}%
\contentsline {subparagraph}{No Notion of Order}{55}{section*.248}%
\contentsline {subparagraph}{Adding Non-Linearities}{55}{section*.249}%
\contentsline {subparagraph}{Future}{55}{section*.250}%
\contentsline {subsection}{\numberline {0.13.2}Transformers}{55}{subsection.0.13.2}%
\contentsline {paragraph}{Self-Attention}{56}{section*.251}%
\contentsline {paragraph}{Multi-Headed Attention}{56}{section*.252}%
\contentsline {paragraph}{Training Tricks}{56}{section*.253}%
\contentsline {subsubsection}{Transformers Library}{57}{section*.254}%
\contentsline {paragraph}{Hugging Face Transformers}{57}{section*.255}%
\contentsline {subsubsection}{Transformers Architectures}{57}{section*.256}%
\contentsline {paragraph}{Pretraining Transformers}{58}{section*.257}%
\contentsline {subparagraph}{Pretraining through language modeling}{58}{section*.258}%
\contentsline {paragraph}{Pretraining-Finetuning Paradigm}{58}{section*.259}%
\contentsline {subparagraph}{SGD}{58}{section*.260}%
\contentsline {paragraph}{Pretraining for Three Types of Architectures}{58}{section*.261}%
\contentsline {subparagraph}{Pretraining Decoders}{58}{section*.262}%
\contentsline {paragraph}{Pretraining Encoders}{59}{section*.263}%
\contentsline {paragraph}{Problems with previous methods}{59}{section*.264}%
\contentsline {subparagraph}{BERT}{59}{section*.265}%
\contentsline {paragraph}{Masked LM}{60}{section*.266}%
\contentsline {paragraph}{Wordpiece}{60}{section*.267}%
\contentsline {paragraph}{Pretraining Encoder-Decoders}{60}{section*.268}%
\contentsline {paragraph}{What pretraining objective to use?}{60}{section*.269}%
\contentsline {paragraph}{Pre-training Tasks}{60}{section*.270}%
\contentsline {paragraph}{Masked LM}{60}{section*.271}%
\contentsline {paragraph}{Next Sentence Prediction}{60}{section*.272}%
\contentsline {paragraph}{In-Context Learning}{60}{section*.273}%
\contentsline {paragraph}{Distillation}{60}{section*.274}%
\contentsline {section}{\numberline {0.14}Analysis of Language Models}{61}{section.0.14}%
\contentsline {paragraph}{Questions About Language Models}{61}{section*.275}%
\contentsline {paragraph}{What Linguistic Knowledge is Present in LM?}{61}{section*.276}%
\contentsline {subparagraph}{Unsupervised NER}{61}{section*.277}%
\contentsline {subparagraph}{LM Effectivness}{61}{section*.278}%
\contentsline {subparagraph}{LM as Linguistic Test Subjects}{61}{section*.279}%
\contentsline {subparagraph}{Prediction Explanations}{62}{section*.280}%
\contentsline {subsection}{\numberline {0.14.1}Probes}{62}{subsection.0.14.1}%
\contentsline {paragraph}{Probing}{62}{section*.281}%
\contentsline {paragraph}{Contextual Representation of Language}{63}{section*.282}%
\contentsline {paragraph}{Structural Probe}{63}{section*.283}%
\contentsline {subparagraph}{Syntax Distance Hypothesis}{63}{section*.284}%
\contentsline {subparagraph}{Finding a Parse Tree Encoding Distance Metric}{63}{section*.285}%
\contentsline {subparagraph}{Finding $B$}{63}{section*.286}%
\contentsline {paragraph}{Conclusion}{63}{section*.287}%
\contentsline {section}{\numberline {0.15}Prompt-Based Learning}{64}{section.0.15}%
\contentsline {paragraph}{Finetuning}{64}{section*.288}%
\contentsline {paragraph}{Zero-shot}{64}{section*.289}%
\contentsline {paragraph}{One-Shot}{64}{section*.290}%
\contentsline {paragraph}{Few-Shot}{64}{section*.291}%
\contentsline {subsection}{\numberline {0.15.1}Prompts}{64}{subsection.0.15.1}%
\contentsline {paragraph}{Prefix Tuning}{65}{section*.292}%
\contentsline {paragraph}{Prompt Tuning}{65}{section*.293}%
\contentsline {paragraph}{Prompt-Tuning Ensembles}{67}{section*.294}%
\contentsline {paragraph}{Interpretability}{67}{section*.295}%
\contentsline {subsection}{\numberline {0.15.2}Soft Prompt Transfer}{67}{subsection.0.15.2}%
\contentsline {section}{\numberline {0.16}Reading Comprehension}{67}{section.0.16}%
\contentsline {paragraph}{Taxonomy of Question Answering}{67}{section*.296}%
\contentsline {paragraph}{Difference Between QA Tasks}{68}{section*.297}%
\contentsline {paragraph}{Machine Comprehension}{68}{section*.298}%
\contentsline {paragraph}{SQuAD}{68}{section*.299}%
\contentsline {subparagraph}{Evaluation}{68}{section*.300}%
\contentsline {paragraph}{SQuAD 2.0}{68}{section*.301}%
\contentsline {subsection}{\numberline {0.16.1}Neural Models for Reading Comprehension}{69}{subsection.0.16.1}%
\contentsline {paragraph}{LSTM-based vs BERT-based}{69}{section*.302}%
\contentsline {paragraph}{Seq2Seq w/ Attention (recap)}{69}{section*.303}%
\contentsline {subsubsection}{BiDAF}{69}{section*.304}%
\contentsline {paragraph}{Encoding}{69}{section*.305}%
\contentsline {paragraph}{Attention}{70}{section*.306}%
\contentsline {paragraph}{Modeling and Output Layers}{70}{section*.307}%
\contentsline {subsubsection}{BERT}{71}{section*.308}%
\contentsline {paragraph}{BERT for Reading Comprehension}{71}{section*.309}%
\contentsline {paragraph}{Comparing BiDAF and BERT}{71}{section*.310}%
\contentsline {subsubsection}{Stanford Attentive Reader}{71}{section*.311}%
\contentsline {subsubsection}{SpanBERT}{71}{section*.312}%
\contentsline {subsection}{\numberline {0.16.2}State-of-the-Art}{71}{subsection.0.16.2}%
\contentsline {paragraph}{Is Reading Comprehension Solved?}{71}{section*.313}%
\contentsline {section}{\numberline {0.17}Open Domain Question Answering}{72}{section.0.17}%
\contentsline {paragraph}{PiQASso}{72}{section*.314}%
\contentsline {paragraph}{Question Analysis}{72}{section*.315}%
\contentsline {paragraph}{Memory}{72}{section*.316}%
\contentsline {subparagraph}{Retriever-Reader Framework}{73}{section*.317}%
\contentsline {subparagraph}{Document Reader}{73}{section*.318}%
\contentsline {paragraph}{Inference Question Answering}{73}{section*.319}%
\contentsline {paragraph}{Reading Comprehension}{73}{section*.320}%
\contentsline {paragraph}{Facebook's bAbI Dataset}{73}{section*.321}%
\contentsline {paragraph}{Question Dependent Recurrent Entity Network for Question Answering}{74}{section*.322}%
\contentsline {subparagraph}{Input encoder}{74}{section*.323}%
\contentsline {subparagraph}{Dynamic Memory}{75}{section*.324}%
\contentsline {subparagraph}{Output Module}{75}{section*.325}%
\contentsline {subparagraph}{Training}{75}{section*.326}%
\contentsline {subparagraph}{Architecture}{76}{section*.327}%
\contentsline {section}{\numberline {0.18}Coreference Resolution}{76}{section.0.18}%
\contentsline {paragraph}{Coreference Resoultion}{76}{section*.328}%
\contentsline {paragraph}{Applications}{76}{section*.329}%
\contentsline {paragraph}{Coreference Resolution in two steps}{76}{section*.330}%
\contentsline {paragraph}{Mention Detection}{76}{section*.331}%
\contentsline {paragraph}{Can We Avoid a Pipelined System?}{77}{section*.332}%
\contentsline {paragraph}{Anaphora}{77}{section*.333}%
\contentsline {paragraph}{Context}{77}{section*.334}%
\contentsline {subsection}{\numberline {0.18.1}Coreference Models}{77}{subsection.0.18.1}%
\contentsline {subsubsection}{Rule-Based Approach}{77}{section*.335}%
\contentsline {paragraph}{Hobb's Naive Algorithm}{77}{section*.336}%
\contentsline {paragraph}{Knowledge-based Pronominal Coreference}{78}{section*.337}%
\contentsline {subsubsection}{Mention Pair/Mention Ranking}{78}{section*.338}%
\contentsline {paragraph}{Mention Pair Training}{78}{section*.339}%
\contentsline {paragraph}{Mention Pair Prediction}{78}{section*.340}%
\contentsline {paragraph}{Coreference Models}{79}{section*.341}%
\contentsline {subparagraph}{Mention Ranking}{79}{section*.342}%
\contentsline {subparagraph}{Training}{79}{section*.343}%
\contentsline {paragraph}{End-To-End Neural Coref Model}{80}{section*.344}%
\contentsline {paragraph}{BERT-based coref: now has the best results}{81}{section*.345}%
\contentsline {paragraph}{CorefQA}{81}{section*.346}%
\contentsline {subparagraph}{Mention Proposal}{81}{section*.347}%
\contentsline {subparagraph}{Mention Linking}{81}{section*.348}%
\contentsline {subparagraph}{Mention Pruning}{82}{section*.349}%
\contentsline {paragraph}{Coreference Evaluation}{82}{section*.350}%
\contentsline {subparagraph}{B-CUBED}{82}{section*.351}%
\contentsline {section}{\numberline {0.19}Integrating Knowledge in Language Models}{82}{section.0.19}%
\contentsline {paragraph}{Recap}{82}{section*.352}%
\contentsline {paragraph}{Eliciting Knowledge}{82}{section*.353}%
\contentsline {subparagraph}{Querying Traditional Knowledgebases}{82}{section*.354}%
\contentsline {paragraph}{Querying LM as Knowledgebases}{82}{section*.355}%
\contentsline {paragraph}{Advantages of LMs over traditional KBs}{83}{section*.356}%
\contentsline {subsection}{\numberline {0.19.1}Adding Knowledge to LMs}{83}{subsection.0.19.1}%
\contentsline {subsubsection}{Add pretrained entity embeddings}{83}{section*.357}%
\contentsline {subparagraph}{Entity Linking}{83}{section*.358}%
\contentsline {paragraph}{Incorporate Entity Embeddings from a Different Embeddings Space}{83}{section*.359}%
\contentsline {subparagraph}{ERNIE}{84}{section*.360}%
\contentsline {subparagraph}{QAGNN/GreaseLM}{85}{section*.361}%
\contentsline {subsubsection}{Use an external memory}{86}{section*.362}%
\contentsline {subparagraph}{KGLM}{86}{section*.363}%
\contentsline {subsubsection}{Modify Pretrained Data}{86}{section*.364}%
\contentsline {subparagraph}{WKLM}{87}{section*.365}%
\contentsline {subsubsection}{Inductive Biases Through Masking}{87}{section*.366}%
\contentsline {paragraph}{ERNIE}{87}{section*.367}%
\contentsline {paragraph}{Salient span masking}{87}{section*.368}%
\contentsline {subsection}{\numberline {0.19.2}Evaluation on Downstream Tasks}{87}{subsection.0.19.2}%
\contentsline {paragraph}{Knowledge-Intensive Downstream Tasks}{87}{section*.369}%
\contentsline {section}{\numberline {0.20}Dialogue Systems}{87}{section.0.20}%
\contentsline {paragraph}{Early Approaches}{87}{section*.370}%
\contentsline {subparagraph}{Templates and Rules}{88}{section*.371}%
\contentsline {paragraph}{Task Oriented vs Open Domain}{88}{section*.372}%
\contentsline {paragraph}{Task Oriented (easier)}{88}{section*.373}%
\contentsline {paragraph}{Open Domain (harder)}{88}{section*.374}%
\contentsline {paragraph}{Retrieval Based vs Generative}{88}{section*.375}%
\contentsline {paragraph}{Retrieval Based (easier)}{88}{section*.376}%
\contentsline {paragraph}{Generative (harder)}{88}{section*.377}%
\contentsline {paragraph}{Short vs Long conversations}{88}{section*.378}%
\contentsline {paragraph}{Short Conversation (easier)}{88}{section*.379}%
\contentsline {paragraph}{Long Conversation (harder)}{88}{section*.380}%
\contentsline {subsection}{\numberline {0.20.1}Task-Oriented}{89}{subsection.0.20.1}%
\contentsline {paragraph}{Natural Language Understanding}{89}{section*.381}%
\contentsline {paragraph}{Dialogue Manager}{89}{section*.382}%
\contentsline {paragraph}{Dialogue State}{90}{section*.383}%
\contentsline {paragraph}{Dialogue Act Labeling and Slot Filling}{90}{section*.384}%
\contentsline {subparagraph}{Slot Filling with ConVEx}{90}{section*.385}%
\contentsline {subparagraph}{DialoGPT}{90}{section*.386}%
\contentsline {subsection}{\numberline {0.20.2}Generation-Oriented}{92}{subsection.0.20.2}%
\contentsline {paragraph}{Transducer Model}{92}{section*.387}%
\contentsline {paragraph}{Neural Models for Dialogue Response Generation}{92}{section*.388}%
\contentsline {paragraph}{Seq2Seq}{92}{section*.389}%
\contentsline {subsection}{\numberline {0.20.3}Retrieval-Based}{92}{subsection.0.20.3}%
\contentsline {paragraph}{Templates}{92}{section*.390}%
\contentsline {paragraph}{Retrieval-Based Chat}{92}{section*.391}%
\contentsline {paragraph}{Neural Response Retrieval}{92}{section*.392}%
\contentsline {subsection}{\numberline {0.20.4}Challenges}{93}{subsection.0.20.4}%
\contentsline {paragraph}{Incorporating Context}{93}{section*.393}%
\contentsline {paragraph}{Coherence}{93}{section*.394}%
\contentsline {paragraph}{Diversity}{93}{section*.395}%
\contentsline {paragraph}{Personality}{94}{section*.396}%
\contentsline {subsection}{\numberline {0.20.5}Google DialogFlow}{94}{subsection.0.20.5}%
\contentsline {paragraph}{Flow}{95}{section*.397}%
\contentsline {paragraph}{Intent}{95}{section*.398}%
\contentsline {section}{\numberline {0.21}Trends and Future of NLP}{95}{section.0.21}%
\contentsline {paragraph}{Applications}{95}{section*.399}%
\contentsline {paragraph}{Attention Mechanism}{96}{section*.400}%
\contentsline {paragraph}{Story of recent years in Deep Learning NLP}{96}{section*.401}%
\contentsline {paragraph}{Large Language Models and GPT-3}{96}{section*.402}%
\contentsline {subsection}{\numberline {0.21.1}Compositional Representations and Systematic Generalization}{97}{subsection.0.21.1}%
\contentsline {paragraph}{Systematicity}{97}{section*.403}%
\contentsline {paragraph}{Compositionality}{97}{section*.404}%
\contentsline {paragraph}{Tree Reconstruction Error}{97}{section*.405}%
\contentsline {subsection}{\numberline {0.21.2}Assessing Deep Learning for NLP}{97}{subsection.0.21.2}%
\contentsline {paragraph}{Neural Network Effectiveness}{97}{section*.406}%
\contentsline {paragraph}{Understanding Models by Breaking Them}{97}{section*.407}%
\contentsline {paragraph}{Remarks}{97}{section*.408}%
\contentsline {subsection}{\numberline {0.21.3}Inferring From Memory}{97}{subsection.0.21.3}%
\contentsline {paragraph}{Current State}{97}{section*.409}%
\contentsline {paragraph}{Memory and Inference}{97}{section*.410}%
\contentsline {paragraph}{Inter-Sentence}{97}{section*.411}%
\contentsline {paragraph}{Limits of Single Task Learning}{97}{section*.412}%
\contentsline {paragraph}{Multiple Tasks}{98}{section*.413}%
\contentsline {paragraph}{Deep Sequence Models}{98}{section*.414}%
\contentsline {paragraph}{The 3 Equivalent NLP-Complete Super Tasks}{98}{section*.415}%
\contentsline {subparagraph}{QA Completeness}{98}{section*.416}%
\contentsline {subparagraph}{Seq2SQL}{98}{section*.417}%
\contentsline {paragraph}{Machine Translation}{99}{section*.418}%
\contentsline {subsection}{\numberline {0.21.4}Huge Models}{99}{subsection.0.21.4}%
\contentsline {paragraph}{GPT-2 Reactions}{99}{section*.419}%
\contentsline {paragraph}{Responsibility}{99}{section*.420}%
\contentsline {paragraph}{High Impact Decisions}{99}{section*.421}%
\contentsline {section}{\numberline {0.22}Thinking Fast and Slow}{100}{section.0.22}%
