\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Introduction}{2}{section.0.1}% 
\contentsline {paragraph}{What will we learn}{2}{section*.2}% 
\contentsline {paragraph}{Books}{2}{section*.3}% 
\contentsline {paragraph}{Exam}{2}{section*.4}% 
\contentsline {paragraph}{Experimental Approach}{2}{section*.5}% 
\contentsline {paragraph}{Motivations}{2}{section*.6}% 
\contentsline {paragraph}{Structured vs unstructured data}{2}{section*.7}% 
\contentsline {section}{\numberline {0.2}State of the Art}{2}{section.0.2}% 
\contentsline {paragraph}{Early History}{2}{section*.8}% 
\contentsline {paragraph}{Resurgence in the 1990s}{2}{section*.9}% 
\contentsline {paragraph}{Statistical Machine Learning}{2}{section*.10}% 
\contentsline {paragraph}{Traditional Supervised Learning Approach}{3}{section*.11}% 
\contentsline {paragraph}{Technological Breakthroughs}{3}{section*.12}% 
\contentsline {paragraph}{Deep Learning Approach}{3}{section*.13}% 
\contentsline {subparagraph}{Feature representation}{3}{section*.14}% 
\contentsline {subparagraph}{Language Model}{3}{section*.15}% 
\contentsline {subparagraph}{Dealing with Sentences}{3}{section*.16}% 
\contentsline {section}{\numberline {0.3}Language Modeling}{3}{section.0.3}% 
\contentsline {paragraph}{Probabilistic Language Model}{3}{section*.17}% 
\contentsline {paragraph}{Markov Model and N-Grams}{4}{section*.18}% 
\contentsline {paragraph}{Maximum likelihood estimate}{4}{section*.19}% 
\contentsline {paragraph}{Shannon Visualization Method}{4}{section*.20}% 
\contentsline {paragraph}{Shannon Game}{4}{section*.21}% 
\contentsline {paragraph}{Perils of Overfitting}{4}{section*.22}% 
\contentsline {subparagraph}{Smoothing}{4}{section*.23}% 
\contentsline {paragraph}{Zipf's Law}{5}{section*.24}% 
\contentsline {subsection}{\numberline {0.3.1}Evaluation and Perplexity}{5}{subsection.0.3.1}% 
\contentsline {paragraph}{Evaluation}{5}{section*.25}% 
\contentsline {paragraph}{Extrinsic Evaluation}{5}{section*.26}% 
\contentsline {paragraph}{Language Identification Task}{6}{section*.27}% 
\contentsline {paragraph}{Difficulty of Extrinsic Evaluation}{6}{section*.28}% 
\contentsline {paragraph}{Perplexity}{6}{section*.29}% 
\contentsline {section}{\numberline {0.4}Representation of Words}{6}{section.0.4}% 
\contentsline {paragraph}{Word Meaning}{6}{section*.30}% 
\contentsline {paragraph}{Linguistic Solution}{6}{section*.31}% 
\contentsline {subparagraph}{Problems with lexical resources}{6}{section*.32}% 
\contentsline {paragraph}{Vector Space Model}{6}{section*.33}% 
\contentsline {subparagraph}{One Hot Representation}{7}{section*.34}% 
\contentsline {subparagraph}{tf*idf Measure}{7}{section*.35}% 
\contentsline {subparagraph}{Classical VSM}{7}{section*.36}% 
\contentsline {paragraph}{Problems with Discrete Symbols}{7}{section*.37}% 
\contentsline {subparagraph}{Intuition}{7}{section*.38}% 
\contentsline {paragraph}{Word Vectors/Embeddings}{7}{section*.39}% 
\contentsline {paragraph}{Distributional Hypothesis}{7}{section*.40}% 
\contentsline {subparagraph}{Word Context Matrix}{7}{section*.41}% 
\contentsline {subparagraph}{Co-Occurrence Matrix}{7}{section*.42}% 
\contentsline {subsection}{\numberline {0.4.1}Word Embeddings}{7}{subsection.0.4.1}% 
\contentsline {paragraph}{Dense Representations}{7}{section*.43}% 
\contentsline {paragraph}{Collobert}{8}{section*.44}% 
\contentsline {paragraph}{Word2Vec}{8}{section*.45}% 
\contentsline {subparagraph}{Skip-Gram}{8}{section*.46}% 
\contentsline {subparagraph}{Objective Function}{9}{section*.47}% 
\contentsline {paragraph}{Softmax}{9}{section*.48}% 
\contentsline {paragraph}{Can we really capture the concept represented by a word?}{9}{section*.49}% 
\contentsline {paragraph}{Negative Sampling}{9}{section*.50}% 
\contentsline {paragraph}{CBoW}{10}{section*.51}% 
\contentsline {paragraph}{Which Embeddings}{10}{section*.52}% 
\contentsline {paragraph}{GloVe}{10}{section*.53}% 
\contentsline {paragraph}{fastText}{10}{section*.54}% 
\contentsline {paragraph}{Co-Occurrence Counts}{10}{section*.55}% 
\contentsline {paragraph}{Weighting}{10}{section*.56}% 
\contentsline {subparagraph}{Which One?}{10}{section*.57}% 
\contentsline {paragraph}{Parallel word2vec}{10}{section*.58}% 
\contentsline {paragraph}{Computing embeddings}{11}{section*.59}% 
\contentsline {paragraph}{Gensim}{11}{section*.60}% 
\contentsline {paragraph}{Fang}{11}{section*.61}% 
\contentsline {subsection}{\numberline {0.4.2}Evaluation}{11}{subsection.0.4.2}% 
\contentsline {paragraph}{Polysemy}{11}{section*.62}% 
\contentsline {paragraph}{Extrinsic Vector Evaluation}{11}{section*.63}% 
\contentsline {subsubsection}{Embeddings in Neural Networks}{11}{section*.64}% 
\contentsline {subsubsection}{Limits of Word Embeddings}{11}{section*.65}% 
\contentsline {paragraph}{Word Senses and Ambiguity}{11}{section*.66}% 
\contentsline {paragraph}{Sentiment Specific}{11}{section*.67}% 
\contentsline {paragraph}{Context Aware Word Embeddings}{11}{section*.68}% 
\contentsline {paragraph}{ELMo}{12}{section*.69}% 
\contentsline {paragraph}{OpenAI GPT-2}{12}{section*.70}% 
\contentsline {paragraph}{BERT}{12}{section*.71}% 
\contentsline {section}{\numberline {0.5}Text Classification}{12}{section.0.5}% 
\contentsline {paragraph}{Definition}{12}{section*.72}% 
\contentsline {paragraph}{Hand-Coded Rules}{13}{section*.73}% 
\contentsline {paragraph}{Supervised Machine Learning}{13}{section*.74}% 
\contentsline {subsection}{\numberline {0.5.1}Naive Bayes}{13}{subsection.0.5.1}% 
\contentsline {paragraph}{Bag of words representation}{13}{section*.75}% 
\contentsline {paragraph}{Bayes Rule}{13}{section*.76}% 
\contentsline {paragraph}{Text classification problem}{13}{section*.77}% 
\contentsline {subsubsection}{Naive Bayes Classifiers}{13}{section*.78}% 
\contentsline {paragraph}{Naive Bayes Assumption}{13}{section*.79}% 
\contentsline {subsubsection}{Multinomial Naive Bayes Text Classification}{13}{section*.80}% 
\contentsline {paragraph}{Learning the Model}{14}{section*.81}% 
\contentsline {paragraph}{Smoothing to Avoid Overfitting}{14}{section*.82}% 
\contentsline {paragraph}{Classifying}{14}{section*.83}% 
\contentsline {paragraph}{Preventing Underflow}{14}{section*.84}% 
\contentsline {paragraph}{Generate}{14}{section*.85}% 
\contentsline {paragraph}{Naive Bayes and Language Modeling}{14}{section*.86}% 
\contentsline {paragraph}{Evaluating Categorization}{14}{section*.87}% 
\contentsline {paragraph}{Micro vs Macro Averaging}{15}{section*.88}% 
\contentsline {paragraph}{Multiclass Classification}{15}{section*.89}% 
\contentsline {paragraph}{Training Size}{15}{section*.90}% 
\contentsline {paragraph}{Violation of Naive Bayes Assumptions}{15}{section*.91}% 
\contentsline {paragraph}{Example: SpamAssassin}{15}{section*.92}% 
\contentsline {section}{\numberline {0.6}\IeC {\textbullet }}{15}{section.0.6}% 
\contentsline {paragraph}{Regular Expressions}{15}{section*.93}% 
\contentsline {paragraph}{Tokenization}{15}{section*.94}% 
\contentsline {subparagraph}{What's a Word?}{15}{section*.95}% 
