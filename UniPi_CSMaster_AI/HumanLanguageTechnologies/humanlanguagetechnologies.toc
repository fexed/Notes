\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Introduction}{2}{section.0.1}% 
\contentsline {paragraph}{What will we learn}{2}{section*.2}% 
\contentsline {paragraph}{Books}{2}{section*.3}% 
\contentsline {paragraph}{Exam}{2}{section*.4}% 
\contentsline {paragraph}{Experimental Approach}{2}{section*.5}% 
\contentsline {paragraph}{Motivations}{2}{section*.6}% 
\contentsline {paragraph}{Structured vs unstructured data}{2}{section*.7}% 
\contentsline {section}{\numberline {0.2}State of the Art}{2}{section.0.2}% 
\contentsline {paragraph}{Early History}{2}{section*.8}% 
\contentsline {paragraph}{Resurgence in the 1990s}{2}{section*.9}% 
\contentsline {paragraph}{Statistical Machine Learning}{2}{section*.10}% 
\contentsline {paragraph}{Traditional Supervised Learning Approach}{3}{section*.11}% 
\contentsline {paragraph}{Technological Breakthroughs}{3}{section*.12}% 
\contentsline {paragraph}{Deep Learning Approach}{3}{section*.13}% 
\contentsline {subparagraph}{Feature representation}{3}{section*.14}% 
\contentsline {subparagraph}{Language Model}{3}{section*.15}% 
\contentsline {subparagraph}{Dealing with Sentences}{3}{section*.16}% 
\contentsline {section}{\numberline {0.3}Language Modeling}{3}{section.0.3}% 
\contentsline {paragraph}{Probabilistic Language Model}{3}{section*.17}% 
\contentsline {paragraph}{Markov Model and N-Grams}{4}{section*.18}% 
\contentsline {paragraph}{Maximum likelihood estimate}{4}{section*.19}% 
\contentsline {paragraph}{Shannon Visualization Method}{4}{section*.20}% 
\contentsline {paragraph}{Shannon Game}{4}{section*.21}% 
\contentsline {paragraph}{Perils of Overfitting}{4}{section*.22}% 
\contentsline {subparagraph}{Smoothing}{4}{section*.23}% 
\contentsline {paragraph}{Zipf's Law}{5}{section*.24}% 
\contentsline {subsection}{\numberline {0.3.1}Evaluation and Perplexity}{5}{subsection.0.3.1}% 
\contentsline {paragraph}{Evaluation}{5}{section*.25}% 
\contentsline {paragraph}{Extrinsic Evaluation}{5}{section*.26}% 
\contentsline {paragraph}{Language Identification Task}{6}{section*.27}% 
\contentsline {paragraph}{Difficulty of Extrinsic Evaluation}{6}{section*.28}% 
\contentsline {paragraph}{Perplexity}{6}{section*.29}% 
\contentsline {section}{\numberline {0.4}Representation of Words}{6}{section.0.4}% 
\contentsline {paragraph}{Word Meaning}{6}{section*.30}% 
\contentsline {paragraph}{Linguistic Solution}{6}{section*.31}% 
\contentsline {subparagraph}{Problems with lexical resources}{6}{section*.32}% 
\contentsline {paragraph}{Vector Space Model}{6}{section*.33}% 
\contentsline {subparagraph}{One Hot Representation}{7}{section*.34}% 
\contentsline {subparagraph}{tf*idf Measure}{7}{section*.35}% 
\contentsline {subparagraph}{Classical VSM}{7}{section*.36}% 
\contentsline {paragraph}{Problems with Discrete Symbols}{7}{section*.37}% 
\contentsline {subparagraph}{Intuition}{7}{section*.38}% 
\contentsline {paragraph}{Word Vectors/Embeddings}{7}{section*.39}% 
\contentsline {paragraph}{Distributional Hypothesis}{7}{section*.40}% 
\contentsline {subparagraph}{Word Context Matrix}{7}{section*.41}% 
\contentsline {subparagraph}{Co-Occurrence Matrix}{7}{section*.42}% 
\contentsline {subsection}{\numberline {0.4.1}Word Embeddings}{7}{subsection.0.4.1}% 
\contentsline {paragraph}{Dense Representations}{7}{section*.43}% 
\contentsline {paragraph}{Collobert}{8}{section*.44}% 
\contentsline {paragraph}{Word2Vec}{8}{section*.45}% 
\contentsline {subparagraph}{Skip-Gram}{8}{section*.46}% 
\contentsline {subparagraph}{Objective Function}{9}{section*.47}% 
\contentsline {paragraph}{Softmax}{9}{section*.48}% 
\contentsline {paragraph}{Can we really capture the concept represented by a word?}{9}{section*.49}% 
\contentsline {paragraph}{Negative Sampling}{10}{section*.50}% 
\contentsline {paragraph}{CBoW}{10}{section*.51}% 
\contentsline {paragraph}{Which Embeddings}{10}{section*.52}% 
\contentsline {paragraph}{GloVe}{10}{section*.53}% 
\contentsline {paragraph}{fastText}{10}{section*.54}% 
\contentsline {paragraph}{Co-Occurrence Counts}{10}{section*.55}% 
\contentsline {paragraph}{Weighting}{10}{section*.56}% 
\contentsline {subparagraph}{Which One?}{11}{section*.57}% 
\contentsline {paragraph}{Parallel word2vec}{11}{section*.58}% 
\contentsline {paragraph}{Computing embeddings}{11}{section*.59}% 
\contentsline {paragraph}{Gensim}{11}{section*.60}% 
\contentsline {paragraph}{Fang}{11}{section*.61}% 
\contentsline {subsection}{\numberline {0.4.2}Evaluation}{11}{subsection.0.4.2}% 
\contentsline {paragraph}{Polysemy}{11}{section*.62}% 
\contentsline {paragraph}{Extrinsic Vector Evaluation}{11}{section*.63}% 
\contentsline {subsubsection}{Embeddings in Neural Networks}{11}{section*.64}% 
\contentsline {subsubsection}{Limits of Word Embeddings}{11}{section*.65}% 
\contentsline {paragraph}{Word Senses and Ambiguity}{12}{section*.66}% 
\contentsline {paragraph}{Sentiment Specific}{12}{section*.67}% 
\contentsline {paragraph}{Context Aware Word Embeddings}{12}{section*.68}% 
\contentsline {paragraph}{ELMo}{12}{section*.69}% 
\contentsline {paragraph}{BERT}{12}{section*.70}% 
\contentsline {section}{\numberline {0.5}Text Classification}{12}{section.0.5}% 
\contentsline {paragraph}{Definition}{12}{section*.71}% 
\contentsline {paragraph}{Hand-Coded Rules}{13}{section*.72}% 
\contentsline {paragraph}{Supervised Machine Learning}{13}{section*.73}% 
\contentsline {subsection}{\numberline {0.5.1}Naive Bayes}{13}{subsection.0.5.1}% 
\contentsline {paragraph}{Bag of words representation}{13}{section*.74}% 
\contentsline {paragraph}{Bayes Rule}{13}{section*.75}% 
\contentsline {paragraph}{Text classification problem}{13}{section*.76}% 
\contentsline {subsubsection}{Naive Bayes Classifiers}{13}{section*.77}% 
\contentsline {paragraph}{Naive Bayes Assumption}{14}{section*.78}% 
\contentsline {subsubsection}{Multinomial Naive Bayes Text Classification}{14}{section*.79}% 
\contentsline {paragraph}{Learning the Model}{14}{section*.80}% 
\contentsline {paragraph}{Smoothing to Avoid Overfitting}{14}{section*.81}% 
\contentsline {paragraph}{Classifying}{14}{section*.82}% 
\contentsline {paragraph}{Preventing Underflow}{14}{section*.83}% 
\contentsline {paragraph}{Generate}{14}{section*.84}% 
\contentsline {paragraph}{Naive Bayes and Language Modeling}{15}{section*.85}% 
\contentsline {paragraph}{Evaluating Categorization}{15}{section*.86}% 
\contentsline {paragraph}{Micro vs Macro Averaging}{15}{section*.87}% 
\contentsline {paragraph}{Multiclass Classification}{15}{section*.88}% 
\contentsline {paragraph}{Training Size}{15}{section*.89}% 
\contentsline {paragraph}{Violation of Naive Bayes Assumptions}{15}{section*.90}% 
\contentsline {paragraph}{Example: SpamAssassin}{15}{section*.91}% 
\contentsline {section}{\numberline {0.6}\IeC {\textbullet }}{15}{section.0.6}% 
\contentsline {paragraph}{Regular Expressions}{15}{section*.92}% 
\contentsline {paragraph}{Tokenization}{15}{section*.93}% 
\contentsline {subparagraph}{What's a Word?}{16}{section*.94}% 
\contentsline {paragraph}{Stanza Tokenizer}{16}{section*.95}% 
\contentsline {paragraph}{Clitics}{16}{section*.96}% 
\contentsline {section}{\numberline {0.7}Classification}{16}{section.0.7}% 
\contentsline {paragraph}{Naive Bayes}{16}{section*.97}% 
\contentsline {paragraph}{Decision Trees}{16}{section*.98}% 
\contentsline {paragraph}{Linear vs non-linear algorithms}{16}{section*.99}% 
\contentsline {subsection}{\numberline {0.7.1}Linear Binary Classification}{17}{subsection.0.7.1}% 
\contentsline {paragraph}{Perceptron}{17}{section*.100}% 
\contentsline {subsection}{\numberline {0.7.2}Hidden Markov Models}{17}{subsection.0.7.2}% 
\contentsline {paragraph}{Markov Chain}{17}{section*.101}% 
\contentsline {paragraph}{Hidden Markov Model}{17}{section*.102}% 
\contentsline {subparagraph}{Example: speech}{17}{section*.103}% 
\contentsline {paragraph}{Markov Assumption}{18}{section*.104}% 
\contentsline {paragraph}{Output-independence assumption}{18}{section*.105}% 
\contentsline {paragraph}{Three basic problems}{18}{section*.106}% 
\contentsline {paragraph}{Computing the likelihood}{18}{section*.107}% 
\contentsline {subparagraph}{Forward Algorithm}{18}{section*.108}% 
\contentsline {paragraph}{Decoding}{19}{section*.109}% 
\contentsline {paragraph}{Viterbi Algorithm}{19}{section*.110}% 
\contentsline {paragraph}{Training a HMM}{20}{section*.111}% 
\contentsline {subsubsection}{Part of Speech Tagging}{20}{section*.112}% 
\contentsline {paragraph}{Two kinds of probabilities}{20}{section*.113}% 
\contentsline {subsubsection}{Sequence Tagging}{20}{section*.114}% 
\contentsline {paragraph}{Discriminative Model}{20}{section*.115}% 
\contentsline {paragraph}{Generative Model}{20}{section*.116}% 
\contentsline {paragraph}{Naive Bayes}{21}{section*.117}% 
\contentsline {paragraph}{Logistic Regression}{21}{section*.118}% 
\contentsline {paragraph}{Problems}{21}{section*.119}% 
\contentsline {paragraph}{MEMM}{21}{section*.120}% 
\contentsline {subsubsection}{Named Entity Tagging}{22}{section*.121}% 
\contentsline {paragraph}{Approaches}{23}{section*.122}% 
\contentsline {section}{\numberline {0.8}Convolutional Neural Networks for NLP}{23}{section.0.8}% 
\contentsline {paragraph}{Distant Supervision}{24}{section*.123}% 
\contentsline {paragraph}{Sentiment Specific Word Embeddings}{24}{section*.124}% 
\contentsline {paragraph}{Sentiment Classification from a Single Neuron}{24}{section*.125}% 
\contentsline {subsection}{\numberline {0.8.1}Regularization}{24}{subsection.0.8.1}% 
\contentsline {section}{\numberline {0.9}Recurrent Neural Networks}{25}{section.0.9}% 
\contentsline {paragraph}{Recap}{25}{section*.126}% 
\contentsline {paragraph}{Recurrent}{25}{section*.127}% 
\contentsline {paragraph}{Hidden Units}{25}{section*.128}% 
\contentsline {paragraph}{Advantages}{25}{section*.129}% 
\contentsline {paragraph}{Disadvantages}{25}{section*.130}% 
\contentsline {paragraph}{Simple RNN Language Model}{25}{section*.131}% 
\contentsline {paragraph}{Vanilla RNN}{26}{section*.132}% 
\contentsline {paragraph}{Training}{26}{section*.133}% 
\contentsline {paragraph}{Backpropagation through time}{26}{section*.134}% 
\contentsline {paragraph}{Training RNN Language Model}{27}{section*.135}% 
\contentsline {paragraph}{Vanishing Gradients}{27}{section*.136}% 
\contentsline {paragraph}{Exploding Gradients}{27}{section*.137}% 
\contentsline {subsection}{\numberline {0.9.1}Specializations}{27}{subsection.0.9.1}% 
\contentsline {paragraph}{Notation}{27}{section*.138}% 
\contentsline {subsubsection}{LSTM}{28}{section*.139}% 
\contentsline {paragraph}{Long Short-Term Memory}{28}{section*.140}% 
\contentsline {subsubsection}{GRU}{29}{section*.141}% 
\contentsline {paragraph}{Gated Recurrent Units}{29}{section*.142}% 
\contentsline {section}{\numberline {0.10}Parsing}{30}{section.0.10}% 
\contentsline {paragraph}{Dealing with Text}{30}{section*.143}% 
\contentsline {paragraph}{Sentence Structure}{30}{section*.144}% 
\contentsline {paragraph}{Practical uses of parsing}{30}{section*.145}% 
\contentsline {subsection}{\numberline {0.10.1}Parsing Approaches}{31}{subsection.0.10.1}% 
\contentsline {subsubsection}{Constituency Grammar}{31}{section*.146}% 
\contentsline {paragraph}{Context Free Grammars}{31}{section*.147}% 
\contentsline {paragraph}{Constituency Parsing}{31}{section*.148}% 
\contentsline {subparagraph}{Statistical Parsing}{31}{section*.149}% 
\contentsline {subsubsection}{Dependency Grammar}{31}{section*.150}% 
\contentsline {paragraph}{Dependency Structure}{31}{section*.151}% 
\contentsline {paragraph}{Difference Between Constituency Tree and Dependency Trees}{32}{section*.152}% 
\contentsline {paragraph}{Annotation Constraints}{32}{section*.153}% 
\contentsline {paragraph}{Data-Driven Dependency Parsing}{32}{section*.154}% 
\contentsline {subparagraph}{Transition-Based Shift-Reduce Parsing}{32}{section*.155}% 
\contentsline {paragraph}{Parsing as Classification}{32}{section*.156}% 
\contentsline {paragraph}{Dependency Graph}{33}{section*.157}% 
\contentsline {paragraph}{Arc Standard Transitions}{33}{section*.158}% 
\contentsline {paragraph}{Parser Algorithm}{33}{section*.159}% 
\contentsline {paragraph}{Oracle}{33}{section*.160}% 
\contentsline {paragraph}{Projectivity}{33}{section*.161}% 
\contentsline {paragraph}{Arc-Standard Algorithm}{33}{section*.162}% 
\contentsline {paragraph}{Arc Eager Transitions}{34}{section*.163}% 
\contentsline {paragraph}{Non-Projective Transitions}{34}{section*.164}% 
\contentsline {paragraph}{Learning Procedure}{34}{section*.165}% 
\contentsline {paragraph}{Dependency Shift-Reduce Parsers}{34}{section*.166}% 
\contentsline {paragraph}{CoNLL-X Shared Task}{34}{section*.167}% 
\contentsline {paragraph}{Problems with Oracles}{34}{section*.168}% 
\contentsline {subsubsection}{Graph-Based Parsing}{34}{section*.169}% 
\contentsline {paragraph}{NN Graph-Based Parser}{35}{section*.170}% 
\contentsline {subparagraph}{Parser}{35}{section*.171}% 
\contentsline {subparagraph}{Dependency Relations}{35}{section*.172}% 
\contentsline {subparagraph}{Self-Attention}{35}{section*.173}% 
\contentsline {subparagraph}{Classifier for Labels}{35}{section*.174}% 
\contentsline {section}{\numberline {0.11}Universal Dependencies}{36}{section.0.11}% 
\contentsline {paragraph}{Goal}{36}{section*.175}% 
\contentsline {paragraph}{Guiding Principles}{36}{section*.176}% 
\contentsline {paragraph}{Design Principles}{36}{section*.177}% 
\contentsline {paragraph}{Morphological Annotation}{36}{section*.178}% 
\contentsline {paragraph}{Syntactic Annotation}{36}{section*.179}% 
\contentsline {paragraph}{Coordination}{37}{section*.180}% 
\contentsline {paragraph}{Enhanced Dependencies}{37}{section*.181}% 
\contentsline {paragraph}{Basic}{37}{section*.182}% 
\contentsline {paragraph}{Enhanced}{37}{section*.183}% 
\contentsline {paragraph}{Dependency Structure}{38}{section*.184}% 
\contentsline {paragraph}{Parsing}{38}{section*.185}% 
\contentsline {section}{\numberline {0.12}Machine Translation}{38}{section.0.12}% 
\contentsline {paragraph}{Issues}{38}{section*.186}% 
\contentsline {paragraph}{Alignment}{38}{section*.187}% 
\contentsline {paragraph}{MT Already Good for...}{38}{section*.188}% 
\contentsline {paragraph}{MT Not Yet Good Enough for...}{38}{section*.189}% 
\contentsline {subsection}{\numberline {0.12.1}Language Similarities and Divergences}{39}{subsection.0.12.1}% 
\contentsline {paragraph}{Typology}{39}{section*.190}% 
\contentsline {paragraph}{Morphology}{39}{section*.191}% 
\contentsline {paragraph}{Morphological Variation}{39}{section*.192}% 
\contentsline {paragraph}{Segmentation Variation}{39}{section*.193}% 
\contentsline {paragraph}{Lexical Gaps}{39}{section*.194}% 
\contentsline {paragraph}{Event-To-Argument Divergences}{39}{section*.195}% 
\contentsline {subsection}{\numberline {0.12.2}Classical Techniques}{39}{subsection.0.12.2}% 
\contentsline {paragraph}{Direct Translation}{40}{section*.196}% 
\contentsline {subparagraph}{Pros}{40}{section*.197}% 
\contentsline {subparagraph}{Cons}{40}{section*.198}% 
\contentsline {paragraph}{Transfer Model}{40}{section*.199}% 
\contentsline {subparagraph}{Lexical Transfer}{40}{section*.200}% 
\contentsline {subparagraph}{Systram}{40}{section*.201}% 
\contentsline {paragraph}{Interlingua}{41}{section*.202}% 
\contentsline {subparagraph}{Pros}{41}{section*.203}% 
\contentsline {subparagraph}{Cons}{41}{section*.204}% 
\contentsline {subsection}{\numberline {0.12.3}Statistical Machine Translation}{41}{subsection.0.12.3}% 
\contentsline {paragraph}{Example}{41}{section*.205}% 
\contentsline {paragraph}{What Makes a Good Translation}{41}{section*.206}% 
\contentsline {paragraph}{Fluency $P(T)$}{42}{section*.207}% 
\contentsline {paragraph}{Faithfulness $P(S\mskip \medmuskip |\mskip \medmuskip T)$}{42}{section*.208}% 
\contentsline {subparagraph}{Sentence Alignment}{42}{section*.209}% 
\contentsline {subparagraph}{Word Alignment}{42}{section*.210}% 
\contentsline {paragraph}{Three Problems for Statistical Machine Translation}{42}{section*.211}% 
\contentsline {subsection}{\numberline {0.12.4}Phrase Based Machine Translation}{42}{subsection.0.12.4}% 
\contentsline {paragraph}{Translation Probabilities}{43}{section*.212}% 
\contentsline {paragraph}{Distortion probability}{43}{section*.213}% 
\contentsline {paragraph}{Training $P(F\mskip \medmuskip |\mskip \medmuskip E)$}{43}{section*.214}% 
\contentsline {paragraph}{Computing Word Alignments}{44}{section*.215}% 
\contentsline {paragraph}{Training Alignment Probabilities}{44}{section*.216}% 
\contentsline {subsubsection}{Phrase-Based Translation Model}{44}{section*.218}% 
\contentsline {paragraph}{Phrase Alignment}{45}{section*.219}% 
\contentsline {paragraph}{Decoding}{45}{section*.220}% 
\contentsline {paragraph}{Evaluation}{45}{section*.221}% 
\contentsline {subparagraph}{Scores}{45}{section*.222}% 
\contentsline {subsection}{\numberline {0.12.5}Syntax Based Statistical Machine Translation}{45}{subsection.0.12.5}% 
\contentsline {paragraph}{Synchronous Grammar}{45}{section*.223}% 
\contentsline {paragraph}{Synchronous Derivations and Translation Models}{46}{section*.224}% 
\contentsline {paragraph}{Use of Dependency Parsing}{46}{section*.225}% 
\contentsline {subsection}{\numberline {0.12.6}Minimum Error Rate Training}{46}{subsection.0.12.6}% 
\contentsline {paragraph}{Conclusions}{46}{section*.226}% 
\contentsline {section}{\numberline {0.13}Neural Machine Translation}{46}{section.0.13}% 
\contentsline {paragraph}{NMT}{46}{section*.227}% 
\contentsline {paragraph}{Beam Search}{47}{section*.228}% 
\contentsline {subparagraph}{Example}{47}{section*.229}% 
\contentsline {subparagraph}{Stopping Criterion}{48}{section*.230}% 
\contentsline {subparagraph}{Finishing Up}{48}{section*.231}% 
\contentsline {subparagraph}{Benefits}{48}{section*.232}% 
\contentsline {subparagraph}{Disadvantages}{48}{section*.233}% 
\contentsline {paragraph}{Attention}{48}{section*.234}% 
\contentsline {subparagraph}{In equations}{49}{section*.235}% 
\contentsline {subparagraph}{Attention Variants}{49}{section*.236}% 
\contentsline {subsection}{\numberline {0.13.1}Self-Attention}{49}{subsection.0.13.1}% 
\contentsline {paragraph}{Attention}{50}{section*.237}% 
\contentsline {paragraph}{Issues with Recurrent Models}{50}{section*.238}% 
\contentsline {paragraph}{Word Windows}{50}{section*.239}% 
\contentsline {paragraph}{Attention}{50}{section*.240}% 
\contentsline {paragraph}{Self-Attention}{50}{section*.241}% 
\contentsline {paragraph}{Vector Notation}{51}{section*.242}% 
\contentsline {paragraph}{Self-Attention as a NLP Building Block}{51}{section*.243}% 
\contentsline {subparagraph}{No Notion of Order}{51}{section*.244}% 
\contentsline {subparagraph}{Adding Non-Linearities}{52}{section*.245}% 
\contentsline {subparagraph}{Future}{52}{section*.246}% 
\contentsline {subsection}{\numberline {0.13.2}Transformers}{52}{subsection.0.13.2}% 
\contentsline {paragraph}{Self-Attention}{52}{section*.247}% 
\contentsline {paragraph}{Multi-Headed Attention}{53}{section*.248}% 
\contentsline {paragraph}{Training Tricks}{53}{section*.249}% 
\contentsline {subsubsection}{Transformers Library}{53}{section*.250}% 
\contentsline {paragraph}{Hugging Face Transformers}{53}{section*.251}% 
\contentsline {subsubsection}{Transformers Architectures}{54}{section*.252}% 
\contentsline {paragraph}{Pretraining Transformers}{54}{section*.253}% 
\contentsline {subparagraph}{Pretraining through language modeling}{54}{section*.254}% 
\contentsline {paragraph}{Pretraining-Finetuning Paradigm}{54}{section*.255}% 
\contentsline {subparagraph}{SGD}{54}{section*.256}% 
\contentsline {paragraph}{Pretraining for Three Types of Architectures}{54}{section*.257}% 
\contentsline {subparagraph}{Pretraining Decoders}{55}{section*.258}% 
\contentsline {paragraph}{Pretraining Encoders}{55}{section*.259}% 
\contentsline {paragraph}{Problems with previous methods}{55}{section*.260}% 
\contentsline {subparagraph}{BERT}{55}{section*.261}% 
\contentsline {paragraph}{Masked LM}{56}{section*.262}% 
\contentsline {paragraph}{Wordpiece}{56}{section*.263}% 
\contentsline {paragraph}{Pretraining Encoder-Decoders}{56}{section*.264}% 
\contentsline {paragraph}{What pretraining objective to use?}{56}{section*.265}% 
\contentsline {paragraph}{Pre-training Tasks}{56}{section*.266}% 
\contentsline {paragraph}{Masked LM}{56}{section*.267}% 
\contentsline {paragraph}{Next Sentence Prediction}{56}{section*.268}% 
\contentsline {section}{\numberline {0.14}Analysis of Language Models}{57}{section.0.14}% 
\contentsline {paragraph}{Questions About Language Models}{57}{section*.269}% 
\contentsline {paragraph}{What Linguistic Knowledge is Present in LM?}{57}{section*.270}% 
\contentsline {subparagraph}{Unsupervised NER}{57}{section*.271}% 
\contentsline {subparagraph}{LM Effectivness}{57}{section*.272}% 
\contentsline {subparagraph}{LM as Linguistic Test Subjects}{57}{section*.273}% 
\contentsline {subparagraph}{Prediction Explanations}{58}{section*.274}% 
\contentsline {subsection}{\numberline {0.14.1}Probes}{58}{subsection.0.14.1}% 
\contentsline {paragraph}{Probing}{58}{section*.275}% 
\contentsline {paragraph}{Contextual Representation of Language}{59}{section*.276}% 
\contentsline {paragraph}{Structural Probe}{59}{section*.277}% 
\contentsline {subparagraph}{Syntax Distance Hypothesis}{59}{section*.278}% 
\contentsline {subparagraph}{Finding a Parse Tree Encoding Distance Metric}{59}{section*.279}% 
\contentsline {subparagraph}{Finding $B$}{59}{section*.280}% 
\contentsline {paragraph}{Conclusion}{59}{section*.281}% 
\contentsline {section}{\numberline {0.15}Prompt-Based Learning}{60}{section.0.15}% 
\contentsline {paragraph}{Finetuning}{60}{section*.282}% 
\contentsline {paragraph}{Zero-shot}{60}{section*.283}% 
\contentsline {paragraph}{One-Shot}{60}{section*.284}% 
\contentsline {paragraph}{Few-Shot}{60}{section*.285}% 
\contentsline {subsection}{\numberline {0.15.1}Prompts}{60}{subsection.0.15.1}% 
\contentsline {paragraph}{Prefix Tuning}{61}{section*.286}% 
\contentsline {paragraph}{Prompt Tuning}{61}{section*.287}% 
\contentsline {paragraph}{Prompt-Tuning Ensembles}{63}{section*.288}% 
\contentsline {paragraph}{Interpretability}{63}{section*.289}% 
\contentsline {subsection}{\numberline {0.15.2}Soft Prompt Transfer}{63}{subsection.0.15.2}% 
