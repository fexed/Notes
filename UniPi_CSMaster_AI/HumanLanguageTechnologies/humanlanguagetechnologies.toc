\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {1}Introduction}{3}{section.0.1}%
\contentsline {paragraph}{What will we learn}{3}{section*.2}%
\contentsline {paragraph}{Books}{3}{section*.3}%
\contentsline {paragraph}{Exam}{3}{section*.4}%
\contentsline {paragraph}{Experimental Approach}{3}{section*.5}%
\contentsline {paragraph}{Motivations}{3}{section*.6}%
\contentsline {paragraph}{Structured vs unstructured data}{3}{section*.7}%
\contentsline {section}{\numberline {2}State of the Art}{3}{section.0.2}%
\contentsline {paragraph}{Early History}{3}{section*.8}%
\contentsline {paragraph}{Resurgence in the 1990s}{3}{section*.9}%
\contentsline {paragraph}{Statistical Machine Learning}{4}{section*.10}%
\contentsline {paragraph}{Traditional Supervised Learning Approach}{4}{section*.11}%
\contentsline {paragraph}{Technological Breakthroughs}{4}{section*.12}%
\contentsline {paragraph}{Deep Learning (DL) Approach}{4}{section*.13}%
\contentsline {subparagraph}{Feature representation}{4}{section*.14}%
\contentsline {subparagraph}{Language Model}{4}{section*.15}%
\contentsline {subparagraph}{Dealing with sentences}{4}{section*.16}%
\contentsline {section}{\numberline {3}Language Modeling}{5}{section.0.3}%
\contentsline {paragraph}{Probabilistic Language Model}{5}{section*.17}%
\contentsline {paragraph}{Markov Model and N-Grams}{5}{section*.18}%
\contentsline {paragraph}{Maximum likelihood estimate (MLE)}{5}{section*.19}%
\contentsline {paragraph}{Shannon Visualization Method}{5}{section*.20}%
\contentsline {paragraph}{Shannon Game}{6}{section*.21}%
\contentsline {paragraph}{Perils of Overfitting}{6}{section*.22}%
\contentsline {subparagraph}{Smoothing}{6}{section*.23}%
\contentsline {paragraph}{Zipf's Law}{6}{section*.24}%
\contentsline {subsection}{\numberline {3.1}Evaluation and Perplexity}{7}{subsection.0.3.1}%
\contentsline {paragraph}{Evaluation}{7}{section*.25}%
\contentsline {paragraph}{Extrinsic Evaluation}{7}{section*.26}%
\contentsline {paragraph}{Language Identification Task}{7}{section*.27}%
\contentsline {paragraph}{Difficulty of Extrinsic Evaluation}{7}{section*.28}%
\contentsline {paragraph}{Perplexity}{7}{section*.29}%
\contentsline {section}{\numberline {4}Representation of Words}{8}{section.0.4}%
\contentsline {paragraph}{Word Meaning}{8}{section*.30}%
\contentsline {paragraph}{Linguistic Solution}{8}{section*.31}%
\contentsline {paragraph}{Vector Space Model}{8}{section*.32}%
\contentsline {subparagraph}{One Hot Representation}{8}{section*.33}%
\contentsline {subparagraph}{tf*idf Measure}{8}{section*.34}%
\contentsline {subparagraph}{Classical VSM}{8}{section*.35}%
\contentsline {paragraph}{Problems with Discrete Symbols}{8}{section*.36}%
\contentsline {subparagraph}{Intuition}{8}{section*.37}%
\contentsline {paragraph}{Word Vectors/Embeddings}{9}{section*.38}%
\contentsline {paragraph}{Distributional Hypothesis}{9}{section*.39}%
\contentsline {subparagraph}{Word Context Matrix}{9}{section*.40}%
\contentsline {subparagraph}{Co-Occurrence Matrix}{9}{section*.41}%
\contentsline {subsection}{\numberline {4.1}Word Embeddings}{9}{subsection.0.4.1}%
\contentsline {paragraph}{Dense Representations}{9}{section*.42}%
\contentsline {paragraph}{Collobert}{9}{section*.43}%
\contentsline {paragraph}{Word2Vec}{9}{section*.44}%
\contentsline {subparagraph}{CBoW}{10}{section*.45}%
\contentsline {subparagraph}{Skip-Gram}{10}{section*.46}%
\contentsline {subparagraph}{Objective Function}{11}{section*.47}%
\contentsline {paragraph}{Softmax}{11}{section*.48}%
\contentsline {paragraph}{Can we really capture the concept represented by a word?}{11}{section*.49}%
\contentsline {paragraph}{Negative Sampling}{11}{section*.50}%
\contentsline {paragraph}{CBoW}{11}{section*.51}%
\contentsline {paragraph}{Which Embeddings}{11}{section*.52}%
\contentsline {paragraph}{GloVe}{11}{section*.53}%
\contentsline {paragraph}{fastText}{12}{section*.54}%
\contentsline {paragraph}{Co-Occurrence Counts}{12}{section*.55}%
\contentsline {paragraph}{Weighting}{12}{section*.56}%
\contentsline {subparagraph}{Which One?}{12}{section*.57}%
\contentsline {paragraph}{Parallel word2vec}{12}{section*.58}%
\contentsline {paragraph}{Computing embeddings}{12}{section*.59}%
\contentsline {paragraph}{Gensim}{12}{section*.60}%
\contentsline {paragraph}{Fang}{12}{section*.61}%
\contentsline {subsection}{\numberline {4.2}Evaluation}{12}{subsection.0.4.2}%
\contentsline {paragraph}{Polysemy}{12}{section*.62}%
\contentsline {paragraph}{Extrinsic Vector Evaluation}{13}{section*.63}%
\contentsline {subsubsection}{Embeddings in Neural Networks}{13}{section*.64}%
\contentsline {subsubsection}{Limits of Word Embeddings}{13}{section*.65}%
\contentsline {paragraph}{Word Senses and Ambiguity}{13}{section*.66}%
\contentsline {paragraph}{Sentiment Specific}{13}{section*.67}%
\contentsline {paragraph}{Context Aware Word Embeddings}{13}{section*.68}%
\contentsline {paragraph}{ELMo}{13}{section*.69}%
\contentsline {paragraph}{BERT}{14}{section*.70}%
\contentsline {section}{\numberline {5}Text Classification}{14}{section.0.5}%
\contentsline {paragraph}{Definition}{14}{section*.71}%
\contentsline {paragraph}{Hand-Coded Rules}{14}{section*.72}%
\contentsline {paragraph}{Supervised Machine Learning}{15}{section*.73}%
\contentsline {subsection}{\numberline {5.1}Naive Bayes}{15}{subsection.0.5.1}%
\contentsline {paragraph}{Bag of words representation}{15}{section*.74}%
\contentsline {paragraph}{Bayes Rule}{15}{section*.75}%
\contentsline {paragraph}{Text classification problem}{15}{section*.76}%
\contentsline {subsubsection}{Naive Bayes Classifiers}{15}{section*.77}%
\contentsline {paragraph}{Naive Bayes Assumption}{15}{section*.78}%
\contentsline {subsubsection}{Multinomial Naive Bayes Text Classification}{15}{section*.79}%
\contentsline {paragraph}{Learning the Model}{16}{section*.80}%
\contentsline {paragraph}{Smoothing to Avoid Overfitting}{16}{section*.81}%
\contentsline {paragraph}{Classifying}{16}{section*.82}%
\contentsline {paragraph}{Preventing Underflow}{16}{section*.83}%
\contentsline {paragraph}{Generate}{16}{section*.84}%
\contentsline {paragraph}{Naive Bayes and Language Modeling}{16}{section*.85}%
\contentsline {paragraph}{Evaluating Categorization}{16}{section*.86}%
\contentsline {paragraph}{Micro vs Macro Averaging}{17}{section*.87}%
\contentsline {paragraph}{Multiclass Classification}{17}{section*.88}%
\contentsline {paragraph}{Training Size}{17}{section*.89}%
\contentsline {paragraph}{Violation of Naive Bayes Assumptions}{17}{section*.90}%
\contentsline {paragraph}{Example: SpamAssassin}{17}{section*.91}%
\contentsline {section}{\numberline {6}Tokenization}{17}{section.0.6}%
\contentsline {paragraph}{Regular Expressions}{17}{section*.92}%
\contentsline {paragraph}{Tokenization}{17}{section*.93}%
\contentsline {subparagraph}{What's a Word?}{17}{section*.94}%
\contentsline {paragraph}{Stanza Tokenizer}{18}{section*.95}%
\contentsline {paragraph}{Clitics}{18}{section*.96}%
\contentsline {paragraph}{Morphology and Stemming}{18}{section*.97}%
\contentsline {paragraph}{Normalization}{18}{section*.98}%
\contentsline {section}{\numberline {7}Classification}{18}{section.0.7}%
\contentsline {subsection}{\numberline {7.1}Common Models}{18}{subsection.0.7.1}%
\contentsline {paragraph}{Naive Bayes}{18}{section*.99}%
\contentsline {paragraph}{Decision Trees}{18}{section*.100}%
\contentsline {paragraph}{Linear vs non-linear algorithms}{19}{section*.101}%
\contentsline {subsubsection}{Linear Binary Classification}{19}{section*.102}%
\contentsline {paragraph}{Perceptron}{19}{section*.103}%
\contentsline {subparagraph}{Multi-Layer Perceptron}{19}{section*.104}%
\contentsline {paragraph}{Universal Approximation Theorem}{19}{section*.105}%
\contentsline {subsubsection}{Hidden Markov Models}{19}{section*.106}%
\contentsline {paragraph}{Markov Chain}{19}{section*.107}%
\contentsline {paragraph}{Hidden Markov Model}{20}{section*.108}%
\contentsline {subparagraph}{Example: speech}{20}{section*.109}%
\contentsline {paragraph}{Markov Assumption}{20}{section*.110}%
\contentsline {paragraph}{Output-independence assumption}{20}{section*.111}%
\contentsline {paragraph}{Three basic problems}{20}{section*.112}%
\contentsline {paragraph}{Computing the likelihood}{20}{section*.113}%
\contentsline {subparagraph}{Forward Algorithm}{21}{section*.114}%
\contentsline {paragraph}{Decoding}{21}{section*.115}%
\contentsline {paragraph}{Viterbi Algorithm}{21}{section*.116}%
\contentsline {paragraph}{Training a HMM}{22}{section*.117}%
\contentsline {subsection}{\numberline {7.2}Part of Speech Tagging}{22}{subsection.0.7.2}%
\contentsline {paragraph}{Two kinds of probabilities}{22}{section*.118}%
\contentsline {subsection}{\numberline {7.3}Sequence Tagging}{23}{subsection.0.7.3}%
\contentsline {paragraph}{Discriminative Model}{23}{section*.119}%
\contentsline {paragraph}{Logistic Regression}{23}{section*.120}%
\contentsline {paragraph}{Generative Model}{23}{section*.121}%
\contentsline {paragraph}{Naive Bayes}{23}{section*.122}%
\contentsline {paragraph}{Problems}{23}{section*.123}%
\contentsline {paragraph}{MEMM}{23}{section*.124}%
\contentsline {subsection}{\numberline {7.4}Named Entity Tagging}{25}{subsection.0.7.4}%
\contentsline {paragraph}{Approaches}{25}{section*.125}%
\contentsline {section}{\numberline {8}Convolutional Neural Networks for NLP}{26}{section.0.8}%
\contentsline {paragraph}{Distant Supervision}{26}{section*.126}%
\contentsline {paragraph}{Sentiment Specific Word Embeddings}{26}{section*.127}%
\contentsline {paragraph}{Ensemble of Classifiers}{27}{section*.128}%
\contentsline {paragraph}{Sentiment Classification from a Single Neuron}{27}{section*.129}%
\contentsline {subsection}{\numberline {8.1}Regularization}{27}{subsection.0.8.1}%
\contentsline {paragraph}{Pitfalls when using word vectors}{27}{section*.130}%
\contentsline {section}{\numberline {9}Recurrent Neural Networks}{27}{section.0.9}%
\contentsline {paragraph}{Recap}{27}{section*.131}%
\contentsline {paragraph}{Recurrent}{28}{section*.132}%
\contentsline {paragraph}{Hidden Units}{28}{section*.133}%
\contentsline {paragraph}{Advantages}{28}{section*.134}%
\contentsline {paragraph}{Disadvantages}{28}{section*.135}%
\contentsline {paragraph}{Simple RNN Language Model}{28}{section*.136}%
\contentsline {paragraph}{Vanilla RNN}{29}{section*.137}%
\contentsline {paragraph}{Training}{29}{section*.138}%
\contentsline {paragraph}{Backpropagation through time}{29}{section*.139}%
\contentsline {paragraph}{Training RNN Language Model}{29}{section*.140}%
\contentsline {paragraph}{Vanishing Gradients}{30}{section*.141}%
\contentsline {paragraph}{Exploding Gradients}{30}{section*.142}%
\contentsline {subsection}{\numberline {9.1}Specializations of RNNs}{30}{subsection.0.9.1}%
\contentsline {paragraph}{Notation}{30}{section*.143}%
\contentsline {subsubsection}{LSTM}{31}{section*.144}%
\contentsline {paragraph}{Long Short-Term Memory}{31}{section*.145}%
\contentsline {subsubsection}{GRU}{32}{section*.146}%
\contentsline {paragraph}{Gated Recurrent Units}{32}{section*.147}%
\contentsline {section}{\numberline {10}Parsing}{33}{section.0.10}%
\contentsline {paragraph}{Dealing with Text}{33}{section*.148}%
\contentsline {paragraph}{Sentence Structure}{33}{section*.149}%
\contentsline {paragraph}{Practical uses of parsing}{33}{section*.150}%
\contentsline {subsection}{\numberline {10.1}Parsing Approaches}{33}{subsection.0.10.1}%
\contentsline {subsubsection}{Constituency Grammar}{33}{section*.151}%
\contentsline {paragraph}{Context Free Grammars}{34}{section*.152}%
\contentsline {paragraph}{Constituency Parsing}{34}{section*.153}%
\contentsline {subparagraph}{Statistical Parsing}{34}{section*.154}%
\contentsline {subsubsection}{Dependency Grammar}{34}{section*.155}%
\contentsline {paragraph}{Dependency Structure}{34}{section*.156}%
\contentsline {paragraph}{Difference Between Constituency Tree and Dependency Trees}{34}{section*.157}%
\contentsline {paragraph}{Annotation Constraints}{35}{section*.158}%
\contentsline {paragraph}{Data-Driven Dependency Parsing}{35}{section*.159}%
\contentsline {subparagraph}{Transition-Based Shift-Reduce Parsing}{35}{section*.160}%
\contentsline {paragraph}{Parsing as Classification}{35}{section*.161}%
\contentsline {paragraph}{Dependency Graph}{35}{section*.162}%
\contentsline {paragraph}{Arc Standard Transitions}{35}{section*.163}%
\contentsline {paragraph}{Parser Algorithm}{36}{section*.164}%
\contentsline {paragraph}{Oracle}{36}{section*.165}%
\contentsline {paragraph}{Projectivity}{36}{section*.166}%
\contentsline {paragraph}{Arc-Standard Algorithm}{36}{section*.167}%
\contentsline {paragraph}{Arc Eager Transitions}{36}{section*.168}%
\contentsline {paragraph}{Learning Procedure}{36}{section*.169}%
\contentsline {paragraph}{CoNLL-X Shared Task}{36}{section*.170}%
\contentsline {paragraph}{Problems with Oracles}{36}{section*.171}%
\contentsline {subsubsection}{Graph-Based Depencency Parsing}{37}{section*.172}%
\contentsline {paragraph}{NN Graph-Based Parser}{37}{section*.173}%
\contentsline {subparagraph}{Parser}{37}{section*.174}%
\contentsline {subparagraph}{Dependency Relations}{37}{section*.175}%
\contentsline {subparagraph}{Self-Attention}{38}{section*.176}%
\contentsline {subparagraph}{Classifier for Labels}{38}{section*.177}%
\contentsline {section}{\numberline {11}Universal Dependencies}{38}{section.0.11}%
\contentsline {paragraph}{Goals}{38}{section*.178}%
\contentsline {paragraph}{Guiding Principles}{38}{section*.179}%
\contentsline {paragraph}{Design Principles}{39}{section*.180}%
\contentsline {paragraph}{Morphological Annotation}{39}{section*.181}%
\contentsline {paragraph}{Syntactic Annotation}{39}{section*.182}%
\contentsline {paragraph}{Coordination}{39}{section*.183}%
\contentsline {paragraph}{Enhanced Dependencies}{40}{section*.184}%
\contentsline {paragraph}{Basic}{40}{section*.185}%
\contentsline {paragraph}{Enhanced}{40}{section*.186}%
\contentsline {paragraph}{Dependency Structure}{40}{section*.187}%
\contentsline {paragraph}{Parsing}{40}{section*.188}%
\contentsline {section}{\numberline {12}Machine Translation}{41}{section.0.12}%
\contentsline {paragraph}{Issues}{41}{section*.189}%
\contentsline {paragraph}{Alignment}{41}{section*.190}%
\contentsline {paragraph}{MT Already Good for...}{41}{section*.191}%
\contentsline {paragraph}{MT Not Yet Good Enough for...}{41}{section*.192}%
\contentsline {subsection}{\numberline {12.1}Language Similarities and Divergences}{41}{subsection.0.12.1}%
\contentsline {paragraph}{Typology}{41}{section*.193}%
\contentsline {paragraph}{Morphology}{41}{section*.194}%
\contentsline {paragraph}{Morphological Variation}{41}{section*.195}%
\contentsline {paragraph}{Segmentation Variation}{42}{section*.196}%
\contentsline {paragraph}{Lexical Gaps}{42}{section*.197}%
\contentsline {paragraph}{Event-To-Argument Divergences}{42}{section*.198}%
\contentsline {subsection}{\numberline {12.2}Classical Techniques}{42}{subsection.0.12.2}%
\contentsline {paragraph}{Direct Translation}{42}{section*.199}%
\contentsline {subparagraph}{Pros}{42}{section*.200}%
\contentsline {subparagraph}{Cons}{42}{section*.201}%
\contentsline {paragraph}{Transfer Model}{42}{section*.202}%
\contentsline {subparagraph}{Lexical Transfer}{42}{section*.203}%
\contentsline {subparagraph}{Systram}{43}{section*.204}%
\contentsline {paragraph}{Interlingua}{43}{section*.205}%
\contentsline {subparagraph}{Pros}{43}{section*.206}%
\contentsline {subparagraph}{Cons}{43}{section*.207}%
\contentsline {subsection}{\numberline {12.3}Statistical Machine Translation}{43}{subsection.0.12.3}%
\contentsline {paragraph}{Example}{43}{section*.208}%
\contentsline {paragraph}{What Makes a Good Translation}{44}{section*.209}%
\contentsline {paragraph}{Fluency $P(T)$}{44}{section*.210}%
\contentsline {paragraph}{Faithfulness $P(S\:|\:T)$}{44}{section*.211}%
\contentsline {subparagraph}{Sentence Alignment}{44}{section*.212}%
\contentsline {subparagraph}{Word Alignment}{44}{section*.213}%
\contentsline {paragraph}{Three Problems for Statistical Machine Translation}{45}{section*.214}%
\contentsline {subsection}{\numberline {12.4}Phrase Based Machine Translation}{45}{subsection.0.12.4}%
\contentsline {paragraph}{Translation Probabilities}{45}{section*.215}%
\contentsline {paragraph}{Distortion probability}{45}{section*.216}%
\contentsline {paragraph}{Training $P(F\:|\:E)$}{45}{section*.217}%
\contentsline {paragraph}{Computing Word Alignments}{46}{section*.218}%
\contentsline {paragraph}{Training Alignment Probabilities}{47}{section*.219}%
\contentsline {subparagraph}{EM Algorithm for Word Alignment}{47}{section*.220}%
\contentsline {subsubsection}{Phrase-Based Translation Model}{47}{section*.221}%
\contentsline {paragraph}{Phrase Alignment}{47}{section*.222}%
\contentsline {paragraph}{Decoding}{47}{section*.223}%
\contentsline {paragraph}{Evaluation}{48}{section*.224}%
\contentsline {subparagraph}{Scores}{48}{section*.225}%
\contentsline {subsection}{\numberline {12.5}Syntax Based Statistical Machine Translation}{48}{subsection.0.12.5}%
\contentsline {paragraph}{Synchronous Grammar}{48}{section*.226}%
\contentsline {paragraph}{Synchronous Derivations and Translation Models}{49}{section*.227}%
\contentsline {paragraph}{Use of Dependency Parsing}{49}{section*.228}%
\contentsline {subsection}{\numberline {12.6}Minimum Error Rate Training}{49}{subsection.0.12.6}%
\contentsline {paragraph}{Conclusions}{49}{section*.229}%
\contentsline {section}{\numberline {13}Neural Machine Translation}{49}{section.0.13}%
\contentsline {paragraph}{NMT}{49}{section*.230}%
\contentsline {paragraph}{Beam Search}{50}{section*.231}%
\contentsline {subparagraph}{Example}{50}{section*.232}%
\contentsline {subparagraph}{Stopping Criterion}{51}{section*.233}%
\contentsline {subparagraph}{Finishing Up}{51}{section*.234}%
\contentsline {subparagraph}{Benefits}{51}{section*.235}%
\contentsline {subparagraph}{Disadvantages}{51}{section*.236}%
\contentsline {subsection}{\numberline {13.1}Attention}{51}{subsection.0.13.1}%
\contentsline {paragraph}{Attention}{51}{section*.237}%
\contentsline {subparagraph}{In equations}{52}{section*.238}%
\contentsline {subparagraph}{Attention Variants}{52}{section*.239}%
\contentsline {subsubsection}{Self-Attention}{53}{section*.240}%
\contentsline {paragraph}{Issues with Recurrent Models}{53}{section*.241}%
\contentsline {paragraph}{Word Windows}{53}{section*.242}%
\contentsline {paragraph}{Attention}{53}{section*.243}%
\contentsline {paragraph}{Self-Attention}{54}{section*.244}%
\contentsline {paragraph}{Vector Notation}{54}{section*.245}%
\contentsline {paragraph}{Self-Attention as a NLP Building Block}{54}{section*.246}%
\contentsline {subparagraph}{No Notion of Order}{54}{section*.247}%
\contentsline {subparagraph}{Adding Non-Linearities}{55}{section*.248}%
\contentsline {subparagraph}{Future}{55}{section*.249}%
\contentsline {subsection}{\numberline {13.2}Transformers}{55}{subsection.0.13.2}%
\contentsline {paragraph}{Self-Attention}{55}{section*.250}%
\contentsline {paragraph}{Multi-Headed Attention}{56}{section*.251}%
\contentsline {paragraph}{Training Tricks}{56}{section*.252}%
\contentsline {subsubsection}{Transformers Library}{56}{section*.253}%
\contentsline {paragraph}{Hugging Face Transformers}{56}{section*.254}%
\contentsline {subsubsection}{Transformers Architectures}{57}{section*.255}%
\contentsline {paragraph}{Pretraining Transformers}{57}{section*.256}%
\contentsline {subparagraph}{Pretraining through language modeling}{57}{section*.257}%
\contentsline {paragraph}{Pretraining-Finetuning Paradigm}{57}{section*.258}%
\contentsline {subparagraph}{SGD}{57}{section*.259}%
\contentsline {paragraph}{Pretraining for Three Types of Architectures}{57}{section*.260}%
\contentsline {subparagraph}{Pretraining Decoders}{58}{section*.261}%
\contentsline {paragraph}{Pretraining Encoders}{58}{section*.262}%
\contentsline {paragraph}{Problems with previous methods}{58}{section*.263}%
\contentsline {subparagraph}{BERT}{58}{section*.264}%
\contentsline {paragraph}{Masked LM}{59}{section*.265}%
\contentsline {paragraph}{Wordpiece}{59}{section*.266}%
\contentsline {paragraph}{Pretraining Encoder-Decoders}{59}{section*.267}%
\contentsline {paragraph}{What pretraining objective to use?}{59}{section*.268}%
\contentsline {paragraph}{Pre-training Tasks}{59}{section*.269}%
\contentsline {paragraph}{Masked LM}{59}{section*.270}%
\contentsline {paragraph}{Next Sentence Prediction}{59}{section*.271}%
\contentsline {paragraph}{In-Context Learning}{60}{section*.272}%
\contentsline {paragraph}{Distillation}{60}{section*.273}%
\contentsline {section}{\numberline {14}Analysis of Language Models}{60}{section.0.14}%
\contentsline {paragraph}{Questions About Language Models}{60}{section*.274}%
\contentsline {paragraph}{What Linguistic Knowledge is Present in LM?}{60}{section*.275}%
\contentsline {subparagraph}{Unsupervised NER}{60}{section*.276}%
\contentsline {subparagraph}{LM Effectivness}{60}{section*.277}%
\contentsline {subparagraph}{LM as Linguistic Test Subjects}{60}{section*.278}%
\contentsline {subparagraph}{Prediction Explanations}{61}{section*.279}%
\contentsline {subsection}{\numberline {14.1}Probes}{61}{subsection.0.14.1}%
\contentsline {paragraph}{Probing}{61}{section*.280}%
\contentsline {paragraph}{Contextual Representation of Language}{62}{section*.281}%
\contentsline {paragraph}{Structural Probe}{62}{section*.282}%
\contentsline {subparagraph}{Syntax Distance Hypothesis}{62}{section*.283}%
\contentsline {subparagraph}{Finding a Parse Tree Encoding Distance Metric}{62}{section*.284}%
\contentsline {subparagraph}{Finding $B$}{62}{section*.285}%
\contentsline {paragraph}{Conclusion}{63}{section*.286}%
\contentsline {section}{\numberline {15}Prompt-Based Learning}{63}{section.0.15}%
\contentsline {paragraph}{Finetuning}{63}{section*.287}%
\contentsline {paragraph}{Zero-shot}{63}{section*.288}%
\contentsline {paragraph}{One-Shot}{63}{section*.289}%
\contentsline {paragraph}{Few-Shot}{63}{section*.290}%
\contentsline {subsection}{\numberline {15.1}Prompts}{64}{subsection.0.15.1}%
\contentsline {paragraph}{Prefix Tuning}{64}{section*.291}%
\contentsline {paragraph}{Prompt Tuning}{65}{section*.292}%
\contentsline {paragraph}{Prompt-Tuning Ensembles}{66}{section*.293}%
\contentsline {paragraph}{Interpretability}{66}{section*.294}%
\contentsline {subsection}{\numberline {15.2}Soft Prompt Transfer}{66}{subsection.0.15.2}%
\contentsline {section}{\numberline {16}Reading Comprehension}{66}{section.0.16}%
\contentsline {paragraph}{Taxonomy of Question Answering}{66}{section*.295}%
\contentsline {paragraph}{Difference Between QA Tasks}{67}{section*.296}%
\contentsline {paragraph}{Machine Comprehension}{67}{section*.297}%
\contentsline {paragraph}{SQuAD}{67}{section*.298}%
\contentsline {subparagraph}{Evaluation}{67}{section*.299}%
\contentsline {paragraph}{SQuAD 2.0}{67}{section*.300}%
\contentsline {subsection}{\numberline {16.1}Neural Models for Reading Comprehension}{68}{subsection.0.16.1}%
\contentsline {paragraph}{LSTM-based vs BERT-based}{68}{section*.301}%
\contentsline {paragraph}{Seq2Seq w/ Attention (recap)}{68}{section*.302}%
\contentsline {subsubsection}{BiDAF}{68}{section*.303}%
\contentsline {paragraph}{Encoding}{68}{section*.304}%
\contentsline {paragraph}{Attention}{69}{section*.305}%
\contentsline {paragraph}{Modeling and Output Layers}{69}{section*.306}%
\contentsline {subsubsection}{BERT}{70}{section*.307}%
\contentsline {paragraph}{BERT for Reading Comprehension}{70}{section*.308}%
\contentsline {paragraph}{Comparing BiDAF and BERT}{70}{section*.309}%
\contentsline {subsubsection}{Stanford Attentive Reader}{70}{section*.310}%
\contentsline {subsubsection}{SpanBERT}{70}{section*.311}%
\contentsline {subsection}{\numberline {16.2}State-of-the-Art}{70}{subsection.0.16.2}%
\contentsline {paragraph}{Is Reading Comprehension Solved?}{70}{section*.312}%
\contentsline {section}{\numberline {17}Open Domain Question Answering}{71}{section.0.17}%
\contentsline {paragraph}{PiQASso}{71}{section*.313}%
\contentsline {paragraph}{Question Analysis}{71}{section*.314}%
\contentsline {paragraph}{Memory}{71}{section*.315}%
\contentsline {subparagraph}{Retriever-Reader Framework}{72}{section*.316}%
\contentsline {subparagraph}{Document Reader}{72}{section*.317}%
\contentsline {paragraph}{Inference Question Answering}{72}{section*.318}%
\contentsline {paragraph}{Reading Comprehension}{72}{section*.319}%
\contentsline {paragraph}{Facebook's bAbI Dataset}{72}{section*.320}%
\contentsline {paragraph}{Question Dependent Recurrent Entity Network for Question Answering}{73}{section*.321}%
\contentsline {subparagraph}{Input encoder}{73}{section*.322}%
\contentsline {subparagraph}{Dynamic Memory}{74}{section*.323}%
\contentsline {subparagraph}{Output Module}{74}{section*.324}%
\contentsline {subparagraph}{Training}{74}{section*.325}%
\contentsline {subparagraph}{Architecture}{75}{section*.326}%
\contentsline {section}{\numberline {18}Coreference Resolution}{75}{section.0.18}%
\contentsline {paragraph}{Coreference Resoultion}{75}{section*.327}%
\contentsline {paragraph}{Applications}{75}{section*.328}%
\contentsline {paragraph}{Coreference Resolution in two steps}{75}{section*.329}%
\contentsline {paragraph}{Mention Detection}{75}{section*.330}%
\contentsline {paragraph}{Can We Avoid a Pipelined System?}{76}{section*.331}%
\contentsline {paragraph}{Anaphora}{76}{section*.332}%
\contentsline {paragraph}{Context}{76}{section*.333}%
\contentsline {subsection}{\numberline {18.1}Coreference Models}{76}{subsection.0.18.1}%
\contentsline {subsubsection}{Rule-Based Approach}{76}{section*.334}%
\contentsline {paragraph}{Hobb's Naive Algorithm}{76}{section*.335}%
\contentsline {paragraph}{Knowledge-based Pronominal Coreference}{77}{section*.336}%
\contentsline {subsubsection}{Mention Pair/Mention Ranking}{77}{section*.337}%
\contentsline {paragraph}{Mention Pair Training}{77}{section*.338}%
\contentsline {paragraph}{Mention Pair Prediction}{77}{section*.339}%
\contentsline {paragraph}{Coreference Models}{78}{section*.340}%
\contentsline {subparagraph}{Mention Ranking}{78}{section*.341}%
\contentsline {subparagraph}{Training}{78}{section*.342}%
\contentsline {paragraph}{End-To-End Neural Coref Model}{79}{section*.343}%
\contentsline {paragraph}{BERT-based coref: now has the best results}{80}{section*.344}%
\contentsline {paragraph}{CorefQA}{80}{section*.345}%
\contentsline {subparagraph}{Mention Proposal}{80}{section*.346}%
\contentsline {subparagraph}{Mention Linking}{80}{section*.347}%
\contentsline {subparagraph}{Mention Pruning}{81}{section*.348}%
\contentsline {paragraph}{Coreference Evaluation}{81}{section*.349}%
\contentsline {subparagraph}{B-CUBED}{81}{section*.350}%
\contentsline {section}{\numberline {19}Integrating Knowledge in Language Models}{81}{section.0.19}%
\contentsline {paragraph}{Recap}{81}{section*.351}%
\contentsline {paragraph}{Eliciting Knowledge}{81}{section*.352}%
\contentsline {subparagraph}{Querying Traditional Knowledgebases}{81}{section*.353}%
\contentsline {paragraph}{Querying LM as Knowledgebases}{81}{section*.354}%
\contentsline {paragraph}{Advantages of LMs over traditional KBs}{82}{section*.355}%
\contentsline {subsection}{\numberline {19.1}Adding Knowledge to LMs}{82}{subsection.0.19.1}%
\contentsline {subsubsection}{Add pretrained entity embeddings}{82}{section*.356}%
\contentsline {subparagraph}{Entity Linking}{82}{section*.357}%
\contentsline {paragraph}{Incorporate Entity Embeddings from a Different Embeddings Space}{82}{section*.358}%
\contentsline {subparagraph}{ERNIE}{83}{section*.359}%
\contentsline {subparagraph}{QAGNN/GreaseLM}{84}{section*.360}%
\contentsline {subsubsection}{Use an external memory}{85}{section*.361}%
\contentsline {subparagraph}{KGLM}{85}{section*.362}%
\contentsline {subsubsection}{Modify Pretrained Data}{85}{section*.363}%
\contentsline {subparagraph}{WKLM}{86}{section*.364}%
\contentsline {subsubsection}{Inductive Biases Through Masking}{86}{section*.365}%
\contentsline {paragraph}{ERNIE}{86}{section*.366}%
\contentsline {paragraph}{Salient span masking}{86}{section*.367}%
\contentsline {subsection}{\numberline {19.2}Evaluation on Downstream Tasks}{86}{subsection.0.19.2}%
\contentsline {paragraph}{Knowledge-Intensive Downstream Tasks}{86}{section*.368}%
\contentsline {section}{\numberline {20}Dialogue Systems}{86}{section.0.20}%
\contentsline {paragraph}{Early Approaches}{86}{section*.369}%
\contentsline {subparagraph}{Templates and Rules}{87}{section*.370}%
\contentsline {paragraph}{Task Oriented vs Open Domain}{87}{section*.371}%
\contentsline {paragraph}{Task Oriented (easier)}{87}{section*.372}%
\contentsline {paragraph}{Open Domain (harder)}{87}{section*.373}%
\contentsline {paragraph}{Retrieval Based vs Generative}{87}{section*.374}%
\contentsline {paragraph}{Retrieval Based (easier)}{87}{section*.375}%
\contentsline {paragraph}{Generative (harder)}{87}{section*.376}%
\contentsline {paragraph}{Short vs Long conversations}{87}{section*.377}%
\contentsline {paragraph}{Short Conversation (easier)}{87}{section*.378}%
\contentsline {paragraph}{Long Conversation (harder)}{87}{section*.379}%
\contentsline {subsection}{\numberline {20.1}Task-Oriented}{88}{subsection.0.20.1}%
\contentsline {paragraph}{Natural Language Understanding}{88}{section*.380}%
\contentsline {paragraph}{Dialogue Manager}{88}{section*.381}%
\contentsline {paragraph}{Dialogue State}{89}{section*.382}%
\contentsline {paragraph}{Dialogue Act Labeling and Slot Filling}{89}{section*.383}%
\contentsline {subparagraph}{Slot Filling with ConVEx}{89}{section*.384}%
\contentsline {subparagraph}{DialoGPT}{89}{section*.385}%
\contentsline {subsection}{\numberline {20.2}Generation-Oriented}{90}{subsection.0.20.2}%
\contentsline {paragraph}{Transducer Model}{90}{section*.386}%
\contentsline {paragraph}{Neural Models for Dialogue Response Generation}{90}{section*.387}%
\contentsline {paragraph}{Seq2Seq}{90}{section*.388}%
\contentsline {subsection}{\numberline {20.3}Retrieval-Based}{90}{subsection.0.20.3}%
\contentsline {paragraph}{Templates}{90}{section*.389}%
\contentsline {paragraph}{Retrieval-Based Chat}{90}{section*.390}%
\contentsline {paragraph}{Neural Response Retrieval}{90}{section*.391}%
\contentsline {subsection}{\numberline {20.4}Challenges}{91}{subsection.0.20.4}%
\contentsline {paragraph}{Incorporating Context}{91}{section*.392}%
\contentsline {paragraph}{Coherence}{91}{section*.393}%
\contentsline {paragraph}{Diversity}{91}{section*.394}%
\contentsline {paragraph}{Personality}{92}{section*.395}%
\contentsline {subsection}{\numberline {20.5}Google DialogFlow}{92}{subsection.0.20.5}%
\contentsline {paragraph}{Flow}{93}{section*.396}%
\contentsline {paragraph}{Intent}{93}{section*.397}%
\contentsline {section}{\numberline {21}Trends and Future of NLP}{93}{section.0.21}%
\contentsline {paragraph}{Applications}{93}{section*.398}%
\contentsline {paragraph}{Attention Mechanism}{94}{section*.399}%
\contentsline {paragraph}{Story of recent years in Deep Learning NLP}{94}{section*.400}%
\contentsline {paragraph}{Large Language Models and GPT-3}{94}{section*.401}%
\contentsline {subsection}{\numberline {21.1}Compositional Representations and Systematic Generalization}{95}{subsection.0.21.1}%
\contentsline {paragraph}{Systematicity}{95}{section*.402}%
\contentsline {paragraph}{Compositionality}{95}{section*.403}%
\contentsline {paragraph}{Tree Reconstruction Error}{95}{section*.404}%
\contentsline {subsection}{\numberline {21.2}Assessing Deep Learning for NLP}{95}{subsection.0.21.2}%
\contentsline {paragraph}{Neural Network Effectiveness}{95}{section*.405}%
\contentsline {paragraph}{Understanding Models by Breaking Them}{95}{section*.406}%
\contentsline {paragraph}{Remarks}{95}{section*.407}%
\contentsline {subsection}{\numberline {21.3}Inferring From Memory}{95}{subsection.0.21.3}%
\contentsline {paragraph}{Current State}{95}{section*.408}%
\contentsline {paragraph}{Memory and Inference}{95}{section*.409}%
\contentsline {paragraph}{Inter-Sentence}{95}{section*.410}%
\contentsline {paragraph}{Limits of Single Task Learning}{95}{section*.411}%
\contentsline {paragraph}{Multiple Tasks}{96}{section*.412}%
\contentsline {paragraph}{Deep Sequence Models}{96}{section*.413}%
\contentsline {paragraph}{The 3 Equivalent NLP-Complete Super Tasks}{96}{section*.414}%
\contentsline {subparagraph}{QA Completeness}{96}{section*.415}%
\contentsline {subparagraph}{Seq2SQL}{96}{section*.416}%
\contentsline {paragraph}{Machine Translation}{97}{section*.417}%
\contentsline {subsection}{\numberline {21.4}Huge Models}{97}{subsection.0.21.4}%
\contentsline {paragraph}{GPT-2 Reactions}{97}{section*.418}%
\contentsline {paragraph}{Responsibility}{97}{section*.419}%
\contentsline {paragraph}{High Impact Decisions}{97}{section*.420}%
\contentsline {section}{\numberline {22}Thinking Fast and Slow}{98}{section.0.22}%
\contentsline {section}{\numberline {23}Cheat Sheet}{99}{section.0.23}%
\contentsline {paragraph}{Language Modeling}{99}{section*.421}%
\contentsline {paragraph}{Perplexity}{99}{section*.422}%
