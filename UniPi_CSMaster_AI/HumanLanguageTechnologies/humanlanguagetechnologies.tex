\documentclass[10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{textcomp}
\graphicspath{ {./img/} }
\usepackage{listings}
\usepackage{makecell}
\usepackage{qtree}
\usepackage{pgfplots}
\usepackage{tikz}
\usepgflibrary{shapes}
\usepgfplotslibrary{fillbetween}
\definecolor{backcolour}{RGB}{255,255,255}
\definecolor{codegreen}{RGB}{27,168,11}
\definecolor{codeblue}{RGB}{35,35,205}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{205,35,56}
\lstdefinestyle{myPython}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{codeblue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\small\ttfamily,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=2pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	language=python
}
\newcommand*\triangled[1]{\tikz[baseline=(char.base)]{
            \node[regular polygon, regular polygon sides=3,draw,inner sep=1pt] (char) {#1};}}
            
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhead{}
\fancyfoot[L]{Telegram: \texttt{@fexed}}
\fancyfoot[R]{Github: \texttt{fexed}}
\begin{document}
\title{Human Language Technologies}
\author{Federico Matteoni}
\date{A.A. 2021/22}
\renewcommand*\contentsname{Index}

\maketitle
\tableofcontents
\pagebreak
\section{Introduction}
Prof. Giuseppe Attardi\\
Prerequisites are: proficiency in Python, basic probability and statistics, calculus and linear algebra and notions of machine learning.
\paragraph{What will we learn} Understanding of and ability to use effective modern methods for \textbf{Natural Language Processing}. From traditional methods to current advanced ones like RNN, Attentions\ldots\\
Understanding the difficulties in dealing with NL and the capabilities of current technologies, with experience with \textbf{modern tools} and aiming towards the ability to build systems for some major NLP tasks: word similarities, parsing, machine translation, entity recognition, question answering, sentiment analysis, dialogue system\ldots
\paragraph{Books} Speech and Language Processing (Jurafsky, Martin), Deep Learning (Goodfellow, Bengio, Courville), Natural Language Processing in Python (Bird, Klein, Loper)
\paragraph{Exam} Project (alone or team of 2-3 people) with the aim to experiment with techniques in a realistic setting using data from competitions (Kaggle, CoNLL, SemEval, Evalita\ldots). The topic will be proposed by the team or chosen from a list of suggestions.
\paragraph{Experimental Approach}\begin{enumerate}
	\item Formulate hypothesis
	\item Implement technique
	\item Train and test
	\item Apply evaluation metric
	\item If not improved:\begin{list}{}{}
		\item Perform error analysis
		\item Revise hypothesis
	\end{list}
	\item Repeat!
\end{enumerate}
\paragraph{Motivations} Language is the most distinctive feature of human intelligence, \textbf{it shapes thought}. Emulating language capabilities is a scientific challenge, a \textbf{keystone for intelligent systems} (see: Turing test)
\paragraph{Structured vs unstructured data} The largest amount of information shared with each other is unstructured, primarily text. Information is mostly communicated by e-mails, reports, articles, conversations, media\ldots and attempts to turn text to structured (HTML) or microformat only scratched the surface.\\
Problems: requires universal agreed \textbf{ontologies} and additional effort. Entity linking attempts to provide a bridge.
\section{State of the Art}
\paragraph{Early History} During 1950s, up until AI winter.
\paragraph{Resurgence in the 1990s} Thanks to statistical methods, novelty, to study language. Challenges arise: NIST, Netflix, DARPA Grand Challenge\ldots\\
During 2010s: deep learning, neural machine translation\ldots
\paragraph{Statistical Machine Learning} Supervised training with \textbf{annotated} documents.\\
The paradigm is composed of the following:
\begin{list}{}{}
	\item Training set $\{x_i,y_y\}$
	\item Representation: choose a set of features to represent data $x\mapsto \phi(x) \in R^D$
	\item Model: choose an hypothesis function to compute $f(x) = F_\Theta(\phi(x))$
	\item Evaluation: define the cost function on error with respect to examples $J(\Theta) = \sum_i (f(x_i) - y_i)^2$
	\item Optimization: find parameters $\Theta$ that minimize $J(\Theta)$
\end{list}
It's a generic method, applicable to any problem.
\paragraph{Traditional Supervised Learning Approach} Freed us from devising algorithms and rules, requiring the creation of annotated training sets and imposing the tyranny of feature engineering.\\
Standard approach for each new problem:
\begin{list}{}{}
	\item Gather as much labeled data as one can
	\item Throw a bunch of models at it
	\item Pick the best
	\item Spend hours hand engineering some features or doing feature selection/dimensionality reduction
	\item Rinse and repeat
\end{list}
\paragraph{Technological Breakthroughs} Improved ML techniques but also large annotated datasets and more computing power, provided by GPUs and dedicated ML processors (like the TPU by Google).\\
ML exploits parallelism: stochastic gradient descent can be parallelized (asynchronous stochastic gradient descent). No need to protect shared memory access, and low (half, single) precision is enough.
\paragraph{Deep Learning Approach} Was a big breakthrough.\begin{list}{}{}
	\item Design a model architecture
	\item Define a loss function
	\item Run the network letting the parameters and the data representations \textbf{self-organize} as to minimize the loss
	\item End-to-end learning: no intermediate stages nor representation
\end{list}
\subparagraph{Feature representation} Use a vector with each entry representing a feature of the domain element\\
	Deep Learning represents data as vectors. Images are vectors (matrices), but words? \textbf{Word Embeddings}: transform a word into a vector of hundreds of dimensions capturing many subtle aspects of its meaning. Computed by the means of \textbf{language model}.\\
From a discrete to distributed representation. Words meaning are dense vectors of weights in a high dimensional space, with algebraic properties.\\
Background: philosophy, linguistics and statistics ML (feature vectors).
\subparagraph{Language Model} Statistical model which tells the probability that a word comes after a given word in a sentence.
\subparagraph{Dealing with Sentences} A sentence is a sequence of words: build a representation of a sequence from those of its words (compositional hypothesis). Sequence to sequence models.\\
Is there more structure in a sentence than a sequence of words? In many cases, tools forgets information when translating sentences into sequences of words, discarding much of the structure.
\end{document}