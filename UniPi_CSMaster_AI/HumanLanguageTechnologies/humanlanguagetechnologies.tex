\documentclass[10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{textcomp}
\graphicspath{ {./img/} }
\usepackage{listings}
\usepackage{makecell}
\usepackage{qtree}
\usepackage{pgfplots}
\usepackage{tikz}
\usepgflibrary{shapes}
\usepgfplotslibrary{fillbetween}
\definecolor{backcolour}{RGB}{255,255,255}
\definecolor{codegreen}{RGB}{27,168,11}
\definecolor{codeblue}{RGB}{35,35,205}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{205,35,56}
\lstdefinestyle{myPython}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{codeblue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\small\ttfamily,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=2pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	language=python
}
\newcommand*\triangled[1]{\tikz[baseline=(char.base)]{
            \node[regular polygon, regular polygon sides=3,draw,inner sep=1pt] (char) {#1};}}
            
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhead{}
\fancyfoot[L]{Telegram: \texttt{@fexed}}
\fancyfoot[R]{Github: \texttt{fexed}}
\begin{document}
\title{Human Language Technologies}
\author{Federico Matteoni}
\date{A.A. 2021/22}
\renewcommand*\contentsname{Index}

\maketitle
\tableofcontents
\pagebreak
\section{Introduction}
Prof. Giuseppe Attardi\\
Prerequisites are: proficiency in Python, basic probability and statistics, calculus and linear algebra and notions of machine learning.
\paragraph{What will we learn} Understanding of and ability to use effective modern methods for \textbf{Natural Language Processing}. From traditional methods to current advanced ones like RNN, Attentions\ldots\\
Understanding the difficulties in dealing with NL and the capabilities of current technologies, with experience with \textbf{modern tools} and aiming towards the ability to build systems for some major NLP tasks: word similarities, parsing, machine translation, entity recognition, question answering, sentiment analysis, dialogue system\ldots
\paragraph{Books} Speech and Language Processing (Jurafsky, Martin), Deep Learning (Goodfellow, Bengio, Courville), Natural Language Processing in Python (Bird, Klein, Loper)
\paragraph{Exam} Project (alone or team of 2-3 people) with the aim to experiment with techniques in a realistic setting using data from competitions (Kaggle, CoNLL, SemEval, Evalita\ldots). The topic will be proposed by the team or chosen from a list of suggestions.
\paragraph{Experimental Approach}\begin{enumerate}
	\item Formulate hypothesis
	\item Implement technique
	\item Train and test
	\item Apply evaluation metric
	\item If not improved:\begin{list}{}{}
		\item Perform error analysis
		\item Revise hypothesis
	\end{list}
	\item Repeat!
\end{enumerate}
\paragraph{Motivations} Language is the most distinctive feature of human intelligence, \textbf{it shapes thought}. Emulating language capabilities is a scientific challenge, a \textbf{keystone for intelligent systems} (see: Turing test)
\paragraph{Structured vs unstructured data} The largest amount of information shared with each other is unstructured, primarily text. Information is mostly communicated by e-mails, reports, articles, conversations, media\ldots and attempts to turn text to structured (HTML) or microformat only scratched the surface.\\
Problems: requires universal agreed \textbf{ontologies} and additional effort. Entity linking attempts to provide a bridge.
\section{State of the Art}
\paragraph{Early History} During 1950s, up until AI winter.
\paragraph{Resurgence in the 1990s} Thanks to statistical methods, novelty, to study language. Challenges arise: NIST, Netflix, DARPA Grand Challenge\ldots\\
During 2010s: deep learning, neural machine translation\ldots
\paragraph{Statistical Machine Learning} Supervised training with \textbf{annotated} documents.\\
The paradigm is composed of the following:
\begin{list}{}{}
	\item Training set $\{x_i,y_y\}$
	\item Representation: choose a set of features to represent data $x\mapsto \phi(x) \in R^D$
	\item Model: choose an hypothesis function to compute $f(x) = F_\Theta(\phi(x))$
	\item Evaluation: define the cost function on error with respect to examples $J(\Theta) = \sum_i (f(x_i) - y_i)^2$
	\item Optimization: find parameters $\Theta$ that minimize $J(\Theta)$
\end{list}
It's a generic method, applicable to any problem.
\paragraph{Traditional Supervised Learning Approach} Freed us from devising algorithms and rules, requiring the creation of annotated training sets and imposing the tyranny of feature engineering.\\
Standard approach for each new problem:
\begin{list}{}{}
	\item Gather as much labeled data as one can
	\item Throw a bunch of models at it
	\item Pick the best
	\item Spend hours hand engineering some features or doing feature selection/dimensionality reduction
	\item Rinse and repeat
\end{list}
\paragraph{Technological Breakthroughs} Improved ML techniques but also large annotated datasets and more computing power, provided by GPUs and dedicated ML processors (like the TPU by Google).\\
ML exploits parallelism: stochastic gradient descent can be parallelized (asynchronous stochastic gradient descent). No need to protect shared memory access, and low (half, single) precision is enough.
\paragraph{Deep Learning Approach} Was a big breakthrough.\begin{list}{}{}
	\item Design a model architecture
	\item Define a loss function
	\item Run the network letting the parameters and the data representations \textbf{self-organize} as to minimize the loss
	\item End-to-end learning: no intermediate stages nor representation
\end{list}
\subparagraph{Feature representation} Use a vector with each entry representing a feature of the domain element\\
	Deep Learning represents data as vectors. Images are vectors (matrices), but words? \textbf{Word Embeddings}: transform a word into a vector of hundreds of dimensions capturing many subtle aspects of its meaning. Computed by the means of \textbf{language model}.\\
From a discrete to distributed representation. Words meaning are dense vectors of weights in a high dimensional space, with algebraic properties.\\
Background: philosophy, linguistics and statistics ML (feature vectors).
\subparagraph{Language Model} Statistical model which tells the probability that a word comes after a given word in a sentence.
\subparagraph{Dealing with Sentences} A sentence is a sequence of words: build a representation of a sequence from those of its words (compositional hypothesis). Sequence to sequence models.\\
Is there more structure in a sentence than a sequence of words? In many cases, tools forgets information when translating sentences into sequences of words, discarding much of the structure.
\section{Language Modeling}
\paragraph{Probabilistic Language Model} The goal is to assign a probability to a sentence.
\begin{list}{}{}
	\item \textbf{Machine Translation}: $P($high winds tonight$) > P($large winds tonight$)$
	\item \textbf{Spell Correction}: $P($about fifteen minutes from$) > P($about fifteen minuets from$)$
	\item \textbf{Speech Recognition}: $P($I saw a van$) > P($eye saw a van$)$
	\item \textbf{Language Identification}: $s$ from unknown language (italian or english) and $L$ita, $L$eng language models for italian and english $\Rightarrow$ $L$ita$(s) > L$eng$(s)$
	\item Summarization, question answering\ldots
\end{list}
We want to compute \begin{list}{}{}
	\item $P(W) = P(w_1,w_2,\ldots,w_n)$ the probability of a sequence
	\item $P(w_4\:|\:w_1,w_2,w_3,w_4)$ the probability of a word given some previous words
\end{list}
The model that computes that is called the \textbf{language model}
\paragraph{Markov Model and N-Grams} Simplify the assumption: the probability of a word given all the previous is the same of the probability of that word given just few (one, two\ldots) previous words. So $P(w_i\:|\:w_{i-},\ldots,w_1) = P(w_i\:|\:w_{i-1})$ (First order Markov chain).\\
With a \textbf{$N$-gram}: $P(w_n\:|\:w_1^{n-1}) \simeq P(w_n\:|\:w_{n-N+1}^{n-1})$\\
In general it's insufficient: language has \textbf{long distance dependencies}, but we can often get away with $N$-gram models. For example:\begin{list}{}{}
	\item “The \textbf{man} next to the large oak tree near the grocery store on the corner \textbf{is} tall.”
	\item “The \textbf{men} next to the large oak tree near the grocery store on the corner \textbf{are} tall.”
\end{list}
Or even semantic dependencies:
\begin{list}{}{}
	\item “The \textbf{bird} next to the large oak tree near the grocery store on the corner \textbf{flies} rapidly.”
	\item “The \textbf{man} next to the large oak tree near the grocery store on the corner \textbf{talks} rapidly.”
\end{list}
So more complex models are needed to handle such dependencies.
\paragraph{Maximum likelihood estimate}$$P(w_n\:|\:w_{n-N+1}^{n-1}) = \frac{\hbox{count}(w_{n-N+1}^{n-1},w_n)}{\hbox{count}(w_{n-N+1}^{n-1})}$$
Maximum because it's the one that maximize $P($Training set $|$ Model$)$
\paragraph{Shannon Visualization Method} Generate random sentences:
\begin{list}{}{}
	\item Choose a random bigram $(\langle s\rangle,w)$ according to its probability
	\item Choose a random bigram $(w,x)$ according to its probability
	\item Repeat until we pick $\langle/s\rangle$
\end{list}
\end{document}