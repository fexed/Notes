{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "postal-clause",
   "metadata": {},
   "source": [
    "# Neural Dependency Parsing\n",
    "\n",
    "Derived from code for Stanford CS224N, by:\n",
    "- Sahil Chopra <schopra8@stanford.edu>\n",
    "- Haoshen Hong <haoshen@stanford.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-scene",
   "metadata": {},
   "source": [
    "In this homework, you’ll be implementing a neural-network based dependency parser with the goal of maximizing performance on the UAS (Unlabeled Attachment Score) metric.\n",
    "\n",
    "A dependency parser analyzes the grammatical structure of a sentence, establishing relationships between head words, and dependent words which modify those heads.\n",
    "There are several types of dependency parsers, including transition-based parsers, graph-based parsers, and feature-based parsers. Your implementation will be a transition-based parser, which incrementally builds up a parse one step at a time.\n",
    "The parser maintains a state, which is represented as follows:\n",
    "\n",
    "- A `stack` of words that are currently being processed.\n",
    "- A `buffer` of words yet to be processed.\n",
    "- A `list` of dependencies predicted by the parser.\n",
    "\n",
    "Initially, the stack only contains `ROOT`, the dependencies list is empty, and the buffer contains the list of words of the sentence. At each step, the parser applies a transition to its state until its buffer is empty and the stack size is 1.\n",
    "The following transitions can be applied:\n",
    "\n",
    "- `SHIFT`: removes the first word from the buffer and pushes it onto the stack.\n",
    "- `LEFT-ARC`: marks the second (second most recently added) item on the stack as a dependent of the first item and removes the second item from the stack, adding a first word → second word dependency to the dependency list.\n",
    "- `RIGHT-ARC`: marks the first (most recently added) item on the stack as a dependent of the second item and removes the first item from the stack, adding a second word → first word dependency to the dependency list.\n",
    "\n",
    "On each step, your parser will decide among the three transitions using a neural network classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-blond",
   "metadata": {},
   "source": [
    "# 1   Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-attempt",
   "metadata": {},
   "source": [
    "## 1.1  Transitions\n",
    "Provide the sequence of Attardi’s non-projective transitions for parsing the following sentence:\n",
    "\n",
    "`The president scheduled a meeting yesterday that nobody attended.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-captain",
   "metadata": {},
   "source": [
    "`S S LA S LA S S LA S RA2 S S S S LA LA RA RA`\n",
    "\n",
    "The action `RA2` ocurs when the state is:\n",
    "\n",
    "stack: `ROOT`, `scheduled`, `meeting`<br/>\n",
    "buffer: `yesterday`, `that`, `nobody`, `attended`\n",
    "\n",
    "creating the arc `scheduled -> yesterday`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-surge",
   "metadata": {},
   "source": [
    "## 1.2 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-franklin",
   "metadata": {},
   "source": [
    "*What is the difference in terms of features between neural network dependency parsers (e.g. Chen&Manning 2014, https://cs.stanford.edu/~danqi/papers/emnlp2014.pdf) and non-neural network dependency parsers (e.g. parsers with lots of features like Zhang&Nivre 2011, www.anthology.aclweb.org/P/P11/P11-2033.pdf), in particular in terms of sparsity?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-checklist",
   "metadata": {},
   "source": [
    "Non-neural network dependency parsers classify based on millions of sparse indicator features that\n",
    "generalize poorly and are computationally expensive to compute.\n",
    "A neural network dependency parser uses a small number of dense features, pretrained on a large amount of documents, that encode hidden representations of tokens. Such parser can be both more accurate and more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-liberty",
   "metadata": {},
   "source": [
    "## 1.3  Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-grade",
   "metadata": {},
   "source": [
    "*What is the ambiguity in parsing the following sentence?*<br/>\n",
    "`There are statistics about poverty that no one is willing to accept`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-illinois",
   "metadata": {},
   "source": [
    "The subordinate might refer to either `statistics` or to `poverty`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-perth",
   "metadata": {},
   "source": [
    "## 1.4 Parse Tree"
   ]
  },
  {
   "attachments": {
    "b0612af2-31cc-4a3e-a7e9-d54a7cfa5359.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "absolute-lebanon",
   "metadata": {},
   "source": [
    "*Mention which errors that make the following an incorrect dependency tree:*\n",
    "\n",
    "![image.png](attachment:b0612af2-31cc-4a3e-a7e9-d54a7cfa5359.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-timothy",
   "metadata": {},
   "source": [
    "This is not a tree since B has two heads. The non-projectivity of arc C->A is not a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-democracy",
   "metadata": {},
   "source": [
    "## Exercise 1.\n",
    "Implement the `__init__` and `step` methods in the `ParseState` class in `parser_state.py`. This implements the transition mechanics your parser will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlling-healing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parser_state import ParserState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-county",
   "metadata": {},
   "source": [
    "We will represent sentences as list of tokens, where tokens are named tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "specific-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Token = namedtuple('Token', ['id', 'form', 'pos', 'head', 'deprel'], defaults=(0,)*5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-desire",
   "metadata": {},
   "source": [
    "Example of a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "private-hygiene",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Token(id=1, form='The', pos=0, head=0, deprel=0),\n",
       " Token(id=2, form='cat', pos=0, head=0, deprel=0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Token(1, 'The'), Token(2, 'cat')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-distinction",
   "metadata": {},
   "source": [
    "## Test a single parser step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "infinite-russell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(transition, stack, buf, deps,\n",
    "              ex_stack, ex_buf, ex_deps):\n",
    "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
    "    ps = ParserState([Token(i, f) for i,f in enumerate(stack)],\n",
    "                    [Token(i + len(stack), f) for i,f in enumerate(buf)],\n",
    "                    deps)\n",
    "    \n",
    "    ps.step(ps.tr2id[transition]) # covert action name to it numeric id\n",
    "    stack = [t.form for t in ps.stack] # collect the words\n",
    "    buf = [t.form for t in ps.buffer]\n",
    "    deps = [(a[0].form, a[1].form) for a in sorted(ps.arcs)]\n",
    "    assert stack == ex_stack, \\\n",
    "        f\"{transition} test resulted in stack {stack}, expected {ex_stack}\"\n",
    "    assert buf == ex_buf, \\\n",
    "        f\"{transition} test resulted in buffer {buf}, expected {ex_buf}\"\n",
    "    assert deps == ex_deps, \\\n",
    "        f\"{transition} test resulted in dependency list {deps}, expected {ex_deps}\"\n",
    "    print(f\"{transition} test passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-trace",
   "metadata": {},
   "source": [
    "Perform a few tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chemical-adoption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S test passed!\n",
      "LA test passed!\n",
      "RA test passed!\n"
     ]
    }
   ],
   "source": [
    "test_step(\"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
    "          [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [])\n",
    "test_step(\"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
    "          [\"ROOT\", \"cat\"], [\"sat\"], [(\"cat\", \"the\")])\n",
    "test_step(\"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
    "          [\"ROOT\", \"run\"], [], [(\"run\", \"fast\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-genesis",
   "metadata": {},
   "source": [
    "## Test parsing a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacterial-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Token(0, 'ROOT')\n",
    "\n",
    "def test_parse():\n",
    "    \"\"\"Simple tests for the PartialParse.parse function.\n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    sentence = [Token(i+1, f) for i,f in enumerate([\"parse\", \"this\", \"sentence\"])]\n",
    "    state = ParserState(stack=[ROOT], buffer=sentence)\n",
    "    dependencies = state.parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
    "    dependencies = [(a[0].form, a[1].form) for a in sorted(dependencies)]\n",
    "    expected = [('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this')]\n",
    "    assert dependencies == expected, \\\n",
    "        f\"parse test resulted in dependencies {dependencies}, expected {expected}\"\n",
    "    assert [t.form for t in sentence] == [\"parse\", \"this\", \"sentence\"], \\\n",
    "        f\"parse test failed: the input sentence should not be modified\"\n",
    "    print(\"parse test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prepared-architecture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse test passed!\n"
     ]
    }
   ],
   "source": [
    "test_parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-strike",
   "metadata": {},
   "source": [
    "# Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-economics",
   "metadata": {},
   "source": [
    "We are now going to train a neural network to predict, given the state of the stack, buffer, and dependencies, which transition should be applied next.<br/>\n",
    "First, the model extracts a feature vector representing the current state. We will be using the feature set presented in the  paper by  Chen and Manning (2014), \"A Fast and Accurate Dependency Parser using Neural Networks\", https://nlp.stanford.edu/pubs/emnlp2014-depparser.pdf.\n",
    "\n",
    "The method `ParserState.extract_features()` to extract these features is  implemented in `parser_state.py`.\n",
    "These features consist of a triple:\n",
    "- a list of tokens (e.g., the last word in the stack, first word in the buffer, dependent of the second-to-last word in the stack if there is one, etc.).\n",
    "- a list of POS tags for the same tokens\n",
    "- a list of DEPRELs for the same tokens.\n",
    "Each element is represented by an integer ids, and therefore it consists of:\n",
    "\n",
    "$$[ [w_1,w_2,...,w_m], [p_1, p_2,...,p_m], [d_1, d_2,..., d_m] ]$$\n",
    "\n",
    "where $m$ is the number of features and each $0 ≤ w_i < |V|$ is the index of a token in the vocabulary ($|V|$ is the vocabulary size) and similarly for $p_i$ and $d_i$.\n",
    "Then our network looks up an embedding for each word and tags and concatenates them into a single input vector:\n",
    "$$x = [E_{w_1},...,E_{w_m},Ep_{p_1},...,Ep_{p_m},Ed_{d_1},...,Ed_{d_m}] ∈ \\mathbb{R}^{(d+d_p+d_d)m}$$\n",
    "where $E ∈ \\mathbb{R}^{|V|×d}$ is an embedding matrix with each row $E_w$ as the vector for a particular word $w$, and similarly $Ep$ and $Ed$ for tags, with dimesions respectively $d_p$ and $d_d$.<br/>\n",
    "We then compute our prediction as:\n",
    "$$h = ReLU(xW + b_1)$$\n",
    "$$l = hU + b_2$$\n",
    "$$\\hat{y} = softmax(l)$$\n",
    "where $h$ is referred to as the hidden layer, $l$ is referred to as the logits, $\\hat{y}$ is referred to as the predictions, and $ReLU(z) = max(z, 0)$. We will train the model to minimize cross-entropy loss:\n",
    "$$J(θ) = CE(y,\\hat{y}) = \\sum_{i=1}^a{−y_i log \\hat{y}_i}$$\n",
    "where $a$ is the number of possible parser actions.\n",
    "To compute the loss for the training set, we average this $J(θ)$ across all training examples.\n",
    "We will use UAS score as our evaluation metric. UAS refers to Unlabeled Attachment Score, which is computed as the ratio between number of correctly predicted dependencies and the number of total dependencies irrespective of the relations.\n",
    "\n",
    "In `model.py` you will find skeleton code to implement this simple neural network using Keras. Complete the `__init__` methods to implement the model.\n",
    "\n",
    "Then complete the train for epoch and train functions. Finally execute python `run.py` to train your model and compute predictions on test data from Penn Treebank (annotated with Universal Dependencies), available in files `data/traing.gold.conll`, `data/dev.gold.conll` and `data/test.gold.conll`.\n",
    "\n",
    "##Note:##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-bread",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "portable-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import read_conll\n",
    "\n",
    "train_file = 'data/train.gold.conll'\n",
    "dev_file = 'data/dev.gold.conll'\n",
    "test_file = 'data/test.gold.conll'\n",
    "\n",
    "max_sent = 1000 # limit sentences during development\n",
    "\n",
    "train_sents = read_conll(train_file, max_sent=max_sent)\n",
    "dev_sents = read_conll(dev_file, max_sent=max_sent//2)\n",
    "test_sents = read_conll(test_file, max_sent=max_sent//2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inside-steps",
   "metadata": {},
   "source": [
    "## Create the parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "modern-valentine",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from parser import Parser\n",
    "\n",
    "parser = Parser(train_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hundred-missouri",
   "metadata": {},
   "source": [
    "## Convert to numeric vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "civilian-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = parser.vectorize(train_sents)\n",
    "dev_vectors = parser.vectorize(dev_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-centre",
   "metadata": {},
   "source": [
    "## Build the training set\n",
    "\n",
    "The implementation considers 18 tokens (i.e. $m=18$), more precisely:\n",
    "  - top 3 from stack, first 3 from buffer                             \n",
    "  - the following children of top 2 stack tokens:                      \n",
    "             lc[0], rc[0], lc[1], rc[1], llc[0], rrc[0]                       \n",
    "The deprels of the following children of the top 2 stack tokens are considered:         \n",
    "             lc[0], rc[0], lc[1], rc[1], llc[0], rrc[0]                         \n",
    "\n",
    "A parser state is represented by a triple of such features:<br/>\n",
    "[list of form ids], [list of POS ids], [list of DEPREL ids].                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sustainable-expansion",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 728.27it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 646.93it/s]\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = parser.create_features(train_vectors)\n",
    "dev_x, dev_y = parser.create_features(dev_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-possibility",
   "metadata": {},
   "source": [
    "Show sample of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "downtown-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([2, 0, 1963, 286, 4281, 249, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "  [2, 0, 13, 38, 32, 30, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "  [2, 2, 2, 2, 2, 2, 16, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_x[3], dev_y[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-progressive",
   "metadata": {},
   "source": [
    "## Build the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spiritual-appreciation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.data import Dataset\n",
    "\n",
    "ds_train = Dataset.from_tensor_slices((train_x, train_y)).shuffle(1000).batch(32)\n",
    "ds_dev = Dataset.from_tensor_slices((dev_x, dev_y)).shuffle(1000).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-segment",
   "metadata": {},
   "source": [
    "## Load the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "moral-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install glove-python-binary\n",
    "from glove import Glove\n",
    "\n",
    "glove_path = 'data/en-cw.txt'\n",
    "\n",
    "glove_embeddings = Glove.load_stanford(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-messaging",
   "metadata": {},
   "source": [
    "## Prepare embedding matrix\n",
    "Trimmed to the parser vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "extended-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(parser.tok2id)\n",
    "embedding_dim = glove_embeddings.word_vectors.shape[1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Fill the matrix with Glove embeddings\n",
    "embedding_matrix = np.random.uniform(-1, 1, (num_tokens, embedding_dim))\n",
    "for word, i in parser.tok2id.items():\n",
    "    idx = glove_embeddings.dictionary.get(word)\n",
    "    if idx is not None:\n",
    "        # Words not found in embedding index will be random.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = glove_embeddings.word_vectors[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-concentrate",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "recreational-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import ParserModel\n",
    "\n",
    "n_pos = len(parser.pos2id)\n",
    "n_tags = len(parser.dep2id)\n",
    "tag_size = 20 # size of embeddings for POS and DEPRELs \n",
    "n_actions = n_tags * 2 + 1 # L-d + R-d + 1\n",
    "hidden_size = 200\n",
    "\n",
    "model = ParserModel(embeddings=embedding_matrix,\n",
    "                    n_pos=n_pos, n_tags=n_tags, tag_size=tag_size,\n",
    "                    n_actions=n_actions, hidden_size=hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-language",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-namibia",
   "metadata": {},
   "source": [
    "Choose an optimizer: `SparseCategoricalCrossentropy` expects numerical categories.\n",
    "\n",
    "Select metrics to measure the loss and the accuracy of the model. These metrics accumulate the values over epochs and then print the overall result.\n",
    "\n",
    "Compile the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "golden-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import losses, metrics, optimizers\n",
    "\n",
    "model.compile(\n",
    "    # Optimizer\n",
    "    optimizer = optimizers.Adam(),\n",
    "    # Loss function to minimize\n",
    "    loss = losses.SparseCategoricalCrossentropy(name='train_loss'),\n",
    "    # List of metrics to monitor\n",
    "    metrics = [metrics.SparseCategoricalAccuracy(name='train UAS')],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-electricity",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "static-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "(None, 3, 18) (None, 1602)\n",
      "(None, 3, 18) (None, 1602)\n",
      "1518/1522 [============================>.] - ETA: 0s - loss: 0.8601 - train UAS: 0.7706(None, 3, 18) (None, 1602)\n",
      "1522/1522 [==============================] - 9s 6ms/step - loss: 0.8591 - train UAS: 0.7708 - val_loss: 0.3923 - val_train UAS: 0.8822\n",
      "Epoch 2/3\n",
      "1522/1522 [==============================] - 9s 6ms/step - loss: 0.3458 - train UAS: 0.8933 - val_loss: 0.2815 - val_train UAS: 0.9122\n",
      "Epoch 3/3\n",
      "1522/1522 [==============================] - 9s 6ms/step - loss: 0.2386 - train UAS: 0.9248 - val_loss: 0.2609 - val_train UAS: 0.9209\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "history = model.fit(ds_train, epochs=EPOCHS,\n",
    "                    validation_data=ds_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-reggae",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "insured-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........UAS: 75.98, LAS: 0.00\n"
     ]
    }
   ],
   "source": [
    "UAS, LAS = parser.parse(test_sents[:10], model)\n",
    "print(f'UAS: {UAS*100:.2f}, LAS: {LAS*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "visible-hampton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"parser_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " features_embedding (Feature  multiple                 268732    \n",
      " sEmbedding)                                                     \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  320600    \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  16683     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 606,015\n",
      "Trainable params: 606,015\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-spelling",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-youth",
   "metadata": {},
   "source": [
    "Print the output of a few sentences in CoNLL-U format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "monetary-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tNo\t_\tRB\t_\t_\t7\tadvmod\t_\t_\n",
      "2\t,\t_\t,\t_\t_\t7\tpunct\t_\t_\n",
      "3\tit\t_\tPRP\t_\t_\t7\tnsubj\t_\t_\n",
      "4\twas\t_\tVBD\t_\t_\t7\tcop\t_\t_\n",
      "5\tn't\t_\tRB\t_\t_\t7\tneg\t_\t_\n",
      "6\tBlack\t_\tNNP\t_\t_\t7\tcompound\t_\t_\n",
      "7\tMonday\t_\tNNP\t_\t_\t0\troot\t_\t_\n",
      "8\t.\t_\t.\t_\t_\t7\tpunct\t_\t_\n",
      "\n",
      "1\tBut\t_\tCC\t_\t_\t0\troot\t_\t_\n",
      "2\twhile\t_\tIN\t_\t_\t10\tmark\t_\t_\n",
      "3\tthe\t_\tDT\t_\t_\t7\tdet\t_\t_\n",
      "4\tNew\t_\tNNP\t_\t_\t7\tcompound\t_\t_\n",
      "5\tYork\t_\tNNP\t_\t_\t7\tcompound\t_\t_\n",
      "6\tStock\t_\tNNP\t_\t_\t7\tcompound\t_\t_\n",
      "7\tExchange\t_\tNNP\t_\t_\t10\tnsubj\t_\t_\n",
      "8\tdid\t_\tVBD\t_\t_\t10\taux\t_\t_\n",
      "9\tn't\t_\tRB\t_\t_\t10\tneg\t_\t_\n",
      "10\tfall\t_\tVB\t_\t_\t1\tadvcl\t_\t_\n",
      "11\tapart\t_\tRB\t_\t_\t12\tadvmod\t_\t_\n",
      "12\tFriday\t_\tNNP\t_\t_\t10\tnmod:tmod\t_\t_\n",
      "13\tas\t_\tIN\t_\t_\t33\tmark\t_\t_\n",
      "14\tthe\t_\tDT\t_\t_\t16\tdet\t_\t_\n",
      "15\tDow\t_\tNNP\t_\t_\t16\tcompound\t_\t_\n",
      "16\tJones\t_\tNNP\t_\t_\t33\tnsubj\t_\t_\n",
      "17\tIndustrial\t_\tNNP\t_\t_\t18\tcompound\t_\t_\n",
      "18\tAverage\t_\tNNP\t_\t_\t19\tnsubj\t_\t_\n",
      "19\tplunged\t_\tVBD\t_\t_\t33\tadvmod\t_\t_\n",
      "20\t190.58\t_\tCD\t_\t_\t21\tnummod\t_\t_\n",
      "21\tpoints\t_\tNNS\t_\t_\t19\tdobj\t_\t_\n",
      "22\t--\t_\t:\t_\t_\t19\tpunct\t_\t_\n",
      "23\tmost\t_\tJJS\t_\t_\t33\tdep\t_\t_\n",
      "24\tof\t_\tIN\t_\t_\t25\tcase\t_\t_\n",
      "25\tit\t_\tPRP\t_\t_\t23\tnmod\t_\t_\n",
      "26\tin\t_\tIN\t_\t_\t29\tcase\t_\t_\n",
      "27\tthe\t_\tDT\t_\t_\t29\tdet\t_\t_\n",
      "28\tfinal\t_\tJJ\t_\t_\t29\tamod\t_\t_\n",
      "29\thour\t_\tNN\t_\t_\t23\tnmod\t_\t_\n",
      "30\t--\t_\t:\t_\t_\t23\tpunct\t_\t_\n",
      "31\tit\t_\tPRP\t_\t_\t33\tnsubj\t_\t_\n",
      "32\tbarely\t_\tRB\t_\t_\t33\tadvmod\t_\t_\n",
      "33\tmanaged\t_\tVBD\t_\t_\t12\tnmod\t_\t_\n",
      "34\tto\t_\tTO\t_\t_\t35\tmark\t_\t_\n",
      "35\tstay\t_\tVB\t_\t_\t33\txcomp\t_\t_\n",
      "36\tthis\t_\tDT\t_\t_\t37\tdet\t_\t_\n",
      "37\tside\t_\tNN\t_\t_\t35\tdobj\t_\t_\n",
      "38\tof\t_\tIN\t_\t_\t39\tcase\t_\t_\n",
      "39\tchaos\t_\tNN\t_\t_\t37\tnmod\t_\t_\n",
      "40\t.\t_\t.\t_\t_\t1\tpunct\t_\t_\n",
      "\n",
      "1\tSome\t_\tDT\t_\t_\t0\t<ROOT>\t_\t_\n",
      "2\t``\t_\t``\t_\t_\t6\tpunct\t_\t_\n",
      "3\tcircuit\t_\tNN\t_\t_\t4\tcompound\t_\t_\n",
      "4\tbreakers\t_\tNNS\t_\t_\t6\tnsubj\t_\t_\n",
      "5\t''\t_\t''\t_\t_\t4\tpunct\t_\t_\n",
      "6\tinstalled\t_\tVBN\t_\t_\t0\t<ROOT>\t_\t_\n",
      "7\tafter\t_\tIN\t_\t_\t11\tcase\t_\t_\n",
      "8\tthe\t_\tDT\t_\t_\t11\tdet\t_\t_\n",
      "9\tOctober\t_\tNNP\t_\t_\t11\tcompound\t_\t_\n",
      "10\t1987\t_\tCD\t_\t_\t11\tnummod\t_\t_\n",
      "11\tcrash\t_\tNN\t_\t_\t6\tnmod\t_\t_\n",
      "12\tfailed\t_\tVBD\t_\t_\t0\t<ROOT>\t_\t_\n",
      "13\ttheir\t_\tPRP$\t_\t_\t15\tnmod:poss\t_\t_\n",
      "14\tfirst\t_\tJJ\t_\t_\t15\tamod\t_\t_\n",
      "15\ttest\t_\tNN\t_\t_\t12\tdobj\t_\t_\n",
      "16\t,\t_\t,\t_\t_\t12\tpunct\t_\t_\n",
      "17\ttraders\t_\tNNS\t_\t_\t18\tnsubj\t_\t_\n",
      "18\tsay\t_\tVBP\t_\t_\t0\t<ROOT>\t_\t_\n",
      "19\t,\t_\t,\t_\t_\t0\t<ROOT>\t_\t_\n",
      "20\tunable\t_\tJJ\t_\t_\t0\t<ROOT>\t_\t_\n",
      "21\tto\t_\tTO\t_\t_\t22\tmark\t_\t_\n",
      "22\tcool\t_\tVB\t_\t_\t20\txcomp\t_\t_\n",
      "23\tthe\t_\tDT\t_\t_\t25\tdet\t_\t_\n",
      "24\tselling\t_\tNN\t_\t_\t25\tcompound\t_\t_\n",
      "25\tpanic\t_\tNN\t_\t_\t22\tdobj\t_\t_\n",
      "26\tin\t_\tIN\t_\t_\t28\tcase\t_\t_\n",
      "27\tboth\t_\tDT\t_\t_\t28\tdet\t_\t_\n",
      "28\tstocks\t_\tNNS\t_\t_\t22\tnmod\t_\t_\n",
      "29\tand\t_\tCC\t_\t_\t22\tcc\t_\t_\n",
      "30\tfutures\t_\tNNS\t_\t_\t22\tconj\t_\t_\n",
      "31\t.\t_\t.\t_\t_\t20\tpunct\t_\t_\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7397260273972602, 0.0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(test_sents[:3], model, conllu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-schema",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "placed-greensboro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tMoscow\t_\tNNP\t_\t_\t2\tnsubj\t_\t_\n",
      "2\tsent\t_\tVBD\t_\t_\t0\t<ROOT>\t_\t_\n",
      "3\ttroops\t_\tNNS\t_\t_\t2\tdobj\t_\t_\n",
      "4\tto\t_\tIN\t_\t_\t5\tcase\t_\t_\n",
      "5\tAfghaninstan\t_\tNNP\t_\t_\t0\t<ROOT>\t_\t_\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex4 = read_conll('data/ex4.conll')\n",
    "parser.parse(ex4[0:1], model, conllu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elegant-comparative",
   "metadata": {},
   "source": [
    "`to Afghanistan` is correctly attached to `sent`, not to `troops`, which would be a `Prepositional Phrase Attachment Error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "retired-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tI\t_\tPR\t_\t_\t2\tnsubj\t_\t_\n",
      "2\tdisembarked\t_\tVBD\t_\t_\t0\t<ROOT>\t_\t_\n",
      "3\tand\t_\tCC\t_\t_\t2\tcc\t_\t_\n",
      "4\twas\t_\tVBD\t_\t_\t5\tauxpass\t_\t_\n",
      "5\theading\t_\tVBG\t_\t_\t0\t<ROOT>\t_\t_\n",
      "6\tto\t_\tIN\t_\t_\t8\tcase\t_\t_\n",
      "7\ta\t_\tDT\t_\t_\t8\tdet\t_\t_\n",
      "8\twedding\t_\tNN\t_\t_\t0\t<ROOT>\t_\t_\n",
      "9\tfearing\t_\tVBG\t_\t_\t0\t<ROOT>\t_\t_\n",
      "10\tfor\t_\tIN\t_\t_\t12\tcase\t_\t_\n",
      "11\tmy\t_\tPRP$\t_\t_\t12\tnmod:poss\t_\t_\n",
      "12\tdeath\t_\tNN\t_\t_\t9\tnmod\t_\t_\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.875, 0.0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(ex4[1:2], model, conllu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-detail",
   "metadata": {},
   "source": [
    "`fearing` is incorrectly attached to `wedding`: a `Verb Phrase Attachement Error`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tough-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tIt\t_\tPRP\t_\t_\t2\tnsubj\t_\t_\n",
      "2\tmakes\t_\tVBZ\t_\t_\t0\troot\t_\t_\n",
      "3\tme\t_\tPRP\t_\t_\t4\tnsubj\t_\t_\n",
      "4\twant\t_\tVBP\t_\t_\t2\txcomp\t_\t_\n",
      "5\tto\t_\tTO\t_\t_\t6\tmark\t_\t_\n",
      "6\trush\t_\tVB\t_\t_\t4\txcomp\t_\t_\n",
      "7\tout\t_\tIN\t_\t_\t6\tadvmod\t_\t_\n",
      "8\tand\t_\tCC\t_\t_\t7\tcc\t_\t_\n",
      "9\trescue\t_\tVB\t_\t_\t7\tconj\t_\t_\n",
      "10\tpeople\t_\tNN\t_\t_\t9\tdobj\t_\t_\n",
      "11\tfrom\t_\tIN\t_\t_\t12\tcase\t_\t_\n",
      "12\tdilemmas\t_\t_\t_\t_\t9\tnmod\t_\t_\n",
      "13\tof\t_\tIN\t_\t_\t16\tcase\t_\t_\n",
      "14\ttheir\t_\tPRP\t_\t_\t16\tnmod:poss\t_\t_\n",
      "15\town\t_\tJJ\t_\t_\t16\tamod\t_\t_\n",
      "16\tmaking\t_\tNN\t_\t_\t12\tnmod\t_\t_\n",
      "17\t.\t_\t.\t_\t_\t2\tpunct\t_\t_\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8235294117647058, 0.0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(ex4[2:3], model, conllu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-scott",
   "metadata": {},
   "source": [
    "`rescue` is incorrectly attached to `out`: a `Coordination Attachment Error`.\n",
    "\n",
    "`dilemmas` is incorrectly attached to `people` instead of `rescue`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "interesting-guitar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tBrian\t_\tNNP\t_\t_\t2\tnsubj\t_\t_\n",
      "2\thas\t_\tVBZ\t_\t_\t0\troot\t_\t_\n",
      "3\tbeen\t_\t_\t_\t_\t2\tdobj\t_\t_\n",
      "4\tone\t_\tCD\t_\t_\t3\tappos\t_\t_\n",
      "5\tof\t_\tIN\t_\t_\t9\tcase\t_\t_\n",
      "6\tthe\t_\tDT\t_\t_\t9\tdet\t_\t_\n",
      "7\tmost\t_\tJJ\t_\t_\t9\tamod\t_\t_\n",
      "8\tcrucial\t_\tJJ\t_\t_\t9\tamod\t_\t_\n",
      "9\telements\t_\tNNS\t_\t_\t4\tnmod\t_\t_\n",
      "10\tto\t_\tIN\t_\t_\t12\tcase\t_\t_\n",
      "11\tthe\t_\tDT\t_\t_\t12\tdet\t_\t_\n",
      "12\tsuccess\t_\tNN\t_\t_\t4\tnmod\t_\t_\n",
      "13\tof\t_\tIN\t_\t_\t14\tcase\t_\t_\n",
      "14\tMozilla\t_\tNNP\t_\t_\t12\tnmod\t_\t_\n",
      "15\t.\t_\t.\t_\t_\t2\tpunct\t_\t_\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5333333333333333, 0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(ex4[3:4], model, conllu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-arnold",
   "metadata": {},
   "source": [
    "`success` should attach to `crucial`, not to `one`, `Mozilla` should depend on `success`.\n",
    "\n",
    "Notice that `crucial -> success` is a non-projective dependency, which crosses the arc `one -> slements`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
