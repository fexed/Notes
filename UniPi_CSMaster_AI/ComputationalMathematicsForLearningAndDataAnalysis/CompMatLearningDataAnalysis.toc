\babel@toc {italian}{}\relax 
\babel@toc {italian}{}\relax 
\contentsline {section}{\numberline {0.1}Introduction}{2}{section.0.1}%
\contentsline {paragraph}{Exam}{2}{section*.2}%
\contentsline {paragraph}{Course}{2}{section*.3}%
\contentsline {paragraph}{Languages}{2}{section*.4}%
\contentsline {chapter}{\numberline {1}Numerical Analysis}{3}{chapter.1}%
\contentsline {section}{\numberline {1.1}Quick recap of linear algebra}{3}{section.1.1}%
\contentsline {paragraph}{Why a real valued function?}{4}{section*.5}%
\contentsline {paragraph}{Norms}{4}{section*.6}%
\contentsline {paragraph}{Orthogonal}{4}{section*.7}%
\contentsline {subparagraph}{Theorem}{4}{section*.8}%
\contentsline {paragraph}{Eigenvalues and eigenvectors}{4}{section*.9}%
\contentsline {paragraph}{Symmetry}{5}{section*.10}%
\contentsline {paragraph}{Spectral Theorem}{5}{section*.11}%
\contentsline {paragraph}{Quadratic form}{5}{section*.12}%
\contentsline {subparagraph}{Theorem}{5}{section*.13}%
\contentsline {paragraph}{Positive Semidefinite}{5}{section*.14}%
\contentsline {paragraph}{Recall theorem}{5}{section*.15}%
\contentsline {paragraph}{Generalization for complex matrices}{5}{section*.16}%
\contentsline {section}{\numberline {1.2}(Linear) Least Squares problems}{5}{section.1.2}%
\contentsline {paragraph}{Geometric View}{5}{section*.17}%
\contentsline {paragraph}{Solvability}{5}{section*.18}%
\contentsline {paragraph}{Polynomial Fitting}{6}{section*.19}%
\contentsline {subparagraph}{Statistical version}{6}{section*.20}%
\contentsline {paragraph}{Theory of Least-Squares Problems}{6}{section*.21}%
\contentsline {subparagraph}{Theorem}{6}{section*.22}%
\contentsline {paragraph}{Positive definite}{6}{section*.23}%
\contentsline {subparagraph}{Algorithm}{6}{section*.24}%
\contentsline {paragraph}{Pseudoinverse}{7}{section*.25}%
\contentsline {section}{\numberline {1.3}Conjugate Gradient}{8}{section.1.3}%
\contentsline {subsection}{\numberline {1.3.1}Krylov Spaces}{8}{subsection.1.3.1}%
\contentsline {subparagraph}{Observation}{8}{section*.26}%
\contentsline {subsection}{\numberline {1.3.2}Conjugate Gradient}{9}{subsection.1.3.2}%
\contentsline {paragraph}{Orthogonal Directions}{9}{section*.27}%
\contentsline {paragraph}{Change of variable}{9}{section*.28}%
\contentsline {section}{\numberline {1.4}SVD}{9}{section.1.4}%
\contentsline {paragraph}{Singular Value Decomposition}{9}{section*.29}%
\contentsline {subparagraph}{Geometric idea}{10}{section*.30}%
\contentsline {subparagraph}{Rectangular SVD}{10}{section*.31}%
\contentsline {subparagraph}{Thin SVD}{10}{section*.32}%
\contentsline {subparagraph}{Cost in Malab}{10}{section*.33}%
\contentsline {paragraph}{Properties}{10}{section*.34}%
\contentsline {section}{\numberline {1.5}Norms}{10}{section.1.5}%
\contentsline {subparagraph}{Properties of the Norms}{11}{section*.35}%
\contentsline {paragraph}{Eckart-Young Theorem}{11}{section*.36}%
\contentsline {paragraph}{Ranks}{11}{section*.37}%
\contentsline {paragraph}{Exercise}{11}{section*.38}%
\contentsline {subsection}{\numberline {1.5.1}SVD Approximation}{12}{subsection.1.5.1}%
\contentsline {subparagraph}{Best approximations}{12}{section*.39}%
\contentsline {paragraph}{Latent Semantic Analysis}{12}{section*.40}%
\contentsline {paragraph}{Principal Component Analysis}{12}{section*.41}%
\contentsline {subparagraph}{Usage of PCA}{13}{section*.42}%
\contentsline {section}{\numberline {1.6}QR}{13}{section.1.6}%
\contentsline {paragraph}{Householder Reflectors}{13}{section*.43}%
\contentsline {paragraph}{Geometric Picture}{13}{section*.44}%
\contentsline {paragraph}{Lemma}{13}{section*.45}%
\contentsline {paragraph}{Lemma}{13}{section*.46}%
\contentsline {paragraph}{Numerical problems}{14}{section*.47}%
\contentsline {paragraph}{QR Factorization}{14}{section*.48}%
\contentsline {paragraph}{Optimizations}{15}{section*.49}%
\contentsline {subparagraph}{Thin QR}{15}{section*.50}%
\contentsline {section}{\numberline {1.7}Least Squares}{16}{section.1.7}%
\contentsline {subsection}{\numberline {1.7.1}Least Squares with QR}{16}{subsection.1.7.1}%
\contentsline {paragraph}{Geometrical Viewpoint}{16}{section*.51}%
\contentsline {subsection}{\numberline {1.7.2}Least Squares with SVD}{17}{subsection.1.7.2}%
\contentsline {paragraph}{Special Solution}{17}{section*.52}%
\contentsline {section}{\numberline {1.8}Conditioning and Stability}{18}{section.1.8}%
\contentsline {paragraph}{Effect of noise in data}{18}{section*.53}%
\contentsline {paragraph}{Tikhonov Regularization/Ridge Regression}{18}{section*.54}%
\contentsline {paragraph}{Sensitivity or conditioning of a problem}{18}{section*.55}%
\contentsline {subsection}{\numberline {1.8.1}Condition Number}{18}{subsection.1.8.1}%
\contentsline {subparagraph}{Example}{18}{section*.56}%
\contentsline {paragraph}{Relative Change}{19}{section*.57}%
\contentsline {subsubsection}{Condition Numbers in Linear Systems}{19}{section*.58}%
\contentsline {paragraph}{Linear Equations}{19}{section*.59}%
\contentsline {subparagraph}{Theorem}{20}{section*.60}%
\contentsline {paragraph}{Least Squares Problem}{20}{section*.61}%
\contentsline {paragraph}{Theorem}{20}{section*.62}%
\contentsline {paragraph}{Condition Number}{20}{section*.63}%
\contentsline {paragraph}{Conditioning of Least Squares Problem}{20}{section*.64}%
\contentsline {subsection}{\numberline {1.8.2}Stability}{21}{subsection.1.8.2}%
\contentsline {paragraph}{Quick recap of Floating Point arithmetic}{21}{section*.65}%
\contentsline {paragraph}{Stability of Algorithms}{21}{section*.66}%
\contentsline {paragraph}{Error analysis}{21}{section*.67}%
\contentsline {subparagraph}{Example}{21}{section*.68}%
\contentsline {paragraph}{Backward Stability}{22}{section*.69}%
\contentsline {subparagraph}{Theorem}{22}{section*.70}%
\contentsline {paragraph}{Residuals and A-Posteriori Stability Checks}{22}{section*.71}%
\contentsline {subparagraph}{Theorem}{23}{section*.72}%
\contentsline {paragraph}{Stability of Matrix Products}{23}{section*.73}%
\contentsline {section}{\numberline {1.9}Algorithms for square linear systems $Ax=b$}{24}{section.1.9}%
\contentsline {subsection}{\numberline {1.9.1}Algorithms for solving linear systems}{24}{subsection.1.9.1}%
\contentsline {subsection}{\numberline {1.9.2}Iterative Methods}{24}{subsection.1.9.2}%
\contentsline {paragraph}{Idea}{24}{section*.74}%
\contentsline {paragraph}{Krylov Subspace}{24}{section*.75}%
\contentsline {paragraph}{}{25}{section*.76}%
\contentsline {subsubsection}{Arnoldi Algorithm}{25}{section*.77}%
\contentsline {paragraph}{Idea}{25}{section*.78}%
\contentsline {paragraph}{Factorization}{25}{section*.79}%
\contentsline {paragraph}{Termination}{26}{section*.80}%
\contentsline {paragraph}{Breakdown}{26}{section*.81}%
\contentsline {paragraph}{Theorem}{26}{section*.82}%
\contentsline {paragraph}{Convergence of Arnoldi}{26}{section*.83}%
\contentsline {paragraph}{Backward Stability}{26}{section*.84}%
\contentsline {paragraph}{Complexity of Arnoldi}{27}{section*.85}%
\contentsline {paragraph}{Eigenvalues}{27}{section*.86}%
\contentsline {subsubsection}{GMRES}{27}{section*.87}%
\contentsline {subparagraph}{Convergence of GMRES}{28}{section*.88}%
\contentsline {subsubsection}{Conjugate Gradient}{28}{section*.89}%
\contentsline {paragraph}{Lanczos}{28}{section*.90}%
\contentsline {paragraph}{Conjugate Gradient}{28}{section*.91}%
\contentsline {paragraph}{Krylov subspace relations}{29}{section*.92}%
\contentsline {paragraph}{Theorem}{29}{section*.93}%
\contentsline {paragraph}{Theorem}{30}{section*.94}%
\contentsline {paragraph}{Theorem}{30}{section*.95}%
\contentsline {paragraph}{Convergence of CG}{30}{section*.96}%
\contentsline {paragraph}{Linear Solvers}{31}{section*.97}%
\contentsline {subsection}{\numberline {1.9.3}Solving Sparse Linear Systems}{31}{subsection.1.9.3}%
\contentsline {paragraph}{Preconditioning}{31}{section*.98}%
\contentsline {paragraph}{Factorization}{31}{section*.99}%
\contentsline {subsection}{\numberline {1.9.4}Gaussian Elimination}{31}{subsection.1.9.4}%
\contentsline {paragraph}{Theorem}{31}{section*.100}%
\contentsline {paragraph}{Stability}{31}{section*.101}%
\contentsline {subparagraph}{Is LU plus pivoting stable?}{32}{section*.102}%
\contentsline {subparagraph}{Another problem}{32}{section*.103}%
\contentsline {subsection}{\numberline {1.9.5}Symmetric Gaussian Elimination}{32}{subsection.1.9.5}%
\contentsline {subparagraph}{Theorem}{32}{section*.104}%
\contentsline {subparagraph}{Lemma}{32}{section*.105}%
\contentsline {paragraph}{In practice}{33}{section*.106}%
\contentsline {subsection}{\numberline {1.9.6}Cholesky factorization}{33}{subsection.1.9.6}%
\contentsline {subparagraph}{Incomplete $LU$}{33}{section*.107}%
\contentsline {subparagraph}{Incomplete Cholesky}{33}{section*.108}%
\contentsline {chapter}{\numberline {2}Optimization}{34}{chapter.2}%
\contentsline {subparagraph}{Example: Linear estimation}{34}{section*.109}%
\contentsline {subparagraph}{Example: Low-rank approximation}{34}{section*.110}%
\contentsline {subparagraph}{Example: Support Vector Machines}{35}{section*.111}%
\contentsline {section}{\numberline {2.1}Optimization problems}{35}{section.2.1}%
\contentsline {paragraph}{Real-Valued functions}{35}{section*.112}%
\contentsline {paragraph}{Multi-objective optimization}{35}{section*.113}%
\contentsline {subsection}{\numberline {2.1.1}Optimization is hard}{36}{subsection.2.1.1}%
\contentsline {paragraph}{Optimization needs to be approximate}{36}{section*.114}%
\contentsline {paragraph}{Optimization is really hard}{36}{section*.115}%
\contentsline {paragraph}{Optimization at least possible}{36}{section*.116}%
\contentsline {subsection}{\numberline {2.1.2}Local Optimization}{37}{subsection.2.1.2}%
\contentsline {paragraph}{Optimally choosing the iterates}{37}{section*.117}%
\contentsline {subsection}{\numberline {2.1.3}Faster Local Optimization}{38}{subsection.2.1.3}%
\contentsline {paragraph}{To make it go faster, give it more information}{38}{section*.118}%
\contentsline {paragraph}{Dichotomic Search}{39}{section*.119}%
\contentsline {paragraph}{Extreme value theorem}{39}{section*.120}%
\contentsline {paragraph}{Fastest local optimization}{39}{section*.121}%
\contentsline {subsection}{\numberline {2.1.4}Measuring algorithms speed}{40}{subsection.2.1.4}%
\contentsline {paragraph}{Improving dichotomic search}{40}{section*.122}%
\contentsline {paragraph}{Newton's method}{40}{section*.123}%
\contentsline {subsection}{\numberline {2.1.5}Global optimization}{41}{subsection.2.1.5}%
\contentsline {paragraph}{Convexity}{41}{section*.124}%
\contentsline {section}{\numberline {2.2}Unconstrained optimization}{41}{section.2.2}%
\contentsline {paragraph}{Unconstraint global optimization}{41}{section*.125}%
\contentsline {paragraph}{Notation}{42}{section*.126}%
\contentsline {paragraph}{Tomography}{42}{section*.127}%
\contentsline {paragraph}{Simple Functions}{42}{section*.128}%
\contentsline {paragraph}{Directional/partial derivatives}{43}{section*.129}%
\contentsline {paragraph}{Jacobian}{43}{section*.130}%
\contentsline {paragraph}{Hessian}{43}{section*.131}%
\contentsline {subparagraph}{Theorem}{44}{section*.132}%
\contentsline {subsection}{\numberline {2.2.1}Optimality conditions}{44}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}Convex functions}{44}{subsection.2.2.2}%
\contentsline {subsection}{\numberline {2.2.3}Gradient Methods}{44}{subsection.2.2.3}%
\contentsline {paragraph}{Multivariate optimization algorithms}{44}{section*.133}%
\contentsline {paragraph}{Compactness}{44}{section*.134}%
\contentsline {paragraph}{First order model $\Leftrightarrow $ gradient method}{45}{section*.135}%
\contentsline {paragraph}{Step selection}{45}{section*.136}%
\contentsline {paragraph}{Gradient for quadratic functions}{45}{section*.137}%
\contentsline {subparagraph}{Analysis}{45}{section*.138}%
\contentsline {paragraph}{When linear convergence may not be enough}{45}{section*.139}%
\contentsline {subsubsection}{Gradient methods for general functions}{46}{section*.140}%
\contentsline {paragraph}{Notes on the stopping criterion}{46}{section*.141}%
\contentsline {paragraph}{Efficiency}{46}{section*.142}%
\contentsline {subsubsection}{Fixed Stepsize}{46}{section*.143}%
\contentsline {paragraph}{L-smoothness}{46}{section*.144}%
\contentsline {paragraph}{Stronger forms of convexity}{47}{section*.145}%
\contentsline {paragraph}{Convergence rate with strong convexity}{47}{section*.146}%
\contentsline {subsubsection}{Inexact Line Search}{47}{section*.147}%
\contentsline {paragraph}{Armijo}{47}{section*.148}%
\contentsline {paragraph}{Wolfe}{48}{section*.149}%
\contentsline {paragraph}{Armijo-Wolfe in practice}{48}{section*.150}%
\contentsline {subsection}{\numberline {2.2.4}More-Than-Gradient Methods}{48}{subsection.2.2.4}%
\contentsline {paragraph}{General descent methods}{48}{section*.151}%
\contentsline {paragraph}{Convergence of general descent methods}{48}{section*.152}%
\contentsline {subparagraph}{Zoutendijk's Theorem}{49}{section*.153}%
\contentsline {paragraph}{Newton's Method}{49}{section*.154}%
\contentsline {paragraph}{Globalized Newton}{49}{section*.155}%
\contentsline {subparagraph}{Theorem 1}{49}{section*.156}%
\contentsline {subparagraph}{Theorem 2}{49}{section*.157}%
\contentsline {subparagraph}{Theorem 3}{49}{section*.158}%
\contentsline {subparagraph}{Nonconvex case}{50}{section*.159}%
\contentsline {paragraph}{Trust Region}{50}{section*.160}%
\contentsline {paragraph}{Quasi-Newton}{50}{section*.161}%
\contentsline {paragraph}{DFP}{51}{section*.162}%
\contentsline {paragraph}{BFGS}{51}{section*.163}%
\contentsline {paragraph}{Conjugate gradient method for quadratic functions}{51}{section*.164}%
\contentsline {subparagraph}{Convergence and efficiency}{52}{section*.165}%
\contentsline {paragraph}{Deflected Gradients methods}{52}{section*.166}%
\contentsline {subparagraph}{Heavy Ball Gradient}{52}{section*.167}%
\contentsline {subparagraph}{Accelerated Gradient}{52}{section*.168}%
\contentsline {subsection}{\numberline {2.2.5}Less-Than-Gradient Methods}{52}{subsection.2.2.5}%
\contentsline {paragraph}{Stochastic Gradient}{52}{section*.169}%
\contentsline {paragraph}{Nondifferentiable functions}{53}{section*.170}%
\contentsline {paragraph}{Smooth methods fail on nonsmooth functions}{53}{section*.171}%
\contentsline {subsubsection}{Convex Nondifferentiable Functions}{53}{section*.172}%
\contentsline {paragraph}{Subgradients and subdifferentials}{53}{section*.173}%
\contentsline {paragraph}{Subgradients in $\mathbb {R}^n$}{53}{section*.174}%
\contentsline {paragraph}{Convex nondifferentiable optimization is hard}{53}{section*.175}%
\contentsline {subsubsection}{Subgradient methods}{54}{section*.176}%
\contentsline {paragraph}{Fundamental relationship}{54}{section*.177}%
\contentsline {paragraph}{Deflected Subgradient}{55}{section*.178}%
\contentsline {subsubsection}{Smoothed Gradient Methods}{55}{section*.179}%
\contentsline {subsubsection}{Bundle Methods}{55}{section*.180}%
\contentsline {paragraph}{Basic Idea}{55}{section*.181}%
\contentsline {paragraph}{Cutting Plane Algorithm}{56}{section*.182}%
\contentsline {subparagraph}{Why}{56}{section*.183}%
\contentsline {subparagraph}{Stabilizing}{56}{section*.184}%
\contentsline {section}{\numberline {2.3}Constrained Optimality and Duality}{56}{section.2.3}%
\contentsline {paragraph}{Constrained Optimization}{56}{section*.185}%
\contentsline {paragraph}{(Local) minima vs. optima}{56}{section*.186}%
\contentsline {subsection}{\numberline {2.3.1}First-Order Optimality Conditions}{57}{subsection.2.3.1}%
\contentsline {subsubsection}{Geometric Version}{57}{section*.187}%
\contentsline {paragraph}{The Tangent Cone}{57}{section*.188}%
\contentsline {paragraph}{Convex Sets}{57}{section*.189}%
\contentsline {subsubsection}{Algebraic Version}{58}{section*.190}%
\contentsline {paragraph}{Describing a Set via Functions}{58}{section*.191}%
\contentsline {paragraph}{Convex Sets out of Convex Functions}{58}{section*.192}%
\contentsline {paragraph}{Linear Equality Constraints}{59}{section*.193}%
\contentsline {paragraph}{Nonlinear Inequalities}{59}{section*.194}%
\contentsline {paragraph}{Farkas' Lemma}{59}{section*.195}%
\contentsline {paragraph}{KKT Conditions}{59}{section*.196}%
\contentsline {paragraph}{KKT Theorem}{60}{section*.197}%
\contentsline {subsection}{\numberline {2.3.2}Second-Order Optimization}{60}{subsection.2.3.2}%
\contentsline {paragraph}{Lagrangian Relaxation}{60}{section*.198}%
\contentsline {paragraph}{Specialized Lagrangians}{60}{section*.199}%
\contentsline {subsection}{\numberline {2.3.3}Constrained Optimization Algorithms}{61}{subsection.2.3.3}%
\contentsline {subsubsection}{Quadratic Problem with Linear Equality Constraints}{61}{section*.200}%
\contentsline {paragraph}{Equality-Constrained QP}{61}{section*.201}%
\contentsline {paragraph}{Active-Set Method}{61}{section*.202}%
\contentsline {subparagraph}{In practice}{62}{section*.203}%
\contentsline {subsubsection}{Projected Gradient Method}{62}{section*.204}%
\contentsline {paragraph}{Special Forms of Constraints}{62}{section*.205}%
\contentsline {paragraph}{Goldstein's Version}{63}{section*.206}%
\contentsline {paragraph}{Rosen's Version}{63}{section*.207}%
\contentsline {subsubsection}{Frank-Wolfe Method}{65}{section*.208}%
\contentsline {paragraph}{Stabilising the Frank-Wolfe Method}{65}{section*.209}%
\contentsline {paragraph}{Constrained Cutting Plane (Bundle)}{65}{section*.210}%
\contentsline {paragraph}{Frank-Wolfe++}{65}{section*.211}%
\contentsline {subsubsection}{Dual Methods}{65}{section*.212}%
\contentsline {paragraph}{Dual Method}{66}{section*.213}%
\contentsline {subparagraph}{Decomposition}{66}{section*.214}%
\contentsline {subsubsection}{Barrier Methods}{66}{section*.215}%
\contentsline {paragraph}{Barrier Function and Central Path}{66}{section*.216}%
\contentsline {paragraph}{Primal-Dual Interior-Point (Barrier) Method}{67}{section*.217}%
