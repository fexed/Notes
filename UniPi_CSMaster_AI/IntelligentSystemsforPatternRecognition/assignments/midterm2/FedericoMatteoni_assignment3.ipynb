{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FedericoMatteoni_assignment3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### Assignment 3\n",
        "###### Federico Matteoni\n",
        "\n",
        "The task required for this assignment is the implementation from scratch of a RBM.\n",
        "This is achieved in the following block of code."
      ],
      "metadata": {
        "id": "l3lSZDwG9Xfu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASpS9tBw78ln"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def logistic(x):  # logistic function to be used in the RBM computations\n",
        "    return 1.0 / (1 + np.exp(-x))\n",
        "  \n",
        "\n",
        "class RBM:  # definition of the Restricted Boltzmann Machine\n",
        "    def __init__(self, hidden_units, visible_units):  # initialization\n",
        "        self.nh = hidden_units  # hidden units of the RBM\n",
        "        self.nv = visible_units  # visible units of the RBM\n",
        "        self.weights = np.random.uniform(-1/self.nv, 1/self.nv, (self.nv, self.nh))  # random initialization for the weights\n",
        "        self.bias_h = np.zeros(self.nh)  # bias initialized to zero for each hidden unit\n",
        "        self.bias_v = np.zeros(self.nv)  # bias initialized to zero for each visible unit\n",
        "        print(\"Built a RBM with \" + str(self.nv) + \" visible units and \" + str(self.nh) + \" hidden units\")\n",
        "\n",
        "\n",
        "    def train(self, Xtr, epochs = 100, learning_rate = 0.1):  # CD-1 training algorithm\n",
        "        n = 6000  # batch size\n",
        "\n",
        "        print(\"Training on \" + str(n) + \" random elements for \" + str(epochs) + \" epochs\")\n",
        "        for epoch in range(epochs):\n",
        "            # Clamp data\n",
        "            idx = np.random.uniform(low = 0, high = Xtr.shape[0], size=n).astype(int)\n",
        "            cXtr = Xtr[idx,:]\n",
        "\n",
        "            # Wake phase\n",
        "            # Hidden probability\n",
        "            h_prob = logistic(np.dot(cXtr, self.weights) + self.bias_h)\n",
        "            wake = np.dot(cXtr.T, h_prob)\n",
        "\n",
        "            # Dream phase\n",
        "            # Hidden states\n",
        "            h_state = h_prob > np.random.rand(n, self.nh)\n",
        "            # Reconstruction probability\n",
        "            reconstruction_data_prob = logistic(np.dot(h_state, self.weights.T) + self.bias_v)\n",
        "            # Reconstructed data\n",
        "            reconstruction_data = reconstruction_data_prob > np.random.rand(n, self.nv)\n",
        "            h_neg_prob = logistic(np.dot(reconstruction_data, self.weights) + self.bias_h)\n",
        "            dream = np.dot(reconstruction_data.T, h_neg_prob)\n",
        "\n",
        "            # Learning phase\n",
        "            error = np.sum((cXtr - reconstruction_data)**2)/n\n",
        "            dW = (wake - dream)/n\n",
        "            dBh = (np.sum(h_prob) - np.sum(h_neg_prob))/n\n",
        "            dBv = (np.sum(cXtr) - np.sum(reconstruction_data))/n\n",
        "            self.weights += learning_rate*dW\n",
        "            self.bias_h += learning_rate*dBh\n",
        "            self.bias_v += learning_rate*dBv\n",
        "            print(\"\\rError:\\t\" + \"{:.5f}\".format(error), end=\"\")\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "    def get_hidden_activations(self, Xtr):  # for the inference process\n",
        "        n = Xtr.shape[0]\n",
        "\n",
        "        print(\"Computing hidden activations for \" + str(n) + \" elements\")\n",
        "        h_states = np.ones((n, self.nh))\n",
        "\n",
        "        h_prob = logistic(np.dot(Xtr, self.weights) + self.bias_h)\n",
        "        h_states[:,:] = h_prob > np.random.rand(n, self.nh)\n",
        "        return h_states"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST data is loaded from the Keras library"
      ],
      "metadata": {
        "id": "C3zivfYl9WgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(Xtr, ytr), (Xts, yts) = mnist.load_data()"
      ],
      "metadata": {
        "id": "0xxdxQyo8EWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is then flattened"
      ],
      "metadata": {
        "id": "99B9rNvH93RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(x):\n",
        "    return x.reshape(x.shape[0], -1)\n",
        "\n",
        "\n",
        "print(str(Xtr.shape[0]) + \" images of \" + str(Xtr.shape[1]) + \"x\" + str(Xtr.shape[2]) + \" pixels =>\", end = \" \")\n",
        "Xtr = flatten(Xtr)\n",
        "Xts = flatten(Xts)\n",
        "print(str(Xtr.shape[0]) + \" vectors \" + str(Xtr.shape[1]) + \" elements\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wl8X0qSn911w",
        "outputId": "ff632bd4-9106-4618-b2db-bd7201db68f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 images of 28x28 pixels => 60000 vectors 784 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we create and train the RBM"
      ],
      "metadata": {
        "id": "vHwly2j9-EJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RBM = RBM(visible_units = (28*28), hidden_units = 500)\n",
        "RBM.train(Xtr, epochs=100, learning_rate=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scAmGUO2-ASh",
        "outputId": "d4cbf085-8105-4465-c457-92a41acfc974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Built a RBM with 784 visible units and 500 hidden units\n",
            "Training on 6000 random elements for 100 epochs\n",
            "Error:\t11336.78300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we compute the hidden activations that will be fed into the classifiers."
      ],
      "metadata": {
        "id": "y-pcZzbsALtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr_h = RBM.get_hidden_activations(Xtr)\n",
        "Xts_h = RBM.get_hidden_activations(Xts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fGZspZKAMK0",
        "outputId": "c884f3fb-8223-46e7-c842-ede3e492602b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing hidden activations for 60000 elements\n",
            "Computing hidden activations for 10000 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the classifier, I tested two common solutions.\n",
        "\n",
        "First, a MLP classifier from the Scikit-Learn library"
      ],
      "metadata": {
        "id": "MJXZv--L-LOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print(\"Building the MLP classifier\")\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(300,200,), learning_rate_init=0.001)\n",
        "print(\"Classifier training started\")\n",
        "classifier.fit(Xtr_h, ytr)\n",
        "print(\"Classifier training ended\")\n",
        "print(\"Gathering classifier predictions\")\n",
        "predicted = classifier.predict(Xts_h)\n",
        "print(\"Computing accuracy score\")\n",
        "accuracy = accuracy_score(yts, predicted)\n",
        "print(\"Accuracy on test set of {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLqkVjO4-HeJ",
        "outputId": "4c5f25e9-e63f-43cc-8a6f-d4980b2a9cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the MLP classifier\n",
            "Classifier training started\n",
            "Classifier training ended\n",
            "Gathering classifier predictions\n",
            "Computing accuracy score\n",
            "Accuracy on test set of 11.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The a K-NN classifier, from the Scikit-Learn library as well"
      ],
      "metadata": {
        "id": "ibm0ukv7-f4X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "print(\"Building the K-NN classifier\")\n",
        "classifier = KNeighborsClassifier(5)\n",
        "print(\"Classifier training started\")\n",
        "classifier.fit(Xtr_h, ytr)\n",
        "print(\"Classifier training ended\")\n",
        "print(\"Gathering classifier predictions\")\n",
        "predicted = classifier.predict(Xts_h)\n",
        "print(\"Computing accuracy score\")\n",
        "accuracy = accuracy_score(yts, predicted)\n",
        "print(\"Accuracy on test set of {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhTY-lXU-lS_",
        "outputId": "5a34df7f-9b05-4292-d0e4-2014a2ed119a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the K-NN classifier\n",
            "Classifier training started\n",
            "Classifier training ended\n",
            "Gathering classifier predictions\n",
            "Computing accuracy score\n",
            "Accuracy on test set of 10.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a comparison, I tried a random classifier too"
      ],
      "metadata": {
        "id": "3RYynKe8_jZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "\n",
        "print(\"Building the random classifier\")\n",
        "classifier = DummyClassifier(strategy='uniform')\n",
        "print(\"Classifier training started\")\n",
        "classifier.fit(Xtr_h, ytr)\n",
        "print(\"Classifier training ended\")\n",
        "print(\"Gathering classifier predictions\")\n",
        "predicted = classifier.predict(Xts_h)\n",
        "print(\"Computing accuracy score\")\n",
        "accuracy = accuracy_score(yts, predicted)\n",
        "print(\"Accuracy on test set of {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZM0M2Fz8_KZk",
        "outputId": "caa2d0e6-52a7-4423-ade6-687fe9b6cefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the random classifier\n",
            "Classifier training started\n",
            "Classifier training ended\n",
            "Gathering classifier predictions\n",
            "Computing accuracy score\n",
            "Accuracy on test set of 9.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The random classifier picks a class with uniform probability among the unique classes it has seen in the training set and assigns the selected class to the example during the prediction phase. In the MNIST dataset there are 10 classes, one for each digit, so a random classification will be correct in 10% of the cases, which is about the result of the random classifier.\n",
        "\n",
        "The MLP and K-NN classifiers, though, do not behave much better, with both achieving about 10% accuracy. This means that both models are as good as random guessing in classifying MNIST data using the hidden activations from my implementation of the RBM. This can have multiple causes: the CD-1 algorithm implemented by me could be flawed (and in general we know that it isn't the best state-of-the-art algorithm available), and also the experimental setup surely played a role in the low generalization capability of the models.\n",
        "\n",
        "Inspider by [this paper](https://christian-igel.github.io/paper/TRBMAI.pdf) I tried binarizing the MNIST data: each value has been set to 0 or 1 based on a threshold of 127. This is done in the preprocessing of the data: the `flatten` method"
      ],
      "metadata": {
        "id": "ZlnyaABP_s7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten(x):\n",
        "    x = np.where(x > 127, 1, 0)\n",
        "    return x.reshape(x.shape[0], -1)"
      ],
      "metadata": {
        "id": "hqyPQl-jFTwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training of the RBM is the same as before"
      ],
      "metadata": {
        "id": "22TDaE7xFdLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(Xtr, ytr), (Xts, yts) = mnist.load_data()\n",
        "print(str(Xtr.shape[0]) + \" images of \" + str(Xtr.shape[1]) + \"x\" + str(Xtr.shape[2]) + \" pixels =>\", end = \" \")\n",
        "Xtr = flatten(Xtr)\n",
        "Xts = flatten(Xts)\n",
        "print(str(Xtr.shape[0]) + \" vectors \" + str(Xtr.shape[1]) + \" elements\")\n",
        "RBM = RBM(visible_units = (28*28), hidden_units = 500)\n",
        "RBM.train(Xtr, epochs=100, learning_rate=0.01)\n",
        "\n",
        "Xtr_h = RBM.get_hidden_activations(Xtr)\n",
        "Xts_h = RBM.get_hidden_activations(Xts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7jugkWDFVXj",
        "outputId": "37e5ea29-4d22-4f42-d6db-156414d29665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 images of 28x28 pixels => 60000 vectors 784 elements\n",
            "Built a RBM with 784 visible units and 500 hidden units\n",
            "Training on 6000 random elements for 100 epochs\n",
            "Error:\t134.32850\n",
            "Computing hidden activations for 60000 elements\n",
            "Computing hidden activations for 10000 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classifiers too"
      ],
      "metadata": {
        "id": "IOD1hH1cHVWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building the MLP classifier\")\n",
        "classifier = MLPClassifier(hidden_layer_sizes=(300,200,), learning_rate_init=0.001)\n",
        "print(\"Classifier training started\")\n",
        "classifier.fit(Xtr_h, ytr)\n",
        "print(\"Classifier training ended\")\n",
        "print(\"Gathering classifier predictions\")\n",
        "predicted = classifier.predict(Xts_h)\n",
        "print(\"Computing accuracy score\")\n",
        "accuracy = accuracy_score(yts, predicted)\n",
        "print(\"Accuracy on test set of {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7pu8rojFgyK",
        "outputId": "418534e5-a42e-4aad-d863-93d347db8b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the MLP classifier\n",
            "Classifier training started\n",
            "Classifier training ended\n",
            "Gathering classifier predictions\n",
            "Computing accuracy score\n",
            "Accuracy on test set of 16.43%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building the K-NN classifier\")\n",
        "classifier = KNeighborsClassifier(5)\n",
        "print(\"Classifier training started\")\n",
        "classifier.fit(Xtr_h, ytr)\n",
        "print(\"Classifier training ended\")\n",
        "print(\"Gathering classifier predictions\")\n",
        "predicted = classifier.predict(Xts_h)\n",
        "print(\"Computing accuracy score\")\n",
        "accuracy = accuracy_score(yts, predicted)\n",
        "print(\"Accuracy on test set of {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0ZYfKLtHbg6",
        "outputId": "0b36f639-f451-43b4-c82a-1410d2e33bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the K-NN classifier\n",
            "Classifier training started\n",
            "Classifier training ended\n",
            "Gathering classifier predictions\n",
            "Computing accuracy score\n",
            "Accuracy on test set of 9.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building the random classifier\")\n",
        "classifier = DummyClassifier(strategy='uniform')\n",
        "print(\"Classifier training started\")\n",
        "classifier.fit(Xtr_h, ytr)\n",
        "print(\"Classifier training ended\")\n",
        "print(\"Gathering classifier predictions\")\n",
        "predicted = classifier.predict(Xts_h)\n",
        "print(\"Computing accuracy score\")\n",
        "accuracy = accuracy_score(yts, predicted)\n",
        "print(\"Accuracy on test set of {:.2f}%\".format(accuracy*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRLdqWZxHdnm",
        "outputId": "9cb51a82-43fb-489d-e5e5-d4c97e385d95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building the random classifier\n",
            "Classifier training started\n",
            "Classifier training ended\n",
            "Gathering classifier predictions\n",
            "Computing accuracy score\n",
            "Accuracy on test set of 10.03%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By binarizing the MNIST data, the MLP classifier increases its accuracy, while the K-NN classifier still has around 10% accuracy. This may be due to the K=5, or other factors.\n",
        "\n",
        "In both cases, a model selection process would probably increase the performance of both models. Using a Restricted Boltzmann Machine to preprocess the data, given a fair amount of finetuning to get the best results, may be useful in handling complex data by extracting, through the RBM, the important features and using this process as an automatic feature selection."
      ],
      "metadata": {
        "id": "bbyuuoZ5eqUL"
      }
    }
  ]
}