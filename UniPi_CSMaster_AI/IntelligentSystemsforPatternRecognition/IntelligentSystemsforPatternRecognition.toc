\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Introduction}{2}{section.0.1}% 
\contentsline {paragraph}{Objectives}{2}{section*.2}% 
\contentsline {paragraph}{Methodology-Oriented Outcomes}{2}{section*.3}% 
\contentsline {paragraph}{Application-Oriented Outcomes}{2}{section*.4}% 
\contentsline {paragraph}{Prerequisites}{2}{section*.5}% 
\contentsline {section}{\numberline {0.2}Pattern Recognition}{2}{section.0.2}% 
\contentsline {paragraph}{Origins}{2}{section*.6}% 
\contentsline {paragraph}{Viola-Jones Algorithm}{2}{section*.7}% 
\contentsline {paragraph}{An historical view}{2}{section*.8}% 
\contentsline {paragraph}{A modern view}{2}{section*.9}% 
\contentsline {paragraph}{The deep learning Lego}{2}{section*.10}% 
\contentsline {subsection}{\numberline {0.2.1}Signals}{2}{subsection.0.2.1}% 
\contentsline {paragraph}{Formalization}{2}{section*.11}% 
\contentsline {paragraph}{Goals}{3}{section*.12}% 
\contentsline {paragraph}{Key Methods}{3}{section*.13}% 
\contentsline {subsubsection}{Time Domain Analysis}{3}{section*.14}% 
\contentsline {paragraph}{Mean}{3}{section*.15}% 
\contentsline {paragraph}{Autocovariance}{3}{section*.16}% 
\contentsline {paragraph}{Autocorrelation}{3}{section*.17}% 
\contentsline {subparagraph}{Autocorrelation plot}{3}{section*.18}% 
\contentsline {paragraph}{Cross-Correlation}{3}{section*.19}% 
\contentsline {subparagraph}{Normalized cross-correlation}{3}{section*.20}% 
\contentsline {paragraph}{Autoregressive Process}{4}{section*.21}% 
\contentsline {paragraph}{ARMA}{4}{section*.22}% 
\contentsline {paragraph}{Estimating Autoregressive Models}{4}{section*.23}% 
\contentsline {paragraph}{Comparing time series by AR}{4}{section*.24}% 
\contentsline {subsubsection}{Spectral Domain Analysis}{4}{section*.25}% 
\contentsline {paragraph}{Fourier Transform}{4}{section*.26}% 
\contentsline {paragraph}{Representing functions}{4}{section*.27}% 
\contentsline {paragraph}{Representing function in Complex space}{5}{section*.28}% 
\contentsline {paragraph}{Representing Discrete Time Series}{5}{section*.29}% 
\contentsline {paragraph}{Discrete Fourier Transform}{5}{section*.30}% 
\contentsline {paragraph}{Basic Spectral Quantities in SFT}{5}{section*.31}% 
\contentsline {paragraph}{DFT in Action}{5}{section*.32}% 
\contentsline {subsection}{\numberline {0.2.2}Image Processing}{5}{subsection.0.2.2}% 
\contentsline {subsubsection}{Descriptors}{5}{section*.33}% 
\contentsline {paragraph}{Machine Vision Applications}{5}{section*.34}% 
\contentsline {paragraph}{Key Questions}{5}{section*.35}% 
\contentsline {paragraph}{Image Histograms}{6}{section*.36}% 
\contentsline {paragraph}{Describing Local Image Properties}{6}{section*.37}% 
\contentsline {paragraph}{Localized Descriptors}{6}{section*.38}% 
\contentsline {paragraph}{SIFT}{6}{section*.39}% 
\contentsline {subparagraph}{Gaussian Filter of an Image}{6}{section*.40}% 
\contentsline {subsubsection}{Detectors}{7}{section*.41}% 
\contentsline {paragraph}{Visual Feature Detector}{7}{section*.42}% 
\contentsline {paragraph}{Edge Detection}{7}{section*.43}% 
\contentsline {subparagraph}{Edges and Gradients}{7}{section*.44}% 
\contentsline {subparagraph}{Prewitt operators}{8}{section*.45}% 
\contentsline {subparagraph}{Sobel Operator}{8}{section*.46}% 
\contentsline {paragraph}{Blob Detection}{8}{section*.47}% 
\contentsline {paragraph}{Affine Detectors}{9}{section*.48}% 
\contentsline {paragraph}{MSER}{9}{section*.49}% 
\contentsline {subparagraph}{Intuitions on MSER}{9}{section*.50}% 
\contentsline {paragraph}{Image Segmentation}{9}{section*.51}% 
\contentsline {subparagraph}{Ncut}{9}{section*.52}% 
\contentsline {subsubsection}{Conclusion}{10}{section*.53}% 
\contentsline {subsection}{\numberline {0.2.3}Wavelets}{10}{subsection.0.2.3}% 
\contentsline {paragraph}{Limitations of DFT}{10}{section*.54}% 
\contentsline {paragraph}{DWT}{11}{section*.55}% 
\contentsline {section}{\numberline {0.3}Generative and Graphical Models}{11}{section.0.3}% 
\contentsline {paragraph}{Generative Learning}{11}{section*.56}% 
\contentsline {paragraph}{Representation}{11}{section*.57}% 
\contentsline {paragraph}{Inference}{11}{section*.58}% 
\contentsline {paragraph}{Learning}{11}{section*.59}% 
\contentsline {paragraph}{Representation}{11}{section*.60}% 
\contentsline {paragraph}{In Deep Learning}{11}{section*.61}% 
\contentsline {paragraph}{Generate new knowledge}{12}{section*.62}% 
\contentsline {subsection}{\numberline {0.3.1}Probability Refresher}{12}{subsection.0.3.1}% 
\contentsline {subsubsection}{Inference}{12}{section*.63}% 
\contentsline {subsection}{\numberline {0.3.2}Graphical Models}{12}{subsection.0.3.2}% 
\contentsline {subsubsection}{Bayesian Networks}{12}{section*.64}% 
\contentsline {paragraph}{Conditional Probability Tables}{12}{section*.65}% 
\contentsline {paragraph}{Plate notation}{12}{section*.66}% 
\contentsline {paragraph}{Full-Plate Notation}{13}{section*.67}% 
\contentsline {subsubsection}{Markov Random Fields}{13}{section*.68}% 
\contentsline {subsection}{\numberline {0.3.3}Conditional Independence and Causality}{13}{subsection.0.3.3}% 
\contentsline {paragraph}{Local Markov Property}{13}{section*.69}% 
\contentsline {paragraph}{Markov Blanket}{13}{section*.70}% 
\contentsline {paragraph}{Joint Probability Factorization}{14}{section*.71}% 
\contentsline {paragraph}{Sampling of a Bayesian Network}{14}{section*.72}% 
\contentsline {subsection}{\numberline {0.3.4}Fundamental Bayesian Network Structures}{14}{subsection.0.3.4}% 
\contentsline {paragraph}{Tail to Tail}{14}{section*.73}% 
\contentsline {paragraph}{Head to Tail}{14}{section*.74}% 
\contentsline {paragraph}{Heat to Head}{15}{section*.75}% 
\contentsline {paragraph}{Derived Conditional Independence Relationships}{15}{section*.76}% 
\contentsline {paragraph}{$d$-separation}{15}{section*.77}% 
\contentsline {paragraph}{Markov Blanket}{15}{section*.78}% 
\contentsline {paragraph}{Are Directed Models Enough?}{15}{section*.79}% 
\contentsline {subsection}{\numberline {0.3.5}Markov Random Fields}{15}{subsection.0.3.5}% 
\contentsline {paragraph}{Joint Probability Factorization}{16}{section*.80}% 
\contentsline {subparagraph}{Clique}{16}{section*.81}% 
\contentsline {subparagraph}{Maximal Clique Factorization}{16}{section*.82}% 
\contentsline {paragraph}{Potential Functions}{16}{section*.83}% 
\contentsline {paragraph}{Boltzmann Distribution}{16}{section*.84}% 
\contentsline {subsubsection}{From Directed to Undirected}{16}{section*.85}% 
\contentsline {subsection}{\numberline {0.3.6}Learning Causation from Data}{17}{subsection.0.3.6}% 
\contentsline {paragraph}{Learning with Bayesian Network}{17}{section*.86}% 
\contentsline {paragraph}{Structure Learning Problem}{17}{section*.87}% 
\contentsline {subparagraph}{Structure Finding Approaches}{17}{section*.88}% 
\contentsline {subsection}{\numberline {0.3.7}Hidden Markov Models}{18}{subsection.0.3.7}% 
\contentsline {paragraph}{Sequence}{18}{section*.89}% 
\contentsline {paragraph}{Markov Chain}{18}{section*.90}% 
\contentsline {paragraph}{Observed Markov Chains}{18}{section*.91}% 
\contentsline {paragraph}{Hidden Markov Models}{18}{section*.92}% 
\contentsline {paragraph}{HMM Joint Probability Factorization}{19}{section*.93}% 
\contentsline {paragraph}{HMMs as Recursive Models}{19}{section*.94}% 
\contentsline {paragraph}{HMMs as Automata}{19}{section*.95}% 
\contentsline {subsection}{\numberline {0.3.8}Notable Inference Problems}{20}{subsection.0.3.8}% 
\contentsline {paragraph}{Smoothing}{20}{section*.96}% 
\contentsline {paragraph}{Learning}{20}{section*.97}% 
\contentsline {paragraph}{Optimal State Assigment}{20}{section*.98}% 
\contentsline {subsubsection}{Forward-Backward Algorithm}{20}{section*.99}% 
\contentsline {paragraph}{Sum-Product Message Passing}{21}{section*.100}% 
\contentsline {subsubsection}{Learning in HMM}{21}{section*.101}% 
\contentsline {paragraph}{Expectation-Maximization}{21}{section*.102}% 
\contentsline {paragraph}{Usefulness of HMMs}{22}{section*.103}% 
\contentsline {paragraph}{Decoding Problem}{22}{section*.104}% 
\contentsline {paragraph}{Viterbi Algorithm}{22}{section*.105}% 
\contentsline {subsection}{\numberline {0.3.9}Input-output Hidden Markov Models}{23}{subsection.0.3.9}% 
\contentsline {paragraph}{Bidirectional Input-Driven Models}{23}{section*.106}% 
\contentsline {paragraph}{Coupled HMMs}{24}{section*.107}% 
\contentsline {paragraph}{Dynamic Bayesian Networks}{24}{section*.108}% 
\contentsline {subsection}{\numberline {0.3.10}Markov Random Fields}{24}{subsection.0.3.10}% 
\contentsline {paragraph}{Likelihood Factorization}{24}{section*.109}% 
\contentsline {paragraph}{Factor Graphs}{25}{section*.110}% 
\contentsline {paragraph}{Sum-Product Inference}{25}{section*.111}% 
\contentsline {subparagraph}{Restricting to Conditional Probabilities}{25}{section*.112}% 
\contentsline {subparagraph}{Conditional Random Field}{25}{section*.113}% 
\contentsline {paragraph}{Feature Functions}{25}{section*.114}% 
\contentsline {paragraph}{Discriminative Learning in Graphical Models}{25}{section*.115}% 
\contentsline {paragraph}{CRF for Sequences}{26}{section*.116}% 
\contentsline {paragraph}{Generalization of HMM}{26}{section*.117}% 
\contentsline {paragraph}{Posterior Inference in LCRF}{26}{section*.118}% 
\contentsline {paragraph}{Training LCRF}{27}{section*.119}% 
\contentsline {paragraph}{Applications}{27}{section*.120}% 
\contentsline {subsection}{\numberline {0.3.11}Bayesian Learning and Variational Inference}{27}{subsection.0.3.11}% 
\contentsline {paragraph}{Latent Variables}{27}{section*.121}% 
\contentsline {paragraph}{Latent Spaces}{27}{section*.122}% 
\contentsline {paragraph}{Tractability}{28}{section*.123}% 
\contentsline {subsubsection}{Kullback-Leiber Divergence}{28}{section*.124}% 
\contentsline {paragraph}{Jensen Inequality}{28}{section*.125}% 
\contentsline {paragraph}{Example}{29}{section*.126}% 
\contentsline {paragraph}{Latent Dirichlet Allocation}{29}{section*.127}% 
\contentsline {paragraph}{Dirichlet distribution}{29}{section*.128}% 
\contentsline {paragraph}{LDA Generative Process}{30}{section*.129}% 
\contentsline {subparagraph}{Learning}{30}{section*.130}% 
\contentsline {paragraph}{Variational Inference}{30}{section*.131}% 
\contentsline {paragraph}{Variational Expectation-Maximization}{31}{section*.132}% 
\contentsline {subsection}{\numberline {0.3.12}Boltzmann Machines}{31}{subsection.0.3.12}% 
\contentsline {paragraph}{Stochastic Binary Neurons}{31}{section*.133}% 
\contentsline {paragraph}{Parallel Dynamics}{32}{section*.134}% 
\contentsline {paragraph}{Glauber Dynamics}{32}{section*.135}% 
\contentsline {subparagraph}{Boltzmann-Gibbs Distribution}{32}{section*.136}% 
\contentsline {paragraph}{Learning}{32}{section*.137}% 
\contentsline {paragraph}{Restricted Boltzmann Machines}{33}{section*.138}% 
\contentsline {paragraph}{Contrastive-Divergence Learning}{34}{section*.139}% 
\contentsline {subsection}{\numberline {0.3.13}Wrap Up}{34}{subsection.0.3.13}% 
\contentsline {paragraph}{Tractability}{34}{section*.140}% 
\contentsline {section}{\numberline {0.4}Sampling Methods}{35}{section.0.4}% 
\contentsline {paragraph}{Probability Recap}{35}{section*.141}% 
\contentsline {paragraph}{Sampling}{35}{section*.142}% 
\contentsline {paragraph}{Sampling Procedure as Distributions}{35}{section*.143}% 
\contentsline {subsection}{\numberline {0.4.1}Univariate Sampling}{36}{subsection.0.4.1}% 
\contentsline {subsection}{\numberline {0.4.2}Multivariate Sampling}{36}{subsection.0.4.2}% 
\contentsline {subparagraph}{Example}{36}{section*.144}% 
\contentsline {paragraph}{Gibbs Sampling Procedure}{37}{section*.145}% 
\contentsline {subparagraph}{$\mathaccent "705E\relax {p}(X)$ of Gibbs sampling}{37}{section*.146}% 
\contentsline {paragraph}{Markov Chain Monte Carlo Sampling Framework}{37}{section*.147}% 
\contentsline {section}{\numberline {0.5}Convolutional Neural Networks}{37}{section.0.5}% 
\contentsline {paragraph}{Deep Learning}{37}{section*.148}% 
\contentsline {paragraph}{CNNs}{37}{section*.149}% 
\contentsline {paragraph}{Dense Vector Multiplication}{38}{section*.150}% 
\contentsline {paragraph}{Stride}{39}{section*.151}% 
\contentsline {paragraph}{Activation Map Size}{39}{section*.152}% 
\contentsline {paragraph}{Zero Padding}{39}{section*.153}% 
\contentsline {paragraph}{Feature Map Transformation}{39}{section*.154}% 
\contentsline {paragraph}{Pooling}{39}{section*.155}% 
\contentsline {paragraph}{Convolutional Architecture}{39}{section*.156}% 
\contentsline {paragraph}{Filter Banks}{40}{section*.157}% 
\contentsline {paragraph}{CNNs as Sparse Neural Networks}{40}{section*.158}% 
\contentsline {subparagraph}{Strided Convolution}{40}{section*.159}% 
\contentsline {subparagraph}{Pooling}{41}{section*.160}% 
\contentsline {subparagraph}{Cross-Channel Pooling and Spatial Invariance}{41}{section*.161}% 
\contentsline {subparagraph}{Hierarchical Feature Organization}{41}{section*.162}% 
\contentsline {paragraph}{CNN Training}{41}{section*.163}% 
\contentsline {paragraph}{Deconvolution}{42}{section*.164}% 
\contentsline {paragraph}{ReLU}{42}{section*.165}% 
\contentsline {paragraph}{VGGNet}{42}{section*.166}% 
\contentsline {paragraph}{GoogLeNet}{42}{section*.167}% 
\contentsline {paragraph}{Batch Normalization}{43}{section*.168}% 
\contentsline {paragraph}{Deconvolution Network}{43}{section*.169}% 
\contentsline {paragraph}{Occlusions}{43}{section*.170}% 
\contentsline {paragraph}{Dense CNN}{43}{section*.171}% 
\contentsline {paragraph}{Causal Convolutions}{43}{section*.172}% 
\contentsline {paragraph}{Fully Convolutional Networks}{44}{section*.173}% 
\contentsline {paragraph}{Deconvolutional Architecture}{44}{section*.174}% 
\contentsline {section}{\numberline {0.6}Autoencoders}{44}{section.0.6}% 
\contentsline {paragraph}{Basic Autoencoder}{44}{section*.175}% 
\contentsline {paragraph}{Neural Autoencoders}{45}{section*.176}% 
\contentsline {subparagraph}{Sparse Autoencoder}{45}{section*.177}% 
\contentsline {subparagraph}{Denoising Autoencoders}{45}{section*.178}% 
\contentsline {subparagraph}{Contractive Autoencoder}{46}{section*.179}% 
\contentsline {subsection}{\numberline {0.6.1}Basic Autoencoders}{46}{subsection.0.6.1}% 
\contentsline {paragraph}{Unsupervised Layerwise Pretraining}{46}{section*.180}% 
\contentsline {paragraph}{Discriminative Fine Tuning}{48}{section*.181}% 
\contentsline {paragraph}{Multimodal DBM}{48}{section*.182}% 
\contentsline {section}{\numberline {0.7}Gated Recurrent Networks}{48}{section.0.7}% 
\contentsline {paragraph}{RNN Design}{48}{section*.183}% 
\contentsline {paragraph}{Supervised Recurrent Tasks}{49}{section*.184}% 
\contentsline {paragraph}{Vanilla RNN}{49}{section*.185}% 
\contentsline {paragraph}{Unfolding RNN}{49}{section*.186}% 
\contentsline {paragraph}{Exploding/Vanishing Gradients}{50}{section*.187}% 
\contentsline {paragraph}{Backward Propagation}{50}{section*.188}% 
\contentsline {subparagraph}{Gradient Clipping for Exploding Gradients}{51}{section*.189}% 
\contentsline {subparagraph}{Constant Error Propagation}{51}{section*.190}% 
\contentsline {subparagraph}{Activation functions}{51}{section*.191}% 
\contentsline {subparagraph}{Recurrent Weights}{51}{section*.192}% 
\contentsline {paragraph}{Gating Units}{51}{section*.193}% 
\contentsline {paragraph}{Forget Gate}{51}{section*.194}% 
\contentsline {subsection}{\numberline {0.7.1}LSTM Cell}{52}{subsection.0.7.1}% 
\contentsline {paragraph}{Long-Short Term Memory Cell}{52}{section*.195}% 
\contentsline {paragraph}{LSTM in Equations}{53}{section*.196}% 
\contentsline {paragraph}{Deep LSTM}{53}{section*.197}% 
\contentsline {paragraph}{Training LSTM}{53}{section*.198}% 
\contentsline {paragraph}{Regularizing LSTM}{53}{section*.199}% 
\contentsline {paragraph}{Practicalities}{53}{section*.200}% 
\contentsline {subsection}{\numberline {0.7.2}GRU Cell}{54}{subsection.0.7.2}% 
\contentsline {paragraph}{Gated Recurrent Unit}{54}{section*.201}% 
\contentsline {subsection}{\numberline {0.7.3}Applications}{54}{subsection.0.7.3}% 
\contentsline {paragraph}{Bidirectional LSTM}{54}{section*.202}% 
\contentsline {paragraph}{Language Modeling}{54}{section*.203}% 
\contentsline {paragraph}{More Differentiable Compositions}{55}{section*.204}% 
\contentsline {subsection}{\numberline {0.7.4}Advanced Topics}{55}{subsection.0.7.4}% 
\contentsline {paragraph}{Language Modeling - Dynamic Evaluation}{55}{section*.205}% 
\contentsline {paragraph}{Compression and Online Adaptation}{55}{section*.206}% 
\contentsline {subsubsection}{Seq2Seq Models}{55}{section*.207}% 
\contentsline {paragraph}{Sequence Transduction}{56}{section*.208}% 
\contentsline {paragraph}{Attention}{57}{section*.209}% 
\contentsline {paragraph}{Generalized Relevance}{58}{section*.210}% 
\contentsline {paragraph}{Hard Attention}{59}{section*.211}% 
\contentsline {paragraph}{Transformers}{59}{section*.212}% 
\contentsline {paragraph}{Self Attention}{59}{section*.213}% 
\contentsline {subsection}{\numberline {0.7.5}Skipping State Updates}{60}{subsection.0.7.5}% 
\contentsline {paragraph}{Convolutional Seq2Seq}{60}{section*.214}% 
\contentsline {paragraph}{Hierarchical RNNs}{60}{section*.215}% 
\contentsline {subparagraph}{Zoneout Regularization}{60}{section*.216}% 
\contentsline {paragraph}{Clockwork RNN}{61}{section*.217}% 
\contentsline {paragraph}{Skip RNN}{61}{section*.218}% 
\contentsline {subsection}{\numberline {0.7.6}Hierarchical Networks}{62}{subsection.0.7.6}% 
\contentsline {paragraph}{Explicit Boundaries}{62}{section*.219}% 
\contentsline {paragraph}{Operations}{62}{section*.220}% 
\contentsline {paragraph}{Recap}{62}{section*.221}% 
\contentsline {section}{\numberline {0.8}Reservoir Computing}{62}{section.0.8}% 
\contentsline {paragraph}{Reservoir Computing}{63}{section*.222}% 
\contentsline {paragraph}{Architecture}{64}{section*.223}% 
\contentsline {paragraph}{Setup}{64}{section*.224}% 
\contentsline {paragraph}{Training}{64}{section*.225}% 
\contentsline {paragraph}{Reservoir}{64}{section*.226}% 
\contentsline {paragraph}{Echo State Property}{64}{section*.227}% 
\contentsline {paragraph}{Initialization}{64}{section*.228}% 
\contentsline {paragraph}{Dynamical Transient}{64}{section*.229}% 
\contentsline {paragraph}{ESN Training}{65}{section*.230}% 
\contentsline {section}{\numberline {0.9}Neural Reasoning}{65}{section.0.9}% 
\contentsline {paragraph}{Memory Networks General Idea}{65}{section*.231}% 
\contentsline {subsection}{\numberline {0.9.1}Memory Network}{65}{subsection.0.9.1}% 
\contentsline {paragraph}{Components}{65}{section*.232}% 
\contentsline {paragraph}{End-to-End Memory Networks}{66}{section*.233}% 
\contentsline {paragraph}{Memory Nets for Visual Question Answering with Attention}{66}{section*.234}% 
\contentsline {subsection}{\numberline {0.9.2}Neural Turing Machines}{67}{subsection.0.9.2}% 
\contentsline {paragraph}{Controller}{67}{section*.235}% 
\contentsline {paragraph}{Memory Read}{67}{section*.236}% 
\contentsline {paragraph}{Memory Write}{67}{section*.237}% 
\contentsline {paragraph}{NTM Attention Focusing}{68}{section*.238}% 
\contentsline {section}{\numberline {0.10}Unsupervised Learning}{69}{section.0.10}% 
\contentsline {paragraph}{The Problem}{69}{section*.239}% 
\contentsline {paragraph}{Why Generative?}{69}{section*.240}% 
\contentsline {paragraph}{Approaching the Problem from a DL Perspective}{69}{section*.241}% 
\contentsline {paragraph}{Learning With Fully Visible Information}{69}{section*.242}% 
\contentsline {paragraph}{NN With Latent Variables?}{70}{section*.243}% 
\contentsline {subparagraph}{Reparametrization}{71}{section*.244}% 
\contentsline {paragraph}{Variational Approximation}{71}{section*.245}% 
\contentsline {subsection}{\numberline {0.10.1}Variational Autoencoder}{71}{subsection.0.10.1}% 
\contentsline {paragraph}{Training}{72}{section*.246}% 
\contentsline {paragraph}{VAE Final Loss}{72}{section*.247}% 
\contentsline {subparagraph}{Information Theoretic Interpretation}{72}{section*.248}% 
\contentsline {paragraph}{Testing}{72}{section*.249}% 
\contentsline {paragraph}{VAE vs Denoising/Contractive AE}{72}{section*.250}% 
\contentsline {paragraph}{Conditional Generation (CVAE)}{72}{section*.251}% 
\contentsline {subsection}{\numberline {0.10.2}Implicit Models}{73}{subsection.0.10.2}% 
\contentsline {paragraph}{Distribution Learning vs Learning to Sample}{73}{section*.252}% 
\contentsline {paragraph}{The GAN Catch}{73}{section*.253}% 
\contentsline {paragraph}{Hard Two-Player Game}{74}{section*.254}% 
\contentsline {paragraph}{Wasserstein Distance Models}{74}{section*.255}% 
\contentsline {paragraph}{DCGAN Architecture}{74}{section*.256}% 
\contentsline {paragraph}{Progressive GAN}{74}{section*.257}% 
\contentsline {paragraph}{Conditional Generation}{75}{section*.258}% 
\contentsline {paragraph}{Best of two worlds}{75}{section*.259}% 
\contentsline {paragraph}{Training AAE}{75}{section*.260}% 
\contentsline {paragraph}{AEE Style Transfer}{75}{section*.261}% 
\contentsline {paragraph}{AEE Semi-Supervised Learning}{75}{section*.262}% 
\contentsline {section}{\numberline {0.11}Continual Learning}{76}{section.0.11}% 
\contentsline {paragraph}{Continual Learning}{76}{section*.263}% 
\contentsline {paragraph}{Stability-plasticity dilemma}{76}{section*.264}% 
\contentsline {paragraph}{}{76}{section*.265}% 
\contentsline {paragraph}{Formally}{76}{section*.266}% 
\contentsline {paragraph}{Benchmarks}{76}{section*.267}% 
\contentsline {paragraph}{Scenarios}{76}{section*.268}% 
\contentsline {paragraph}{Common Baselines}{77}{section*.269}% 
\contentsline {paragraph}{Random Replay}{77}{section*.270}% 
\contentsline {section}{\numberline {0.12}Reinforcement Learning}{77}{section.0.12}% 
\contentsline {subsection}{\numberline {0.12.1}Fundamentals}{77}{subsection.0.12.1}% 
\contentsline {paragraph}{Rewards}{77}{section*.271}% 
\contentsline {paragraph}{Sequential Decision Making}{77}{section*.272}% 
\contentsline {paragraph}{Agent and Environment}{77}{section*.273}% 
\contentsline {paragraph}{History and State}{78}{section*.274}% 
\contentsline {paragraph}{Environment State}{78}{section*.275}% 
\contentsline {paragraph}{Agent State}{78}{section*.276}% 
\contentsline {paragraph}{Information (Markov) State}{78}{section*.277}% 
\contentsline {paragraph}{Observability}{78}{section*.278}% 
\contentsline {subsection}{\numberline {0.12.2}Components}{78}{subsection.0.12.2}% 
\contentsline {subparagraph}{Policy}{78}{section*.279}% 
\contentsline {subparagraph}{Value Function}{78}{section*.280}% 
\contentsline {subparagraph}{Model}{79}{section*.281}% 
\contentsline {paragraph}{Characterizing RL Agents}{79}{section*.282}% 
\contentsline {subsection}{\numberline {0.12.3}Problems}{79}{subsection.0.12.3}% 
\contentsline {paragraph}{Learning vs Planning}{79}{section*.283}% 
\contentsline {paragraph}{Exploration vs Exploitation}{79}{section*.284}% 
\contentsline {paragraph}{Prediction vs Control}{79}{section*.285}% 
\contentsline {paragraph}{Overview}{79}{section*.286}% 
