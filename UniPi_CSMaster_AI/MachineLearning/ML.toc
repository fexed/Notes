\babel@toc {italian}{}
\babel@toc {italian}{}
\contentsline {section}{\numberline {0.1}Machine Learning}{2}{section.0.1}% 
\contentsline {paragraph}{Luxury or necessity?}{2}{section*.2}% 
\contentsline {paragraph}{Learning}{2}{section*.3}% 
\contentsline {paragraph}{When can we use ML?}{2}{section*.4}% 
\contentsline {paragraph}{Definition}{2}{section*.5}% 
\contentsline {paragraph}{Data}{2}{section*.6}% 
\contentsline {paragraph}{Task}{2}{section*.7}% 
\contentsline {subsection}{\numberline {0.1.1}Supervised learning}{3}{subsection.0.1.1}% 
\contentsline {subsection}{\numberline {0.1.2}Unsupervised learning}{3}{subsection.0.1.2}% 
\contentsline {subsection}{\numberline {0.1.3}Learning algorithm}{3}{subsection.0.1.3}% 
\contentsline {paragraph}{Learning}{3}{section*.8}% 
\contentsline {paragraph}{Inductive bias}{4}{section*.9}% 
\contentsline {paragraph}{Version Space}{4}{section*.10}% 
\contentsline {paragraph}{Unbiased Learner}{4}{section*.11}% 
\contentsline {paragraph}{Why prefer the search bias?}{4}{section*.12}% 
\contentsline {paragraph}{Loss}{4}{section*.13}% 
\contentsline {paragraph}{Learning and generalization}{4}{section*.14}% 
\contentsline {paragraph}{Phases}{5}{section*.15}% 
\contentsline {paragraph}{Machine Learning issues}{5}{section*.16}% 
\contentsline {subsection}{\numberline {0.1.4}Statistical Learning Theory}{5}{subsection.0.1.4}% 
\contentsline {subsubsection}{Vapnik-Chervonenkis dim and SLT}{5}{section*.17}% 
\contentsline {subsubsection}{Structural risk minimization}{6}{section*.18}% 
\contentsline {subsubsection}{Complexity control}{6}{section*.19}% 
\contentsline {subsection}{\numberline {0.1.5}Validation}{6}{subsection.0.1.5}% 
\contentsline {paragraph}{Confusion Matrix}{7}{section*.20}% 
\contentsline {paragraph}{ROC Curve}{7}{section*.21}% 
\contentsline {subsection}{\numberline {0.1.6}Design Cycle}{7}{subsection.0.1.6}% 
\contentsline {subsection}{\numberline {0.1.7}Misinterpretations}{8}{subsection.0.1.7}% 
\contentsline {section}{\numberline {0.2}Linear Models}{8}{section.0.2}% 
\contentsline {subsection}{\numberline {0.2.1}Univariate Linear Regression}{8}{subsection.0.2.1}% 
\contentsline {paragraph}{Least Mean Square}{8}{section*.22}% 
\contentsline {paragraph}{Notation}{8}{section*.23}% 
\contentsline {subsection}{\numberline {0.2.2}Classification}{9}{subsection.0.2.2}% 
\contentsline {subsection}{\numberline {0.2.3}Learning Algorithms}{9}{subsection.0.2.3}% 
\contentsline {paragraph}{Direct Approach with a normal equation}{10}{section*.24}% 
\contentsline {subsection}{\numberline {0.2.4}Gradient Descent}{10}{subsection.0.2.4}% 
\contentsline {subsubsection}{Batch version}{11}{section*.25}% 
\contentsline {subsubsection}{Online/Stochastic version}{11}{section*.26}% 
\contentsline {subsubsection}{Gradient Descent as error correction delta rule}{11}{section*.27}% 
\contentsline {subsection}{\numberline {0.2.5}Extending the linear model}{11}{subsection.0.2.5}% 
\contentsline {paragraph}{Inductive Bias}{11}{section*.28}% 
\contentsline {subparagraph}{Limitations}{11}{section*.29}% 
\contentsline {paragraph}{Extending}{11}{section*.30}% 
\contentsline {paragraph}{Linear Basis Expansion}{11}{section*.31}% 
\contentsline {subparagraph}{Ridge Regression}{12}{section*.32}% 
\contentsline {paragraph}{Learning Timing}{12}{section*.33}% 
\contentsline {subsection}{\numberline {0.2.6}K-NN}{13}{subsection.0.2.6}% 
\contentsline {paragraph}{Nearest}{13}{section*.34}% 
\contentsline {subparagraph}{Voronoi Diagram}{13}{section*.35}% 
\contentsline {paragraph}{Multiclass}{13}{section*.36}% 
\contentsline {subparagraph}{Weighted Distance}{13}{section*.37}% 
\contentsline {paragraph}{K-NN vs linear}{13}{section*.38}% 
\contentsline {subparagraph}{Bayes Error Rate}{13}{section*.39}% 
\contentsline {subparagraph}{Inductive bias of K-NN}{14}{section*.40}% 
\contentsline {subparagraph}{Limitations}{14}{section*.41}% 
\contentsline {section}{\numberline {0.3}Neural Networks}{15}{section.0.3}% 
\contentsline {subsection}{\numberline {0.3.1}Artificial Neuron}{15}{subsection.0.3.1}% 
\contentsline {subsubsection}{Perceptron}{15}{section*.42}% 
\contentsline {subparagraph}{Xor}{15}{section*.43}% 
\contentsline {paragraph}{Hidden Representation}{16}{section*.44}% 
\contentsline {paragraph}{Learning for one unit model}{16}{section*.45}% 
\contentsline {paragraph}{Perceptron Convergence theorem}{16}{section*.46}% 
\contentsline {subparagraph}{Preliminaries}{16}{section*.47}% 
\contentsline {subparagraph}{Proof}{17}{section*.48}% 
\contentsline {paragraph}{Differences}{17}{section*.49}% 
\contentsline {paragraph}{Activation functions}{18}{section*.50}% 
\contentsline {paragraph}{Neural Network}{19}{section*.51}% 
\contentsline {subparagraph}{Architectures}{19}{section*.52}% 
\contentsline {subparagraph}{Flexibility}{19}{section*.53}% 
\contentsline {subparagraph}{Universal approximation}{20}{section*.54}% 
\contentsline {subparagraph}{Expressive Power}{20}{section*.55}% 
\contentsline {subparagraph}{Learning Algorithm}{20}{section*.56}% 
\contentsline {subsection}{\numberline {0.3.2}Backpropagation Algorithm}{21}{subsection.0.3.2}% 
\contentsline {subparagraph}{Step (1)}{21}{section*.57}% 
\contentsline {paragraph}{Issues in training neural networks}{22}{section*.58}% 
\contentsline {subparagraph}{Starting values}{22}{section*.59}% 
\contentsline {subparagraph}{Multiple Minima}{22}{section*.60}% 
\contentsline {subparagraph}{Online/Batch}{22}{section*.61}% 
\contentsline {paragraph}{Batch}{22}{section*.62}% 
\contentsline {paragraph}{Online}{22}{section*.63}% 
\contentsline {subparagraph}{Learning rate}{23}{section*.64}% 
\contentsline {subparagraph}{Stopping criteria}{23}{section*.65}% 
\contentsline {subparagraph}{Overfitting and regularization}{23}{section*.66}% 
\contentsline {subparagraph}{Number of units}{24}{section*.67}% 
\contentsline {subparagraph}{Input scaling and output representation}{25}{section*.68}% 
\contentsline {section}{\numberline {0.4}Model Selection and Model Assessment}{25}{section.0.4}% 
\contentsline {subsection}{\numberline {0.4.1}Bias-Variance}{25}{subsection.0.4.1}% 
\contentsline {subsection}{\numberline {0.4.2}Motivations}{26}{subsection.0.4.2}% 
\contentsline {subsection}{\numberline {0.4.3}Validation}{26}{subsection.0.4.3}% 
\contentsline {paragraph}{Two aims}{26}{section*.69}% 
\contentsline {paragraph}{Grid Search}{27}{section*.70}% 
\contentsline {paragraph}{K-fold Cross Validation}{27}{section*.71}% 
\contentsline {paragraph}{Double cross-validation}{28}{section*.72}% 
\contentsline {paragraph}{Example with K-Fold CV}{28}{section*.73}% 
\contentsline {paragraph}{Particular cases}{28}{section*.74}% 
\contentsline {subparagraph}{Lucky/unlucky sampling}{28}{section*.75}% 
\contentsline {subparagraph}{Very few data}{28}{section*.76}% 
\contentsline {subparagraph}{When to stop?}{28}{section*.77}% 
\contentsline {subparagraph}{Early Stopping}{28}{section*.78}% 
\contentsline {subparagraph}{Random initialization}{29}{section*.79}% 
\contentsline {section}{\numberline {0.5}Statistical Learning Theory}{29}{section.0.5}% 
\contentsline {subsection}{\numberline {0.5.1}VC-dim}{29}{subsection.0.5.1}% 
\contentsline {paragraph}{Shattering}{29}{section*.80}% 
\contentsline {subparagraph}{Example}{29}{section*.81}% 
\contentsline {paragraph}{VC-Dimension}{29}{section*.82}% 
\contentsline {paragraph}{Analytical Bound}{29}{section*.83}% 
\contentsline {paragraph}{Remarks}{30}{section*.84}% 
\contentsline {subsection}{\numberline {0.5.2}Structural Risk Minimization}{30}{subsection.0.5.2}% 
\contentsline {paragraph}{Model selection}{30}{section*.85}% 
\contentsline {paragraph}{Use of the bound}{30}{section*.86}% 
\contentsline {section}{\numberline {0.6}Support Vector Machines}{31}{section.0.6}% 
\contentsline {paragraph}{Separating hyperplane}{31}{section*.87}% 
\contentsline {paragraph}{Support Vector}{31}{section*.88}% 
\contentsline {paragraph}{Computing the distance}{32}{section*.89}% 
\contentsline {paragraph}{Computing the margin}{32}{section*.90}% 
\contentsline {paragraph}{Quadratic Optimization Problem}{32}{section*.91}% 
\contentsline {subparagraph}{Primal Form}{33}{section*.92}% 
\contentsline {subparagraph}{Kuhn-Tucker Conditions}{33}{section*.93}% 
\contentsline {subparagraph}{How does this improve the generalization?}{34}{section*.94}% 
\contentsline {subparagraph}{Theorem (Vapnik)}{34}{section*.95}% 
\contentsline {paragraph}{An elegant approach}{34}{section*.96}% 
\contentsline {paragraph}{Soft margin SVM}{34}{section*.97}% 
\contentsline {subparagraph}{Kuhn-Tucker Conditions}{35}{section*.98}% 
\contentsline {subparagraph}{Solving the problem}{35}{section*.99}% 
\contentsline {subsection}{\numberline {0.6.1}High-Dimensional feature spaces}{35}{subsection.0.6.1}% 
\contentsline {paragraph}{Kernel}{35}{section*.100}% 
\contentsline {subparagraph}{Kernel Matrix}{36}{section*.101}% 
\contentsline {subparagraph}{Mercer's Theorem}{36}{section*.102}% 
\contentsline {subparagraph}{Properties}{36}{section*.103}% 
\contentsline {paragraph}{Wrapping up}{36}{section*.104}% 
\contentsline {paragraph}{Examples of kernels}{36}{section*.105}% 
\contentsline {subsection}{\numberline {0.6.2}SVM for non-linear regression}{36}{subsection.0.6.2}% 
\contentsline {paragraph}{$\epsilon $-insensitive loss function}{36}{section*.106}% 
\contentsline {paragraph}{Optimization problem}{37}{section*.107}% 
\contentsline {paragraph}{Wrapping up}{37}{section*.108}% 
\contentsline {paragraph}{Summary of the main characteristics}{38}{section*.109}% 
\contentsline {subparagraph}{Pros}{38}{section*.110}% 
\contentsline {subparagraph}{Cons}{38}{section*.111}% 
\contentsline {subparagraph}{In practice}{38}{section*.112}% 
\contentsline {subsection}{\numberline {0.6.3}Kernel Methods}{38}{subsection.0.6.3}% 
\contentsline {section}{\numberline {0.7}Bias-Variance}{39}{section.0.7}% 
\contentsline {subparagraph}{Recall of statistics}{39}{section*.113}% 
\contentsline {subsection}{\numberline {0.7.1}Bias-Variance Decomposition}{39}{subsection.0.7.1}% 
\contentsline {section}{\numberline {0.8}Ensemble Learning}{40}{section.0.8}% 
\contentsline {subsection}{\numberline {0.8.1}Bagging}{40}{subsection.0.8.1}% 
\contentsline {paragraph}{Bootstrap Aggregating}{40}{section*.114}% 
\contentsline {subsection}{\numberline {0.8.2}Boosting}{40}{subsection.0.8.2}% 
\contentsline {subsection}{\numberline {0.8.3}Feature Selection}{41}{subsection.0.8.3}% 
\contentsline {section}{\numberline {0.9}Applications}{41}{section.0.9}% 
\contentsline {subsection}{\numberline {0.9.1}Character recognition (classification)}{41}{subsection.0.9.1}% 
\contentsline {paragraph}{First approaches}{41}{section*.115}% 
\contentsline {paragraph}{Basic idea}{41}{section*.116}% 
\contentsline {subsection}{\numberline {0.9.2}Convolutional Neural Networks}{41}{subsection.0.9.2}% 
\contentsline {paragraph}{The name}{41}{section*.117}% 
\contentsline {paragraph}{2D convolution}{41}{section*.118}% 
\contentsline {paragraph}{Pooling}{42}{section*.119}% 
\contentsline {paragraph}{Overview}{43}{section*.120}% 
\contentsline {paragraph}{Advantages}{43}{section*.121}% 
\contentsline {paragraph}{How to use?}{43}{section*.122}% 
\contentsline {paragraph}{Modern CNNs}{43}{section*.123}% 
\contentsline {paragraph}{Parallelize linear operations on GPU}{43}{section*.124}% 
\contentsline {subsection}{\numberline {0.9.3}Deep Learning}{43}{subsection.0.9.3}% 
\contentsline {paragraph}{Framework}{43}{section*.125}% 
\contentsline {paragraph}{Implement}{44}{section*.126}% 
\contentsline {subparagraph}{Hierarchical Abstraction}{44}{section*.127}% 
\contentsline {paragraph}{Techniques}{44}{section*.128}% 
\contentsline {paragraph}{Do we \textit {need} many layers?}{44}{section*.129}% 
\contentsline {subparagraph}{Examples}{44}{section*.130}% 
\contentsline {subparagraph}{Theoretical Analysis}{44}{section*.131}% 
\contentsline {subparagraph}{Inductive Bias}{45}{section*.132}% 
\contentsline {subparagraph}{Curse of Dimensionality}{45}{section*.133}% 
\contentsline {subparagraph}{Practical issues}{45}{section*.134}% 
\contentsline {paragraph}{Representation Learning}{45}{section*.135}% 
\contentsline {subparagraph}{Basic ideas}{45}{section*.136}% 
\contentsline {subparagraph}{Obtaining or exploiting hidden representation}{45}{section*.137}% 
\contentsline {subsubsection}{Implementing Deep Learning}{45}{section*.138}% 
\contentsline {paragraph}{Pretraining}{45}{section*.139}% 
\contentsline {paragraph}{Autoencoders}{46}{section*.140}% 
\contentsline {subparagraph}{Layer-Wise Pretraining}{46}{section*.141}% 
\contentsline {subparagraph}{Needed?}{46}{section*.142}% 
\contentsline {paragraph}{Transfer Learning}{46}{section*.143}% 
\contentsline {subparagraph}{Example of pretrained networks}{46}{section*.144}% 
\contentsline {paragraph}{Distributed Representation}{47}{section*.145}% 
\contentsline {subparagraph}{Input or internal representation?}{47}{section*.146}% 
\contentsline {subparagraph}{Count the difference}{47}{section*.147}% 
\contentsline {subparagraph}{Sharing attributes}{47}{section*.148}% 
\contentsline {subparagraph}{Example}{47}{section*.149}% 
\contentsline {subparagraph}{Beyond Neural Networks}{47}{section*.150}% 
\contentsline {subparagraph}{Interpretability}{47}{section*.151}% 
\contentsline {paragraph}{Deep Distributed Representation}{47}{section*.152}% 
\contentsline {paragraph}{Deep Learning Techniques}{48}{section*.153}% 
\contentsline {paragraph}{Gradient Issues}{48}{section*.154}% 
\contentsline {subparagraph}{ReLU}{48}{section*.155}% 
\contentsline {paragraph}{Batch Normalization}{48}{section*.156}% 
\contentsline {paragraph}{Dropout}{49}{section*.157}% 
\contentsline {paragraph}{L1 Regularization}{49}{section*.158}% 
\contentsline {paragraph}{Adversarial Training}{49}{section*.159}% 
\contentsline {subsection}{\numberline {0.9.4}Random Weights Neural Networks}{49}{subsection.0.9.4}% 
\contentsline {paragraph}{Overview}{49}{section*.160}% 
\contentsline {paragraph}{Structure}{50}{section*.161}% 
\contentsline {paragraph}{Cover's Theorem}{50}{section*.162}% 
\contentsline {subsubsection}{Feedforward Ranbdomized Neural Networks}{50}{section*.163}% 
\contentsline {paragraph}{Pros and cons}{50}{section*.164}% 
\contentsline {paragraph}{Suitable for}{50}{section*.165}% 
\contentsline {subsection}{\numberline {0.9.5}Unsupervised Learning in Neural Networks}{50}{subsection.0.9.5}% 
\contentsline {paragraph}{Unsupervised Learning}{50}{section*.166}% 
\contentsline {subparagraph}{Tasks}{50}{section*.167}% 
\contentsline {paragraph}{Clustering}{51}{section*.168}% 
\contentsline {subparagraph}{Similarity Measures}{51}{section*.169}% 
\contentsline {paragraph}{Vector Quantization}{51}{section*.170}% 
\contentsline {subparagraph}{Goal}{51}{section*.171}% 
\contentsline {subparagraph}{Loss function}{52}{section*.172}% 
\contentsline {paragraph}{Online $K$-means}{52}{section*.173}% 
\contentsline {paragraph}{Softmax}{52}{section*.174}% 
\contentsline {subsubsection}{Self-Organizing Maps}{52}{section*.175}% 
\contentsline {paragraph}{Usage}{53}{section*.176}% 
\contentsline {paragraph}{Competitive learning}{53}{section*.177}% 
\contentsline {paragraph}{Learning algorithm}{53}{section*.178}% 
\contentsline {paragraph}{Topological Order}{53}{section*.179}% 
\contentsline {paragraph}{Visualization}{53}{section*.180}% 
\contentsline {subsection}{\numberline {0.9.6}Recurrent Neural Networks}{54}{subsection.0.9.6}% 
\contentsline {paragraph}{"The Neural Network That Remembers"}{54}{section*.181}% 
\contentsline {subsubsection}{Memory in neural networks}{54}{section*.182}% 
\contentsline {paragraph}{Input Delay Neural Networks}{54}{section*.183}% 
\contentsline {paragraph}{Recurrent Unit}{55}{section*.184}% 
\contentsline {paragraph}{State Transition System - Recurrent Model}{55}{section*.185}% 
\contentsline {paragraph}{Recurrent Neural Networks}{55}{section*.186}% 
\contentsline {paragraph}{Unfolding}{56}{section*.187}% 
\contentsline {paragraph}{Learning Algorithms}{56}{section*.188}% 
\contentsline {paragraph}{Advanced Models}{56}{section*.189}% 
\contentsline {paragraph}{Related Approaches}{56}{section*.190}% 
\contentsline {paragraph}{Towards ESN}{56}{section*.191}% 
\contentsline {subsection}{\numberline {0.9.7}Echo State Networks}{57}{subsection.0.9.7}% 
\contentsline {paragraph}{Recurrent plus deep}{57}{section*.192}% 
\contentsline {paragraph}{Toward Structured Domains}{57}{section*.193}% 
\contentsline {section}{\numberline {0.10}Structured Domains}{57}{section.0.10}% 
\contentsline {paragraph}{Why structured data?}{57}{section*.194}% 
\contentsline {paragraph}{Graph Representations}{57}{section*.195}% 
\contentsline {paragraph}{Classes}{58}{section*.196}% 
\contentsline {paragraph}{Overview of SD learning}{58}{section*.197}% 
\contentsline {paragraph}{Transductions}{58}{section*.198}% 
\contentsline {subsection}{\numberline {0.10.1}Recurrent/Recursive Approaches for Trees}{58}{subsection.0.10.1}% 
\contentsline {paragraph}{RRNs}{58}{section*.199}% 
\contentsline {paragraph}{Encoding network}{59}{section*.200}% 
\contentsline {paragraph}{Useful concepts}{59}{section*.201}% 
\contentsline {subsubsection}{Family of Models}{59}{section*.202}% 
\contentsline {paragraph}{Recursive Cascade Correlation}{59}{section*.203}% 
\contentsline {paragraph}{TreeESN}{60}{section*.204}% 
\contentsline {paragraph}{HTMM}{60}{section*.205}% 
\contentsline {paragraph}{Unsupervised Recursive Models}{60}{section*.206}% 
\contentsline {subsubsection}{Analysis}{61}{section*.207}% 
\contentsline {paragraph}{Assumptions and Open Problems}{61}{section*.208}% 
\contentsline {paragraph}{Towards Context}{61}{section*.209}% 
\contentsline {subsection}{\numberline {0.10.2}Other Approaches and Other Tasks}{62}{subsection.0.10.2}% 
\contentsline {subsubsection}{Kernel Methods for SD}{62}{section*.210}% 
\contentsline {paragraph}{Kernel Modularity}{62}{section*.211}% 
\contentsline {paragraph}{Marginalized Kernel}{62}{section*.212}% 
\contentsline {subsubsection}{Adaptive Kernels with Generative Models}{63}{section*.213}% 
\contentsline {subsubsection}{Statistical Relational Learning}{63}{section*.214}% 
\contentsline {subsubsection}{And others\ldots }{63}{section*.215}% 
\contentsline {section}{\numberline {0.11}Toward Research}{63}{section.0.11}% 
\contentsline {paragraph}{General Challenges}{63}{section*.216}% 
