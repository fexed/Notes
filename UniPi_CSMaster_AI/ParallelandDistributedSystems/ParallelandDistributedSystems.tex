\documentclass[10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{textcomp}
\graphicspath{ {./img/} }
\usepackage{listings}
\usepackage{makecell}
\usepackage{qtree}
\usepackage{pgfplots}
\usepackage{tikz}
\usepgflibrary{shapes}
\usepgfplotslibrary{fillbetween}
\definecolor{backcolour}{RGB}{255,255,255}
\definecolor{codegreen}{RGB}{27,168,11}
\definecolor{codeblue}{RGB}{35,35,205}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{205,35,56}
\lstdefinestyle{myPython}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{codeblue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\small\ttfamily,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=2pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	language=python
}
\newcommand*\triangled[1]{\tikz[baseline=(char.base)]{
            \node[regular polygon, regular polygon sides=3,draw,inner sep=1pt] (char) {#1};}}
            
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhead{}
\fancyfoot[L]{Telegram: \texttt{@fexed}}
\fancyfoot[R]{Github: \texttt{fexed}}
\begin{document}
\title{Parallel and Distributed Systems}
\author{Federico Matteoni}
\date{A.A. 2021/22}
\renewcommand*\contentsname{Index}

\maketitle
\tableofcontents
\pagebreak
\section{Introduction}
Prof.: Marco Danelutto
\paragraph{Program} Techniques for both parallel (single system, many core) and distributed (clusters of systems) systems.\\
Principles of parallel programming, structured parallel programming, parallel programming lab with standard and advanced (general purpose) \textbf{parallel programming frameworks}.
\paragraph{Technical Introduction} Each machine has more cores, perhaps multithreaded cores, but also GPUs (maybe with AVX support, which support operations floating point operations, \textbf{flops}, in a single instruction).\\
Between 1950 and 2000 the VLSI technology arised, integrated circuits which nowadays are in the order of 7nm (moving towards 2nm): printed circuits!\\
In origin, everything happened in a single clock cycle: fetch, decode, execute, write results in registers, with perhaps some memory accesses. The we had more complex control where in a single clock cycle we do just one of the phases (fetch \textit{or} decode \textit{or}\ldots), like a \textbf{pipeline}. More components are used the higher the frequency but the more power we need to dissipate, and we're coming to a point were the power we need to dissipate is too much and risks to melt the circuit, so we're reaching a \textbf{physical limit} in chip miniaturization. But temperature and computing power do not go in tandem: computing power is proportional to the chip dimensions, while temperature is proportional to the area. So it's better to put more processors (\textbf{cores}) and let them work together rather than make a bigger single processor.\\
An approach is to have few powerful cores and more less powerful cores (for example, in the Xeon Phi processors). Now, the processors follow this architecture, with the performance of a single core decreasing a bit with every generation but it's leveled by adding more cores.\\\\
Up to the 2000, during the single core era, code written years before will run faster on newer machines. Now, code could run slower due to not exploiting more cores and the decreasing in performance of the single core.\\With accelerators the situation is even more different: for example GPUs, accelerator for graphics libraries, with their own memory and specialized in certain kinds of operations. This can require the transfer of data between the accelerator's memory and the main memory, so the architecture of the accelerator is impactful on the overall performance.
\section{General Paradigms of Parallel Programming}
\paragraph{Parallelism} Execution of different parts of a program on different computing devices at the same time. We can imagine different flows of control (sequences of instruction) that all together are a program and are executed on different computing devices. Note that more flows on a singe computing device is \textbf{concurrency}, not parallelism.
\subparagraph{Concurrency} Similar concept: things that \textit{may} happen in parallel respecting the ordering between elements.
\paragraph{Computing Devices}\begin{list}{}{}
	\item \textbf{Threads}, implying shared memory
	\item \textbf{Processes}, implying separated memories
	\item \textbf{GPU Cores}
	\item \textbf{Hardware Layouts} on a FPGA (Field Programmable Gate Array) 
\end{list}
\paragraph{Sequential Task} A "program" with its own input data that can be executed by a single computing entity
\paragraph{Overhead} Actions required to organize the computation but that are note included in the program. For example: time spent in organizing the result. Basically, time spent orchestrating the parallel computation and not present in the sequential computation.
\paragraph{Speedup} Fundamental things that we're looking for, it's the ratio between the sequential time and the parallel time.$$\hbox{SpeedUp} = \frac{\hbox{Sequential time}}{\hbox{Parallel time}}$$
Assuming the best sequential time.\\
We have a slightly different measure, too
$$\hbox{Scalability}=\frac{\hbox{Parallel time with 1 computing device}}{\hbox{Parallel time}}$$
\paragraph{Stream of tasks} In some cases it's not important considering just one computation but may be useful considering more computations and we want to optimize a set of tasks.
\paragraph{Example: Book Translation} With $m=600$ pages, for example. Let's assume I can translate a page in $t_p = 0.5$h.\\
The sequential task is: take the book and spend time until I can deliver the translated book. The time is circa $m\cdot t_p = 300$h.\\
In parallel, ideally every page can be translated independently so I can split the book in two pieces of $\frac{m}{2}$ pages each (overhead), giving each half to a person. Both can translate at the same time, so ideally the time required is $\frac{m}{2}\cdot t_p$ for each, producing the translated halves. At this point I get the halves and produce the translated version (overhead). Ideally the time require is more or less $\frac{m}{2}\cdot t_p$, with "more or less" given by the time spent in splitting the book and reuniting the two halves. So the exact time is $T = T_{split} + \frac{m}{2} \cdot t_p + T_{merge}$.\\
What if the two person have different $t_p$s? For example $t_1 > t_2$. When a translator finishes, it spends some time synchronizing its work with me. With $nw$ "workers" (translators, in this instance) $T = nw\cdot T_{split} + nw\cdot T_{merge} + \frac{m}{nw}T_{work}$ with $nw\cdot T_{split}$ time spent delivering work to each worker and $nw\cdot T_{merge}$ time in merging each result.\\\\
Init is the time where every worker has work to do, and finish is the time where the last worker finished working. So the exact formula is with a single $T_{merge}$.\\
So $\frac{m}{nw}T_{work}$ is the time that needs to happen, found in the sequential computation too, whereas the other two factors are \textbf{overhead}.
$$\hbox{SpeedUp} = \frac{\hbox{Best sequential time}}{\hbox{Parallel time}}$$ but the parallel time depends on the $nw$ so $$\hbox{SpeedUp}(nw) = \frac{\hbox{Best sequential time}}{\hbox{Parallel time}(nw)} \simeq \frac{\not m\cdot \not t_p}{\frac{\not m}{nw}\cdot \not t_p} = nw$$ This not taking into account the overhead. It's a realistic assumption because usually the time splitting the work is very small. But we have to take into account that, in case it's not negligible. $$\hbox{SpeedUp}(nw) = \frac{m\cdot t_p}{\frac{m}{nw}\cdot t_p + \underline{nw\cdot T_{split} + T_{merge}}}$$
%TODO graph sp(nw) over (nw)
\paragraph{Example: Conference Bag} $T_{bag} = t_{bag} + t_{pen} + t_{paper} + t_{proc}$ and with $m$ bags we have $T = m\cdot T_{bag}$\\
We could build a pipeline, a building chain, with 4 people and each person does one task: \begin{list}{}{}
	\item One takes the bag and gives to the next
	\item One puts the pen into the bag and passes it
	\item One puts the paper into the bag and passes it
	\item One puts the proceedings into the bag
\end{list}
So $w_b, w_{pen}, w_{paper}, w_{proc}$ workers. When the first worker has passed the bag, it could begin taking the next bag. Same for the others.
\begin{center}
	\includegraphics[scale=0.5]{1.png}
\end{center}
So in sequential we have $m\cdot(t_{bag} + t_{pen} + t_{paper} + t_{proc})$, and in parallel per 1 bag we have $t_{bag} + t_{comm} + t_{pen} + t_{comm} + t_{paper} + t_{comm} + t_{proc} + t_{comm}$ with $t_{comm}$ spent passing the bag from one to the other, so total of $m\cdot T_{seq} + m\cdot t_{comm}$. But that's not correct, because we work in parallel: ideally we have a parallelogram of $m\cdot(t_{proc} + t_{comm}$ base, and we require $t_{bag} + t_{pen} + t_{paper} + 3\cdot t_{comm}$ time to get up to speed and "fill the pipeline". But this required time is negligible, and in the end the overall time is given by the base of the parallelogram.
\paragraph{Pipeline} With $m$ tasks and $nw$ stages, with the completion of the stage $i$ required in stage $i+1$. So the output is $f_{nw}(f_{nw-1}(\ldots f_1(x_i)\ldots))$. With $t$ time required for each stage.
\begin{center}
	\includegraphics[scale=0.5]{2.png}
\end{center}
We spend $(nw-1)t$ to get the last stage working and $m\cdot t$ time spent by the last stage to complete all the tasks.
$$T_{par}(nw) = (nw-1)\cdot t + m\cdot t$$
$$\hbox{SpeedUp}(nw)=\frac{(nw\cdot t)\cdot m}{(nw-1)\cdot t + m\cdot t}$$
So the higher the $m$ is, the lower is the impact of the time required to get up to speed. So $m >> nw \Rightarrow T_{par}(nw) \simeq m\cdot t$
\paragraph{Throughput} Tasks completed per unit of time.
\section{Measures}
On one side we can have more speed with more resources (computing devices). On the other side we can use more complex applications, with more resources. For example more precise computations, so extra resources not for improving the time but to improve the quality of the computations.\\
Finally, we could aim at computing results with less energy thanks to parallelism. This is a recent perspective on parallelism.\\\\
We've seen the SpeedUp($n$) = $\frac{T_{seq}}{T_{par}(n)}$, where the plot has to lie below the bisection of the cartesian graph.
\subsection{Base Measurements}
\paragraph{Latency $L$} Measure of the wall-clock time between the start and end of the single task.
\paragraph{Service Time $T_s$} It's related to the possibility of executing more tasks. It's the measure of the time between the delivery of two consecutive results, for example between $f(x_i)$ and $f(x_{i+1})$\\
Even if $x_i$ and $x_{i+1}$ arrive at the same time, $f$ would still be computing $f(x_i)$ so it'll start computing $f(x_{i+1})$ when it has finished.
\paragraph{Completion Time $T_c$} The latency related to a number of tasks. $T_c = L\cdot m$ for $x_m,\ldots,x_1$ inputs to a sequential system.\\
With a parallel system, instead, we have $T_c\simeq m\cdot T_s$.
\paragraph{Example} A 3 stage pipeline, with each node being sequential and with latency $L_i$ for node $i$.\\
At $t_0$ the first stage $N_1$ gets the first tasks and computes it in $L_1$, then $N_2$ computes in $L_2$ and $N_3$ computes in $L_3$ so a total of $t_0 + L_1 + L_2 + L_3$.\\
When the pipeline is filled, $T_s$ is dominated by the longest $L_i$, so $T_s = \max\{L_1, L_2, L_3\}$ and $T_c = \sum L_i + (m-1)T_s$\\
If $m$ is large with respect to $n=$ number of stages, the "base of the parallelogram" would be very long, so $m >> n \Leftrightarrow T_c = m\cdot T_s$
\subsection{Derived Measurements}
\paragraph{SpeedUp} $$\hbox{SpeedUp}(n) = \frac{T_{seq}}{T_{par}(n)}$$ Could be latencies, service times\ldots depending on what we want to measure the speedup of.
\paragraph{Scalability} $$\hbox{Scalability}(n)=\frac{T_{par}(1)}{T_{par}(n)}$$
\paragraph{Efficiency} $$\hbox{Efficiency}(n) = \frac{\hbox{Ideal parallel time}(n)}{T_{par}(n)} = \frac{\frac{T_{seq}}{n}}{T_{par}(n)} = \frac{T_{seq}}{n\cdot T_{par}(n)} = \frac{\hbox{SpeedUp}(n)}{n}$$
Measures the tradeoff between what you gain with the speedup and the cost of the speedup.
\paragraph{Throughput} $$\hbox{Throughput} = \frac{1}{T_s}$$
\paragraph{Amdahl Law} Taken the total time of a computation, $T_{seq}$, it can be divided into something that can and something that cannot be computed in parallel (for example, dividing the book is a sequential activity). So we can say that\\$T_{seq} =$ serial fraction + parallel fraction and the \textbf{serial fraction cannot be parallelized}. $f\in[0,1]\:|\:f\cdot T_{seq}$ is the serial fraction.
$$T_{seq} = f\cdot T_{seq} + (1-f)\cdot T_{seq}$$
The parallel fraction can be splitted between the workers, but we would have to compute the serial fraction too. By splitting more and more and more, we have that $$\lim_{n\to\infty} T_{par}(n) = f\cdot T_{seq}$$ $$\hbox{SpeedUp}(n) = \frac{T_{seq}}{f\cdot T_{seq}} = \frac{1}{f}$$
So we have a very low upper bound on the achievable speedup. This is referred to as \textbf{strong scaling}: strong meaning using more resources to get the computation faster.
\paragraph{Gustaffson Law} $$\hbox{SpeedUp}(n) = N - S\cdot(N-1)$$ With $S$ being the serial fraction. This comes from the fact that we're considering a different perspective: Gustaffson assumes that the computation increases with the parallelism, something that's called \textbf{weak scaling}, getting the speedup from using more computational devices, using more data. %TODO rivedere 17/02 inizio parte 2
\paragraph{Cores} In modern computers, we have a main memory (slow), a disk (even slower) and the memory is connected to at least 3 levels of cache. At the bottom we have some cores (4, 8\ldots), each one has its own level 1 cache (usually split in data and instruction cache).\\
With an activity with a working set that fills the cache, in case of strong scaling splitting the computation across cores we process less data per core because the size of the problem is the same.\\
With weak scaling, we assume that the data increases so by using more cores we process the same data on all cores but the data grows so we could have extra overhead because of the working set size.\\
We will have patterns of parallel computation that differentiate in how we process the data.
\paragraph{Application as Graphs} The applications can be seen as graphs of sequential nodes with dependencies.
%TODO
The maximum speedup is the work over the span, because in every case I need to go from the first to the goal node. I take the longest one because at least the longest path must be computed, and all the rest can be done in parallel and I assume to have enough resources to compute the rest in the time of the span.\\
We can use this model %TODO
\section{Technicalities}
\paragraph{Examples}
A simple program that "translates" an ASCII file by transforming lower letters into capital letters. We split the text into $n_{workers}$ parts, we wait for all the threads to finish and then verify the performances.
\\The translator is:
\begin{lstlisting}[style=myPython]
#include <string>

char translate_char(char c) {
	if (islower(c))
		return(toupper(c));
	else
		return(tolower(c));
}
\end{lstlisting}
%TODO
\subsection{Threads}
We used to write instructions sequentially. At a given point now we \texttt{fork} another flow of computation: we get two flows that are executed together \textbf{in the same address space}, so the new thread inherits all the memory of the original thread.
\paragraph{Concurrence} \texttt{todo} %TODO
\paragraph{Packaged Threads}
\paragraph{Overheads} % repeatedly fork an join an see what happens, measure mean time of fork-join
All the time spent that is not involved in the sequential execution: time spent organizing the parallel computation, gathering solutions\ldots so there's a tradeoff between the time spent to setup the parallel activity and the time earned because of the parallel execution.\\
What can we do to get rid of the setup time? I can create the threads once and reuse them when needed: \textbf{threadpools}.
\paragraph{Cache Coherence Protocols} Snoopy, or directories of shared data consulted any time I access shared data and propagates the edits.\\
Coherency works at the cache line level!
\subparagraph{In Stencil} Reuse cache coherence protocols for each write.
\subparagraph{False Sharing Problem} We use padding techniques to transform the vector in such way that the original vector has some pad values up to the point where the cache line finishes. This is used in maps.
\subparagraph{Disabling} Sometimes useful to speed up the computations.
\subparagraph{} So we have to take into account that we have to ensure locality as much as possible and be careful of the possibilities of fault sharing problems, like similar iterations in short time.
\paragraph{\texttt{taskset}} To restrict the cores of a process, \texttt{taskset -c 0-3 command} restricts \texttt{command} to the cores 0, 1, 2 and 3.

\section{Patterns}
Computations with particular shapes and semantics that can be understood and implemented depending on the situations, not linked to languages and technicalities. Patterns are a useful concepts, allow programmers to reuse experience of other programmers an not reinventing the wheel.\\
Parallel patterns:
\begin{list}{}{}
	\item Data parallel
	\item Stream parallel
\end{list}
The same patterns can be referred with different names.
\subsection{Data Parallel Patterns}
Parallelism comes from data: we split the data in pieces, compute a set of results that can be combined into a single final result. The book translation examples is a data parallel pattern. What matters is $L$.\\
The general pattern is:
\begin{list}{}{}
	\item Decomposition
	\item Partial results
	\item Recomposition
\end{list}\begin{multicols}{2}
\subparagraph{Map Pattern} Also called applytoall:\begin{list}{}{}
	\item $\forall$ item of the collection
	\item Function $f($item$)$
	\item $\forall$ $f($item$) \rightarrow$ isomorphic collection
\end{list}
\subparagraph{Reduce Pattern} Also called fold:\begin{list}{}{}
	\item $\forall$ item of the collection
	\item $\oplus(x,y)$
	\item $\oplus(\oplus(a,b),\oplus(c,d))$
\end{list}
\end{multicols}
\subparagraph{Stencil Pattern} \begin{list}{}{}
	\item In partially overlapping position, e.g. of a matrix or an image
	\item Function $f($item$)$
	\item $\forall$ $f($item$) \rightarrow$ isomorphic collection
\end{list}
Different kind of problems: overlapping positions will yield the new value, so we have to account for that.
\subparagraph{"Google" mapreduce}
\begin{list}{}{}
	\item $\forall$ items
	\item $f($item$) + \oplus($item$, $item$)$
	\item Item
\end{list}
What I apply to each item is an $f$ that maps to $\langle$key, value$\rangle$ and $\oplus$ applies the sum to each value.\\
For example in a document, $f($word$)=\langle w,1\rangle$ and $\oplus(\langle w_k,v_1\rangle,\langle w_k,v_2\rangle) = \langle w_k, \oplus(v_1,v_2)\rangle$\\
"The lesson given by the professor", $f$ will output $\langle$the, 1$\rangle,\langle$lesson, 1$\rangle$\ldots and $\oplus$ will for example output $\langle$the, 2$\rangle$.\\
We can apply the map function over all the data distributed in various databases, for example.\\
This is \texttt{map(f)} and \texttt{reduce($\oplus$)}, but we want something like \texttt{map(reduce($\oplus$))}. Combining elementary patterns to achieve more complex results. Like two nested \texttt{for}s.\\
So I want building blocks, something that guarantees correctness of implementation that can be used to build upon. For example \texttt{map(function<A(B)>, collection<B>)}\\
So build a bag of tools that we can combine to undertake common situations with good efficiency, speedup, scalability\ldots
\subsection{Stream Parallel Patterns}
Stream of data, flowing in time. In data parallel we process a data collection, while in stream parallel we don't have data appearing all at the same time. So stream as a collection with items appearing at different times. We want to take the single items and try to process in parallel, parallel execution of $f$ over different items of the stream.\\
What matters is $T_S$
\paragraph{Pipeline}
\begin{center}
	\includegraphics[scale=0.5]{3.png}
\end{center}
Inside the pipeline we have $f_1\rightarrow f_2\rightarrow\ldots\rightarrow f_k$ with each $f_i$ corresponding to a phase, with $f_i$ taking input from $f_{i-1}$.\\
So $x_i\mapsto f_1(x_i)\mapsto f_2(f_1(x_i))\mapsto \ldots$, and the parallelism is in the computation of different phases of different items (much like what we've seen with the CPU fetch-decode-execute pipeline).
\begin{center}
	\includegraphics[scale=0.5]{4.png}
\end{center}
\paragraph{Farm}
We have a number $nw$ of instances of the same function $f$ each processing one single item.
\begin{center}
	\includegraphics[scale=0.5]{5.png}
\end{center}
We have no interference between computations of $x_i,x_j$ with $i\neq j$, no need for synchronization.
\paragraph{} $T_C \simeq m\cdot T_S$ and $T_S$ in sequential is $\simeq L$ this means that I can try to decrease the latency by decreasing the stages in the pipeline or the workers in a farm.
\paragraph{Two Tier Model} Let's assume a grammar of patterns.\begin{list}{}{}
	\item Pat = Seq(f) $|$ DPP $|$ SPP
	\item DPP = Map(Pat) $|$ Reduce(Pat)
	\item SPP = Farm(Pat) $|$ Pipe(Pat, Pat)
\end{list}
This defines parallel computations and we aim at assuring that this can be done, a way of implementing this. So\begin{list}{}{}
	\item Map(Pipe(Seq(f), Seq(g)))
\end{list}
can be a data parallel computation where on the single item we compute a 2-stages pipeline of $f$ and $g$. But each element of the map is given to a single pipeline, or each pipeline receives a single item, so the stream parallelism is useless.
\begin{list}{}{}
	\item Farm(Map(Seq(f)))
\end{list}
Here we have a stream of items that will be processed by a Farm, each item splitted by Map and processed. This can deliver the result faster and access the next item.\\
So Data parallel with Stream parallel is not very good, Stream parallel with Data parallel is better: \textbf{two tier model}.\\
So we have an initial part of the pattern which is Stream Parallel, the second part is Data Parallel and eventually the last stages (the leafs of the tree) which are sequential.
\paragraph{Parallel Design Patterns}
Also called \textbf{algorithm skeletons}: programming abstraction that model some pattern. The programmer has a framework, libraries, languages and that includes algorithm abstractions.
\subsection{Composing}
These are building blocks, so we can \textbf{compose} them. Let's see how that works and what are the expected performances.
\subsubsection{Pipeline}
We have a number $k$ of stages for $m$ tasks: Pipeline$(s_1,\ldots,s_k)$ meaning that this is a composition yielding $s_k(\ldots s_1(\:)\ldots)$
$$\hbox{Input stream}\longrightarrow s_1\rightarrow\ldots\rightarrow s_k\longrightarrow\hbox{Output stream}$$
With each $\rightarrow$ being a stream $s_i\rightarrow s_{i+1}$ and each $s_i$ taking input from $s_{i-1}$.\\
The latency of the pipeline is the sum of the latencies of the stages $$L(\hbox{Pipeline}(s_1,\ldots,s_k)) = \sum_{i=1}^k L(s_i)$$
We do not consider the time required to pass input to the next stage, $t_{comm}$, which would be based on size, nature of the computation\ldots\\
The steady state is when all the stage are "filled": the longest of the stages will dominate the service time $T_S$
$$T_S(\hbox{Pipeline}(s_1,\ldots,s_k)) = \max_{i=1}^k\{T_S(s_i)\} = \max_{i=1}^k\{L(s_i)\}$$
The completion time is $T_C$
$$T_C(\hbox{Pipeline}(s_1,\ldots,s_k)) = \left(\sum_{i=1}^k L(s_i)\right) + (m-1)\max_{i=1}^k\{L_i\}$$
and when $m >> k$ we can approximate it with
$$T_C = mT_S$$
because the number of tasks required, the "base of the parallelogram", will dominate the number of tasks, the "height of the parallelogram".
\paragraph{Boundary Conditions} We have to take into account the interarrival time $T_A$, time spent to get another item from the input stream, and the interdeparture time $T_D$, the time spent to get another item into the output stream.\\
Let's suppose that $L(s_i) = i$ seconds, so ideally $T_S = k$ seconds: we process 1 item each $k$ seconds. If $T_A > L(s_i)$ we have to wait the second item when I finish processing the first, same thing for the next after the second: the interrarival time looks like an interstage between $s_{i-1}$ and $s_i$.\\
$T_D$ behaves at the same way: we have to wait that $T_D$ finishes before giving it out output, behaving like an interstage.\\
So the previous behavior, analyzed before, happens $\Leftrightarrow T_A<T_S$ and $T_D<T_S$ \textbf{something that we have always to take into account}.
\subsubsection{Farm}
Sometimes we denote as Farm$(s, nw)$, otherwise we omit the number of workers and simply write Farm$(s)$.\\
We assume to know $L_w$ and $T_w$ of the workers. We have some scheduler (\textbf{emitter} $E$) that distributes the items from the input stream to the workers, and a gatherer (\textbf{collector} $C$) that gets the results from the workers and delivers them to the output stream. Those can simply be data structures: queues, for example.
$$L(\hbox{Farm}(s,nw)) = t_E + L_w + t_C$$
This can appear as a pipeline of three stages, where the Emitter produces to the second stage (the workers) which produce to the third stage (the Collector)
$$T_S(\hbox{Farm}(s,nw)) = \max\left\{t_E, \frac{T_w}{nw}, t_C\right\}$$
We assume to have $m$ tasks
$$T_C(\hbox{Farm}(s,nw)) = m\cdot T_S(\hbox{Farm}(s,nw))$$
With boundary conditions
$$T_S=\max\{T_s(\hbox{Farm}()),T_A,T_D\}$$
What if we want to achieve a given performance? Compute a $nw$ suitable to achieve a wanted performance by inverting the very same formulas.\\
With a target $T_S = T_A = 1$s
$$T_S = 1\hbox{s} = \max\left\{t_E, t_C, \frac{10\hbox{s}}{nw}\right\}$$
But $t_E,t_C$ are negligible so $$\frac{10\hbox{s}}{nw} = 1\hbox{s} \Rightarrow nw = 10$$
%TODO riascoltare 2a parte 2/03
\subsubsection{Map} We consider it as made by three phases:
\begin{list}{}{}
	\item Split: divide the collection into sub collections, a \textbf{set of subcollections}
	\item Map: compute the \textbf{set of subresults}
	\item Merge: produce the final \textbf{collection results} (usually in the same shape as the input collection)
\end{list}
With $m$ dimension of the collection and $t_f$ to compute the function of the map $$L(\hbox{Map}) = \frac{m\cdot t_f}{nw} + t_{split} + t_{merge}$$
$t_{split}$ and $t_{merge}$ are non-negligible in distributed architectures, but are negligible in shared-memory architecture.
$$T_S(\hbox{Map}) = L(\hbox{Map})$$
No concept of $T_C$ because the map is applied to a single collection.\\
If I have multiple collections we consider a splitter node as $s_1$, map ($t_f$) as $s_2$ and merger node as $s_3$, giving
$$T_S=\max\{t_{split}, t_{map}, t_{merge}\}$$
$$t_{map} = \frac{m}{nw}t_f$$
\pagebreak
\subsubsection{Reduce}
From a vector we want to output the sum: a scalar from a collection. We split in $t_{split}$, then each of the $nw$ workers applies the function $\oplus$ to its subcollection in $\frac{m}{nw}t_\oplus$ and finally we merge in $nw\cdot t_\oplus$ because we have to compute $\oplus$ over all the $nw$ subresults.
$$L=t_{split} + \left(\frac{m}{nw}-1\right)t_\oplus+(nw-1)t_\oplus$$
$$T_S = L$$
With multiple collections, same argument as before
$$T_S = \max\{\ldots\}$$
But the computation can be organized in a logarithmic tree, too. We would have $\log_2(m)$ phases each with half the activities of the previous phase.\begin{center}
	\includegraphics[scale=0.5]{6.png}
\end{center}
But we have efficiency 1 only in the first phase, then half, then one fourth\ldots
$$L=t_{split}+\lceil\log_2 m\rceil t_\oplus$$
provided that $nw \geq \frac{m}{2}$. If we use threads, the split phase is just telling the threads what they have to do, then we merge with a simple loop.
\subsubsection{Stencil}
For example computing the average of three neighbor items in a vector, with necessary boundary conditions.\\
Split phase that produces for example two halves of the vector: when computing the right extreme of the first half, I need to add the first item of the second vector, same with the left extreme of the second half: we have some shared positions. No problem when reading, the problem if we write the shared position. We can use a buffer for the write, using the old values only for reading and swapping the vectors. Or we can use a small buffer to host modified values the neighborhood.
$$L(\hbox{Stencil}) = t_{split} + \frac{m}{nw}t_{stencil} +t_{merge}$$
Where $t_{stencil}$ includes buffer management with the second solution and $t_{merge}$ includes swapping the buffers in the first solution.
\paragraph{Composition} Given a Pipeline($s_1,s_2,s_3$) with $s_i$ sequential, $s_2$ may be data parallel (map), $m$ stream items and each being a vector of $k$ items.
$$L = L_1+L_2+L_3$$
$$T_S=\max\{L_1,L_2,L_3\}$$
$$T_C\simeq m\cdot T_S$$
$s_2$ is sequential but can be turned into a map, takes $L_2$ so I can imagine $t_f \simeq \frac{L_2}{k}$
$$T_S(\hbox{Pipeline}) = \max\left\{L_1,L_3,\frac{L_2}{nw_{map}}\right\}$$
$s_2$ is the slowest stage, we can use a Farm$(s_2, new_{farm})$, but in this case latency stays the same, while using the Map the latency decreases.
\paragraph{Overhead related to memory allocations} New objects are created when instantiating inputs and results, for example. This requires allocating memory in the heap, with corresponding \texttt{malloc}s and \texttt{free}s. So we may need to be smarter, with solutions that work on thread-level memories, small heaps where to allocate objects and releasing when no more needed.
\subparagraph{\texttt{jmalloc} library} Used in BSD systems, FireFox and Facebook among others.\\
It manages \textbf{chunks of memory} called \textbf{arenas}, distributed in a round robin way per thread with each thread using one arena. A metaarena is used a common place.\\
Arenas A1, A2,\ldots, Ak are assigned in round robin to th1, th2\ldots. When some data comes from an arena, and we free that data we free the original arena not the local one. \texttt{jmalloc} has its own API, but other than that it uses the classical API: \texttt{malloc} and \texttt{free}, same as stdlib.\\
Taking a normal program \texttt{a.out}, with \texttt{./a.out p1 \ldots pk} we will go with \texttt{malloc} and \texttt{free} of the stdlib. If we prefix with \texttt{LD\_PRELOAD=libjmalloc.so ./aout p1 \ldots pk} then \texttt{malloc} and \texttt{free} will be loaded from the \texttt{jmalloc} library.
\section{Load Balancing} Giving the same amount of work to all the cores involved. Even if a single thread takes longer than all the other, we wait that it fishes so we have a lot of empty time in the other threads. The efficiency lowers a lot, and poor speedup too.\\
One of the reasons could be that the computation per se is unbalanced.
\subsection{Static Techniques}
Related to the usage of different splitting policies.
\paragraph{Chunk policy} Take the vector and split into adjacent parts
\paragraph{Cyclic policy} First item to first thread, second to second thread\ldots e.g. with two threads: first to th1, second to th2, third to th1, fourth to th2\ldots
\paragraph{Mix} A cyclic distribution of blocks. Split into blocks and assign the first to a thread, the second to the next\ldots following the cyclic policy.
\subsection{Dynamic Techniques}
We can do much more, adapting to the situation giving more things to do to the thread that so far have done less.
\paragraph{Autoscheduling} Threads are not assigned a block/item or a distribution, but each threads \textit{asks} for something to be computed. Some code like \begin{lstlisting}
while (more work to do) {
	ask work
	compute
	deliver result
}
\end{lstlisting}
The threads that gets longer tasks stops asking for more tasks for a while, and more tasks will be executed by the other threads. When tasks are almost finished, it may happen that some thread gets the "last" long task still taking longer than all the other threads. But it's not as impactful as stated before, as it's the worst case.\\
In general, prefetch $k \simeq 2$ or $3$ tasks
\paragraph{Job Stealing} Bunch of tasks, cyclic static assignment. With $nw$ threads and $m$ tasks, each thread gets $\frac{m}{nw}$ tasks. With job stealing, the thread that has finished its tasks and perceives that there's more to compute, steals a task from another thread. Problems: "size" of the steal, synchronize accesses, who to target\ldots the solution is a random policy: threads that finishes their own assigned task steal a random number $\in[0, nw]$ and steal that number of tasks.
\paragraph{Autoscheduling + variable size chunks}

\paragraph{Template Based Implementation} For each pattern (pipe, farm, map, stencil\ldots) we have templates:\begin{list}{}{}
	\item for each target architecture (multicore, cloud\ldots)
	\item activity graph
	\item performance modelling, also a way to have an idea of what we can achieve in terms of perfomance
\end{list}
Let's analyze a template for a pipeline on a shared memory multicore machine (\texttt{template(pipeline, SMmulticore)}):
$$\longrightarrow\hbox{thread}\rightarrow\hbox{thread}\rightarrow\ldots\rightarrow\hbox{thread}\longrightarrow$$
With the $\rightarrow$ being communication channels (queues). Modelling the speedup$(m)\simeq m$ with $m$ being the number of stages, so a linear speedup, if and only if $\max\{t_i\}\simeq t_j$ so the times of each stage are more or less the same.\\\\
But we can have another template
$$x_m\ldots x_2\:x_1\longrightarrow\hbox{queue}$$
Where in the queue we put "\texttt{compute} $f$ \texttt{over} $x_i$" as $\langle f,x_i\rangle$ for each $i$. Each worker gets $\langle f,x\rangle$ and puts in the queue $\langle g, f(x)\rangle$. For $\langle g,x\rangle$ returns $\langle h,g(x)\rangle$ and for $\langle h,x\rangle$ returns $\langle$ end$,h(x)\rangle$.\\
So the queue will have different types of tasks: the input tasks are assigned to $f$ but the intermediate tasks are assigned to $g$ or $h$: it's a pipeline. Of course, the end tasks means outputting on the output queue.\\
A problem: the queue is a bottleneck. We can use some techniques: set of queues, local queues\ldots Also the ordering, but i can keep the index $i$ for each stage and use it at the end to keep in a buffer and output in the correct order.\\
With modeling provided sufficient $T_A$, the speedup can be proportional to $nw$ and not $m$.\\\\
Another template is a round-robin scheduler that assigns to some workers that deliver results to sorter with respect to the input order. The scheduler gives $\langle i, x_i\rangle$ and the output is $h(g(f(x_i)))$ ordered on $i$. Pipeline. Uses $nw+2$ concurrent activities, with a queue for each worker plus a queue for the sorter. With $T_A$ low, we have speedup circa $nw$.\\\\
Changing architecture, from shared memory multicore to a clustered workstation. I can use any template ported to a cluster. For example
$$\hbox{workstation}_1\rightarrow_{\hbox{TCP/IP}}\hbox{workstation}_2\ldots$$
The speedup is circa the number of stages. But we can also have a number of workers in each workstation, with a scheduler and a merger per each. A kind of composed template, the third inside the stages of the first.
$$T_S\simeq\frac{\max\{T_i\}_{nw}}{nw}$$
The overhead is in the scheduling and merging parts of each stage. The scheduling is necessary, but in the ordering part we sort data on the input order, but we can do that just at the end of the last stage.
\paragraph{Macro Data Flow Implementation} With Data Flow we denote the operations where the order of computations are dictated by the flow of data encountered by the program counter.\\
So we have nodes that are composed of $\langle$ operator, variables, output$\langle$. For example, $$(a+b)\cdot(c-d)$$ can be represented as 
\begin{center}
	\includegraphics[scale=0.5]{7.png}
\end{center}
We have tokens on the inputs, that represent when data are available. When all data is available, the node becomes "fireable". This technique can be used to implement parallel patterns, Macro because instead of considering operations as nodes with functions an input/output nodes we consider macros with full portions of code instead of primitive functions.\\
Let's consider a program \texttt{main.cpp} that uses some pattern internally from a pattern library. First step: from a pattern tree we compile the MDF (Macro Data Flow graph). We have a graph repository and a pool of executors: anytime I have some input data I create a copy of the graph in the repository with the tokens that represent the data.\begin{center}
	\includegraphics[scale=0.5]{8.png}
\end{center}
Each executor executes a loop where they get a fireable instruction, compute and deliver the output tokens.
\paragraph{Compiler} From the Pattern Tree it outputs the MDF graph.
\begin{list}{}{}
	\item Compile(Pipe($f,g$)) $\rightarrow$ $\langle f,$ in, A$\rangle$, $\langle g,$ A, out$\rangle$
	\item Compile(Seq($f$)) $\rightarrow$ $\langle f,$ in, out$\rangle$
	\item Compile(Farm($f$)) $\rightarrow$ Compile($f$) for each instance of input
	\item Compile(Map($f$)) $\rightarrow$ Compile($f$) for each element of the input collection
	\begin{center}
		\includegraphics[scale=0.5]{9.png}
	\end{center}
	\item Compile(Reduce($f$)) $\rightarrow$ \begin{center}
		\includegraphics[scale=0.5]{10.png}
	\end{center}
\end{list}
Each one of the graphs has one input token and one output token, mandatory to be able to compose the graph. The Farm parallelism disappears %TODO
and third %TODO
\paragraph{Problems}\begin{list}{}{}
	\item Contention over repository
	\item Token delivery
	\item Maintain the list of fireable instructions
	\item Reordering on the out stream
\end{list}
\paragraph{Refactoring Rules}
The grammar used to write our program is:
\begin{center}
	Pat = Seq() $|$ Pipe(Pat$_1$, Pat$_2$) $|$ Comp(Pat$_1$, Pat$_2$) $|$ Farm(Pat, $nw$) $|$ Map(Pat) $|$ Reduce(Map)\\
	Prog $\Rightarrow$ Pat : $x$\\
	\includegraphics[scale=0.5]{12.png}
\end{center}
Whit \textbf{functional semantics} we mean \textbf{what is computed}. \textbf{Non-functional semantics} refers to \textbf{how} the result \textbf{is computed}.\\
So refactoring rules describe the equivalences in the \textbf{functional semantics}. So different performance, parallelism degrees\ldots\\
Some examples:
\begin{list}{}{}
	\item Farm($x, n$) $\equiv$ Farm($x, m$) with $n\neq m$
	\item Pipe($x, y$) $\equiv$ Comp($x, y$)
	\item Pipe(Farm($x, \_$), Farm($y, \_$)) $\equiv$ Farm(Pipe($x, y$) \_)
\end{list}
How to figure out the better ones? We will consider a small set of rules:
\begin{list}{}{}
	\item \textbf{Pipe introduction/elimination}: Comp($x,y$) $\equiv$ Pipe($x,y$)
	\item \textbf{Farm introduction/elimination}: $x \equiv$ Farm$(x)$
	\item \textbf{Map fusion}: Map(Comp($x,y$)) $\equiv$ Comp(Map($x,y$))
	\item \textbf{Pardegree change}: Farm($x, n$) $\equiv$ Farm($x, m$) with $n\neq m$
\end{list}
The idea is to write a program (so a tree $T_0$) and apply the rules (via a tool) getting $T_1,T_2,\ldots$ all functionally equivalent to $T_0$. Usually the goal is a low $T_{S_i}$. We take the produced trees, mapping each to its $T_S$ and reducing to $\min$ to get the minimum $T_{S_k}$.\\
But maybe we compute worse trees. I can do another thing: taking $T_0$ trying to find the path in the possible trees. I apply rules and measure the metric (in this example, $T_S$). Then I don't go exploring all the subtrees, applying all applicable rules, but I just go one level down and pick the best one each time. We follow a path that always gives a better (or equivalent) performing solution. But this is \textbf{not possible}.
\end{document}