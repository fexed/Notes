\documentclass[10pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[italian]{babel}
\usepackage{multicol}
\usepackage[bookmarks]{hyperref}
\usepackage[a4paper, total={18cm, 25cm}]{geometry}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{textcomp}
\graphicspath{ {./img/} }
\usepackage{listings}
\usepackage{makecell}
\usepackage{qtree}
\usepackage{pgfplots}
\usepackage{tikz}
\usepgflibrary{shapes}
\usepgfplotslibrary{fillbetween}
\definecolor{backcolour}{RGB}{255,255,255}
\definecolor{codegreen}{RGB}{27,168,11}
\definecolor{codeblue}{RGB}{35,35,205}
\definecolor{codegray}{RGB}{128,128,128}
\definecolor{codepurple}{RGB}{205,35,56}
\lstdefinestyle{myPython}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{codeblue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\small\ttfamily,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=2pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2,
	language=python
}
\newcommand*\triangled[1]{\tikz[baseline=(char.base)]{
            \node[regular polygon, regular polygon sides=3,draw,inner sep=1pt] (char) {#1};}}
            
\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\fancyhead{}
\fancyfoot[L]{Telegram: \texttt{@fexed}}
\fancyfoot[R]{Github: \texttt{fexed}}
\begin{document}
\title{Parallel and Distributed Systems}
\author{Federico Matteoni}
\date{A.A. 2021/22}
\renewcommand*\contentsname{Index}

\maketitle
\tableofcontents
\pagebreak
\section{Introduction}
Prof.: Marco Danelutto
\paragraph{Program} Techniques for both parallel (single system, many core) and distributed (clusters of systems) systems.\\
Principles of parallel programming, structured parallel programming, parallel programming lab with standard and advanced (general purpose) \textbf{parallel programming frameworks}.
\paragraph{Technical Introduction} Each machine has more cores, perhaps multithreaded cores, but also GPUs (maybe with AVX support, which support operations floating point operations, \textbf{flops}, in a single instruction).\\
Between 1950 and 2000 the VLSI technology arised, integrated circuits which nowadays are in the order of 7nm (moving towards 2nm): printed circuits!\\
In origin, everything happened in a single clock cycle: fetch, decode, execute, write results in registers, with perhaps some memory accesses. The we had more complex control where in a single clock cycle we do just one of the phases (fetch \textit{or} decode \textit{or}\ldots), like a \textbf{pipeline}. More components are used the higher the frequency but the more power we need to dissipate, and we're coming to a point were the power we need to dissipate is too much and risks to melt the circuit, so we're reaching a \textbf{physical limit} in chip miniaturization. But temperature and computing power do not go in tandem: computing power is proportional to the chip dimensions, while temperature is proportional to the area. So it's better to put more processors (\textbf{cores}) and let them work together rather than make a bigger single processor.\\
An approach is to have few powerful cores and more less powerful cores (for example, in the Xeon Phi processors). Now, the processors follow this architecture, with the performance of a single core decreasing a bit with every generation but it's leveled by adding more cores.\\\\
Up to the 2000, during the single core era, code written years before will run faster on newer machines. Now, code could run slower due to not exploiting more cores and the decreasing in performance of the single core.\\With accelerators the situation is even more different: for example GPUs, accelerator for graphics libraries, with their own memory and specialized in certain kinds of operations. This can require the transfer of data between the accelerator's memory and the main memory, so the architecture of the accelerator is impactful on the overall performance.
\section{General Paradigms of Parallel Programming}
\paragraph{Parallelism} Execution of different parts of a program on different computing devices at the same time. We can imagine different flows of control (sequences of instruction) that all together are a program and are executed on different computing devices. Note that more flows on a singe computing device is \textbf{concurrency}, not parallelism.
\subparagraph{Concurrency} Similar concept: things that \textit{may} happen in parallel respecting the ordering between elements.
\paragraph{Computing Devices}\begin{list}{}{}
	\item \textbf{Threads}, implying shared memory
	\item \textbf{Processes}, implying separated memories
	\item \textbf{GPU Cores}
	\item \textbf{Hardware Layouts} on a FPGA (Field Programmable Gate Array) 
\end{list}
\paragraph{Sequential Task} A "program" with its own input data that can be executed by a single computing entity
\paragraph{Overhead} Actions required to organize the computation but that are note included in the program. For example: time spent in organizing the result. Basically, time spent orchestrating the parallel computation and not present in the sequential computation.
\paragraph{Speedup} Fundamental things that we're looking for, it's the ratio between the sequential time and the parallel time.$$\hbox{SpeedUp} = \frac{\hbox{Sequential time}}{\hbox{Parallel time}}$$
Assuming the best sequential time.\\
We have a slightly different measure, too
$$\hbox{Scalability}=\frac{\hbox{Parallel time with 1 computing device}}{\hbox{Parallel time}}$$
\paragraph{Stream of tasks} In some cases it's not important considering just one computation but may be useful considering more computations and we want to optimize a set of tasks.
\paragraph{Example: Book Translation} With $m=600$ pages, for example. Let's assume I can translate a page in $t_p = 0.5$h.\\
The sequential task is: take the book and spend time until I can deliver the translated book. The time is circa $m\cdot t_p = 300$h.\\
In parallel, ideally every page can be translated independently so I can split the book in two pieces of $\frac{m}{2}$ pages each (overhead), giving each half to a person. Both can translate at the same time, so ideally the time required is $\frac{m}{2}\cdot t_p$ for each, producing the translated halves. At this point I get the halves and produce the translated version (overhead). Ideally the time require is more or less $\frac{m}{2}\cdot t_p$, with "more or less" given by the time spent in splitting the book and reuniting the two halves. So the exact time is $T = T_{split} + \frac{m}{2} \cdot t_p + T_{merge}$.\\
What if the two person have different $t_p$s? For example $t_1 > t_2$. When a translator finishes, it spends some time synchronizing its work with me. With $nw$ "workers" (translators, in this instance) $T = nw\cdot T_{split} + nw\cdot T_{merge} + \frac{m}{nw}T_{work}$ with $nw\cdot T_{split}$ time spent delivering work to each worker and $nw\cdot T_{merge}$ time in merging each result.\\\\
Init is the time where every worker has work to do, and finish is the time where the last worker finished working. So the exact formula is with a single $T_{merge}$.\\
So $\frac{m}{nw}T_{work}$ is the time that needs to happen, found in the sequential computation too, whereas the other two factors are \textbf{overhead}.
$$\hbox{SpeedUp} = \frac{\hbox{Best sequential time}}{\hbox{Parallel time}}$$ but the parallel time depends on the $nw$ so $$\hbox{SpeedUp}(nw) = \frac{\hbox{Best sequential time}}{\hbox{Parallel time}(nw)} \simeq \frac{\not m\cdot \not t_p}{\frac{\not m}{nw}\cdot \not t_p} = nw$$ This not taking into account the overhead. It's a realistic assumption because usually the time splitting the work is very small. But we have to take into account that, in case it's not negligible. $$\hbox{SpeedUp}(nw) = \frac{m\cdot t_p}{\frac{m}{nw}\cdot t_p + \underline{nw\cdot T_{split} + T_{merge}}}$$
%TODO graph sp(nw) over (nw)
\paragraph{Example: Conference Bag} $T_{bag} = t_{bag} + t_{pen} + t_{paper} + t_{proc}$ and with $m$ bags we have $T = m\cdot T_{bag}$\\
We could build a pipeline, a building chain, with 4 people and each person does one task: \begin{list}{}{}
	\item One takes the bag and gives to the next
	\item One puts the pen into the bag and passes it
	\item One puts the paper into the bag and passes it
	\item One puts the proceedings into the bag
\end{list}
So $w_b, w_{pen}, w_{paper}, w_{proc}$ workers. When the first worker has passed the bag, it could begin taking the next bag. Same for the others.
\begin{center}
	\includegraphics[scale=0.5]{1.png}
\end{center}
So in sequential we have $m\cdot(t_{bag} + t_{pen} + t_{paper} + t_{proc})$, and in parallel per 1 bag we have $t_{bag} + t_{comm} + t_{pen} + t_{comm} + t_{paper} + t_{comm} + t_{proc} + t_{comm}$ with $t_{comm}$ spent passing the bag from one to the other, so total of $m\cdot T_{seq} + m\cdot t_{comm}$. But that's not correct, because we work in parallel: ideally we have a parallelogram of $m\cdot(t_{proc} + t_{comm}$ base, and we require $t_{bag} + t_{pen} + t_{paper} + 3\cdot t_{comm}$ time to get up to speed and "fill the pipeline". But this required time is negligible, and in the end the overall time is given by the base of the parallelogram.
\paragraph{Pipeline} With $m$ tasks and $nw$ stages, with the completion of the stage $i$ required in stage $i+1$. So the output is $f_{nw}(f_{nw-1}(\ldots f_1(x_i)\ldots))$. With $t$ time required for each stage.
\begin{center}
	\includegraphics[scale=0.5]{2.png}
\end{center}
We spend $(nw-1)t$ to get the last stage working and $m\cdot t$ time spent by the last stage to complete all the tasks.
$$T_{par}(nw) = (nw-1)\cdot t + m\cdot t$$
$$\hbox{SpeedUp}(nw)=\frac{(nw\cdot t)\cdot m}{(nw-1)\cdot t + m\cdot t}$$
So the higher the $m$ is, the lower is the impact of the time required to get up to speed. So $m >> nw \Rightarrow T_{par}(nw) \simeq m\cdot t$
\paragraph{Throughput} Task completed per unit of time
\end{document}